<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>SnelDB: The Complete Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/sneldb-daee71b0.css">
        <link rel="stylesheet" href="theme/codedocs-63cc7b92.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-435af5f2.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-fcaacb9e.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">SnelDB: The Complete Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="quickstart"><a class="header" href="#quickstart">Quickstart</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<pre><code class="language-bash">test command
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="running-sneldb"><a class="header" href="#running-sneldb">Running SnelDB</a></h1>
<p>The easiest way to get hands-on is the embedded Playground.</p>
<ul>
<li>Start SnelDB (dev config enables the Playground by default):
<ul>
<li><code>server.http_addr = "127.0.0.1:8085"</code></li>
<li><code>[playground] enabled = true</code></li>
</ul>
</li>
<li>Open <code>http://127.0.0.1:8085/</code> in your browser.</li>
<li>Type commands like:</li>
</ul>
<pre><code class="language-sneldb">DEFINE subscription FIELDS { "id": "int", "plan": "string" }
STORE subscription FOR ctx1 PAYLOAD {"id":1,"plan":"free"}
STORE subscription FOR ctx2 PAYLOAD {"id":2,"plan":"pro"}
QUERY subscription WHERE id=1
</code></pre>
<p>Notes</p>
<ul>
<li>The UI posts raw command lines to <code>POST /command</code> (no JSON API required).</li>
<li>Set <code>server.output_format</code> to <code>text</code> (terminal-like), <code>json</code>, or <code>arrow</code> (Apache Arrow IPC stream).</li>
<li>To disable the Playground, set <code>[playground] enabled = false</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="a-gentle-guide-for-engineers"><a class="header" href="#a-gentle-guide-for-engineers">A Gentle Guide for Engineers</a></h1>
<p>SnelDB is built to be small and simple. It keeps track of what happened, in order, and makes it easy to get those facts back out quickly. That’s it. This guide will walk you through how to think about events, how to design them so they’re useful, and how to use SnelDB’s tiny set of commands—<code>DEFINE</code>, <code>STORE</code>, <code>QUERY</code>, and <code>REPLAY</code>. Along the way we’ll use a retail shop as an example, but the same ideas apply in many domains.</p>
<blockquote>
<p>Quick start: the easiest way to try SnelDB is to open the embedded Playground at <code>http://127.0.0.1:8085/</code> (enabled by default in dev). Type commands directly and see results immediately.</p>
</blockquote>
<h2 id="why-events"><a class="header" href="#why-events">Why events?</a></h2>
<p>An event is just a record that something happened: <em>an order was created</em>, <em>a customer signed up</em>, <em>a parcel was delivered</em>. Events don’t change once they’re stored. By keeping them all, you get a trustworthy history. Your application can look back, replay them, and figure out the current state whenever it needs. SnelDB focuses on storing these events and letting you fetch them again quickly. The “what do these events mean?” part stays in your application.</p>
<h2 id="two-ways-of-reading"><a class="header" href="#two-ways-of-reading">Two ways of reading</a></h2>
<p>With SnelDB, there are really only two ways you read:</p>
<ol>
<li>
<p><strong>Replay a timeline for one thing.</strong> All the events for a single <code>context_id</code> (like an order, a customer, or a device) form a story. If you <code>REPLAY FOR order-9001</code>, you’ll see every event for that order in sequence. Your code can fold those into the current state.</p>
</li>
<li>
<p><strong>Query across many things.</strong> Sometimes you don’t want the whole story of one order, you want a slice across all orders. For that, you use <code>QUERY</code>. For example: <code>QUERY order_created WHERE status="submitted"</code>. Behind the scenes, SnelDB uses tricks like enum bitmaps and filters to make those queries quick, so you don’t have to think about indexes.</p>
</li>
</ol>
<p>If you remember one thing: replay for one thing’s story, query for slices across many things.</p>
<h2 id="choosing-a-context"><a class="header" href="#choosing-a-context">Choosing a context</a></h2>
<p>So what is this <code>context_id</code>? Think of it as “whose story am I telling?” For a retail system:</p>
<ul>
<li>An <strong>order</strong> has a start and an end, so it makes sense to use <code>order-&lt;id&gt;</code> as the context.</li>
<li>Inventory belongs to a <strong>SKU</strong>, so <code>sku-&lt;code&gt;</code> is a context.</li>
<li>A <strong>customer profile</strong> belongs to a customer, so <code>customer-&lt;id&gt;</code> works.</li>
</ul>
<p>When you want to be able to say <em>“show me everything that ever happened to X”</em>, that X should be a context.</p>
<h2 id="designing-an-event"><a class="header" href="#designing-an-event">Designing an event</a></h2>
<p>Name events the way you’d explain them to a teammate: <code>order_created</code>, <code>customer_registered</code>, <code>shipment_delivered</code>. Keep the payload small and clear. Always include:</p>
<ul>
<li>The IDs you’ll need to filter by later (<code>order_id</code>, <code>customer_id</code>, <code>sku</code>).</li>
<li>Enums for fixed sets of values. For example:
<pre><code class="language-sneldb">"plan": ["basic", "pro", "enterprise"]
</code></pre>
</li>
<li>A timestamp for when it happened.</li>
</ul>
<p>Here are a few examples:</p>
<pre><code class="language-sneldb">DEFINE customer_registered FIELDS {
  "customer_id":"string",
  "email":"string",
  "plan":["basic","pro","enterprise"],
  "created_at":"timestamp"
}

DEFINE order_created FIELDS {
  "order_id":"string",
  "customer_id":"string",
  "status":["pending","submitted","cancelled"],
  "created_at":"timestamp"
}

DEFINE shipment_delivered FIELDS {
  "shipment_id":"string",
  "order_id":"string",
  "carrier":["UPS","DHL","FedEx"],
  "delivered_at":"timestamp"
}
</code></pre>
<h2 id="storing-events"><a class="header" href="#storing-events">Storing events</a></h2>
<p>The very first need is to <strong>record facts</strong>: something happened, and you want to keep it. Writing an event in SnelDB is just that—adding a new fact to the timeline.</p>
<pre><code class="language-sneldb">STORE customer_registered FOR customer-123
  PAYLOAD {"customer_id":"123","email":"a@b.com","plan":"pro"}

STORE order_created FOR order-9001
  PAYLOAD {"order_id":"9001","customer_id":"123","status":"pending"}

STORE shipment_delivered FOR ship-5001
  PAYLOAD {"shipment_id":"5001","order_id":"9001","carrier":"UPS"}
</code></pre>
<p>Later on, when dealing with retries or external systems, you might add optional fields like <code>idempotency_key</code>. But the heart of storing events is simply: write down the fact.</p>
<h2 id="reading-events"><a class="header" href="#reading-events">Reading events</a></h2>
<p>If you want to know the <strong>current state of one thing</strong>, replay its story:</p>
<pre><code class="language-sneldb">REPLAY FOR order-9001
</code></pre>
<p>If you want to know <strong>which events match a condition across many things</strong>, query:</p>
<pre><code class="language-sneldb">QUERY order_created WHERE customer_id="123"
</code></pre>
<p>If you need to follow a chain—like from an order to its shipment—query by the keys you included in the payload:</p>
<pre><code class="language-sneldb">QUERY shipment_delivered WHERE order_id="9001"
</code></pre>
<h2 id="how-to-evolve"><a class="header" href="#how-to-evolve">How to evolve</a></h2>
<p>SnelDB is built on immutability. Once an event is stored it never changes. If the shape of an event needs to change, we don’t edit old events or add fields onto them. Instead, we create a new version of the schema or define a new event type that represents the new shape.</p>
<p>Older events remain valid and replayable; newer ones follow the updated schema. This way, every event clearly shows which version of the schema it follows, and your code can handle old and new versions side by side. Immutability guarantees that history is stable, while evolution ensures you can keep writing new chapters without breaking the old ones.</p>
<h2 id="scaling-without-extra-knobs"><a class="header" href="#scaling-without-extra-knobs">Scaling without extra knobs</a></h2>
<p>You don’t manage indexes or query planners. You simply design your events with the right fields. SnelDB takes care of compression and filtering internally. If a query feels heavy, ask yourself: <em>did I include the right key in the payload?</em></p>
<h2 id="streaming"><a class="header" href="#streaming">Streaming</a></h2>
<p>If you need near‑real‑time processing, you don’t need a new command. Just poll with <code>SINCE</code> on your timestamp:</p>
<pre><code class="language-sneldb">QUERY order_created WHERE created_at &gt;= "2025-09-07T00:00:00Z" LIMIT 1000
</code></pre>
<p>Keep track of the last event you saw in your application and continue from there.</p>
<h2 id="other-domains"><a class="header" href="#other-domains">Other domains</a></h2>
<ul>
<li><strong>Billing:</strong> replay a subscription’s events to learn its current plan; query invoices or payments by <code>customer_id</code>.</li>
<li><strong>IoT:</strong> replay one device’s events to see its config; query telemetry since last night.</li>
<li><strong>Logistics:</strong> replay one parcel’s journey; query all parcels delivered today.</li>
</ul>
<h2 id="what-sneldb-wont-do"><a class="header" href="#what-sneldb-wont-do">What SnelDB won’t do</a></h2>
<p>SnelDB will never enforce your workflows, run aggregates, or decide who is allowed to see data. Those belong in your application or other tools. SnelDB’s job is narrower: keep facts safe, and give them back quickly.</p>
<h2 id="a-closing-picture"><a class="header" href="#a-closing-picture">A closing picture</a></h2>
<p>Think of two simple moves:</p>
<ul>
<li><strong>Down:</strong> replay the whole story for one thing.</li>
<li><strong>Across:</strong> query slices across many things.</li>
</ul>
<p>Nearly everything you need can be done by combining these two moves. The database is small on purpose. If you design your events carefully, SnelDB will give you speed and reliability without ever getting in your way.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sneldb-design-philosophy"><a class="header" href="#sneldb-design-philosophy">SnelDB Design Philosophy</a></h1>
<h2 id="hassle-free-by-design"><a class="header" href="#hassle-free-by-design">Hassle-free by design</a></h2>
<p>SnelDB is small on purpose. You don’t need to learn dozens of commands, fiddle with query planners, or manage indexes. Four simple verbs—<code>DEFINE</code>, <code>STORE</code>, <code>QUERY</code>, <code>REPLAY</code>—cover almost everything you need. Less to remember, less to break.</p>
<h2 id="immutability-at-the-core"><a class="header" href="#immutability-at-the-core">Immutability at the core</a></h2>
<p>Facts don’t change once written. Immutability makes your history reliable and auditable. If things evolve, you add new event types or new schema versions. Old events remain intact; new ones live alongside them.</p>
<h2 id="evolution-over-correction"><a class="header" href="#evolution-over-correction">Evolution over correction</a></h2>
<p>Rather than patching or rewriting, you let the story grow. Each new event is another page in the log. That makes timelines honest, reproducible, and easy to debug.</p>
<h2 id="performance-without-knobs"><a class="header" href="#performance-without-knobs">Performance without knobs</a></h2>
<p>SnelDB is built for performance, but you don’t need to manage any of it. Internally, it uses shards to spread load, an LSM-tree design to keep writes fast, and columnar storage with enum bitmaps and XOR filters to make queries efficient. You never have to tune these parts yourself—they just work in the background so you can focus on your application.</p>
<h2 id="universal-patterns"><a class="header" href="#universal-patterns">Universal patterns</a></h2>
<p>Two simple movements cover most use cases:</p>
<ul>
<li><strong>Replay</strong> one context’s timeline to rebuild its state.</li>
<li><strong>Query</strong> across many contexts with filters.</li>
</ul>
<p>This model is the same whether you’re preparing order data in retail, collecting device signals in IoT, managing subscriptions in SaaS, or feeding clean event streams into data and AI/ML teams for training and analysis.</p>
<h2 id="staying-in-its-lane"><a class="header" href="#staying-in-its-lane">Staying in its lane</a></h2>
<p>SnelDB doesn’t do business logic, aggregations, or access control. Those belong in your services and tools. The database’s job is to keep track of everything faithfully and give it back quickly.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>SnelDB is a lightweight, high‑performance database for immutable events. You append facts, then filter or replay them—quickly and reliably.</p>
<h2 id="what-it-is"><a class="header" href="#what-it-is">What it is</a></h2>
<ul>
<li>Store: append events with a type, context_id, timestamp, and payload</li>
<li>Query: filter by event type, context, time, and conditions</li>
<li>Sequence queries: find events that occur in order for the same entity</li>
<li>Replay: stream events for a context in original order</li>
</ul>
<pre><code class="language-sneldb">DEFINE payment FIELDS {"amount":"int","status":"string"}
STORE payment FOR user-123 PAYLOAD {"amount":250,"status":"verified"}
QUERY payment WHERE status="verified"
QUERY page_view FOLLOWED BY order_created LINKED BY user_id
REPLAY FOR user-123
</code></pre>
<h2 id="why-it-exists"><a class="header" href="#why-it-exists">Why it exists</a></h2>
<p>General-purpose databases and queues struggle with large, evolving event logs. SnelDB is built for:</p>
<ul>
<li>Immutable, append-only data</li>
<li>Fast filtering at scale (columnar + pruning)</li>
<li>Ordered replay per context</li>
<li>Minimal ops overhead</li>
</ul>
<h2 id="key-features"><a class="header" href="#key-features">Key features</a></h2>
<ul>
<li>Append-only storage (perfect audit trails; predictable recovery)</li>
<li>Simple, human‑readable commands (JSON‑native)</li>
<li>Fast queries at scale (shards, zones, compaction)</li>
<li>Sequence matching (find ordered event pairs for funnel analysis and conversion tracking)</li>
<li>Modern temporal indexing (per-field calendars and slabbed temporal indexes)</li>
<li>Replay built in (time‑travel debugging, sequence modeling)</li>
<li>Flexible schemas (strict validation; optional fields)</li>
<li>Lightweight &amp; safe (Rust; embeddable; no GC)</li>
</ul>
<h2 id="who-its-for"><a class="header" href="#who-its-for">Who it’s for</a></h2>
<ul>
<li>Product analytics and auditing</li>
<li>ML pipelines on event sequences</li>
<li>Operational debugging and timeline reconstruction</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="why-sneldb"><a class="header" href="#why-sneldb">Why SnelDB?</a></h1>
<p>Most databases were never built for events.</p>
<p>They’re optimized for records that change: customer profiles, inventory counts, order statuses. But in the real world, especially in modern systems and data pipelines, we’re dealing more and more with <strong>things that happened</strong> — not things that are.</p>
<ul>
<li>A user signed up.</li>
<li>A sensor pinged.</li>
<li>A document was approved.</li>
<li>A model prediction was stored.</li>
</ul>
<p>These aren’t updates. They’re <strong>facts</strong>. Immutable. Time-stamped. Contextual.</p>
<h2 id="the-gap"><a class="header" href="#the-gap">The gap</a></h2>
<p>If you’ve tried to build on top of these kinds of events, you’ve probably run into one of these:</p>
<ul>
<li><strong>Slow queries</strong> over millions of records because you’re using a general-purpose SQL database</li>
<li><strong>Too much ceremony</strong>, it’s painful to rebuild a timeline of actions (what happened, when, and in what order)</li>
<li><strong>Custom tooling</strong> just to read back historical behavior</li>
<li><strong>Mixing logs and storage</strong> (Kafka for ingest, S3 for storage, Athena for queries… and duct tape in between)</li>
<li><strong>Hard to filter</strong>, trace, or correlate things once the data grows</li>
</ul>
<p>And if you work in <strong>AI or data science</strong>, you’ve probably dealt with brittle pipelines, long joins, and the question:</p>
<blockquote>
<p><em>“How do I get all the events for this user/session/date range — and trust the output?”</em></p>
</blockquote>
<h2 id="the-idea"><a class="header" href="#the-idea">The idea</a></h2>
<p><strong>SnelDB</strong> was born to make event-driven storage and retrieval feel natural — for developers, data engineers, and model builders alike.</p>
<p>It’s a database designed from scratch for:</p>
<ul>
<li><strong>Immutable, append-only data</strong></li>
<li><strong>High-throughput ingest</strong></li>
<li><strong>Fast filtering and replay</strong></li>
<li><strong>Event-type-aware columnar storage</strong></li>
<li><strong>Schema evolution without migrations</strong></li>
<li><strong>Minimal operational overhead</strong></li>
</ul>
<p>You store events. You query them. You replay them. That’s it. It does the rest — segmenting, zoning, indexing, compaction — in the background.</p>
<h2 id="why-not-just-use-x"><a class="header" href="#why-not-just-use-x">Why not just use X?</a></h2>
<ul>
<li><strong>Kafka?</strong> Great for streaming, not for historical querying.</li>
<li><strong>PostgreSQL?</strong> Fantastic RDBMS, but not built for multi-billion-row event logs.</li>
<li><strong>Snowflake?</strong> Powerful, but heavy and expensive for interactive filtering.</li>
<li><strong>ClickHouse?</strong> Blazing fast, but not optimized for replay semantics and evolving schemas.</li>
</ul>
<p>SnelDB is a <strong>sweet spot</strong>: light like SQLite, fast like ClickHouse, event-native like Kafka — but simple to reason about.</p>
<h2 id="built-for-builders"><a class="header" href="#built-for-builders">Built for builders</a></h2>
<p>Whether you’re:</p>
<ul>
<li>Building product analytics dashboards from raw event logs</li>
<li>Tracking user behavior over time, across sessions or contexts</li>
<li>Training machine learning models on real-world event sequences</li>
<li>Auditing critical flows or investigating anomalies</li>
<li>Archiving time-stamped data for compliance or reporting</li>
<li>Creating time-travel debugging tools or operational replay systems</li>
</ul>
<p>SnelDB gives you a clean, reliable foundation to work with immutable facts — fast to store, easy to query, and simple to reason about.</p>
<p><strong>Simple to embed. Easy to query. Scales with clarity.</strong></p>
<p>That’s why we built SnelDB.</p>
<h2 id="stories-from-the-field"><a class="header" href="#stories-from-the-field">Stories from the field</a></h2>
<p>To see why SnelDB exists, it helps to look at a few real situations where traditional tools fall short.</p>
<ul>
<li>
<p><strong>Product analytics at scale</strong>
A growing SaaS company wants to track how users move through their app.
At first, PostgreSQL is fine. But soon the tables balloon into billions of rows. Queries slow to a crawl, analysts create brittle pipelines, and nobody fully trusts the numbers.
With SnelDB, they could store clean, immutable event streams, filter them quickly by context, and build dashboards that actually stay fast as volume grows.</p>
</li>
<li>
<p><strong>Machine learning pipelines</strong>
A data science team trains fraud detection models using transaction histories.
They struggle to rebuild consistent training sets: data is scattered across Kafka topics, S3 buckets, and ad-hoc SQL queries.
With SnelDB, they can reliably fetch “all sequences of events leading to flagged outcomes,” ensuring reproducibility and shortening the path from raw logs to usable training data.</p>
</li>
<li>
<p><strong>Auditing in regulated industries</strong>
A fintech startup needs to prove to auditors what happened, when, and by whom.
Traditional databases allow updates and deletes, which introduces doubt.
SnelDB’s append-only design guarantees that past events remain untouched, making it straightforward to demonstrate compliance with minimal operational effort.</p>
</li>
<li>
<p><strong>Operational debugging</strong>
An infrastructure engineer gets paged at 2am for a production outage.
Logs are rotated, metrics are sampled, and the picture is incomplete.
With SnelDB, they can replay the exact sequence of system events leading up to the failure, reconstruct the timeline, and pinpoint the root cause without guesswork.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-sneldb"><a class="header" href="#what-is-sneldb">What is SnelDB?</a></h1>
<p>SnelDB is a lightweight, high-performance database designed for <strong>immutable events</strong>.</p>
<p>At its core, it’s a system where you can:</p>
<ul>
<li><strong>Store</strong> events in an append-only fashion</li>
<li><strong>Query</strong> them efficiently by type, context, or time</li>
<li><strong>Replay</strong> them in order to understand what happened</li>
</ul>
<p>That’s it. No updates. No deletes. Just fast, reliable access to a growing stream of facts.</p>
<h2 id="not-your-average-database"><a class="header" href="#not-your-average-database">Not your average database</a></h2>
<p>SnelDB is not a general-purpose relational database, a message broker, or a data lake.
It’s a specialized tool focused on <strong>event-driven data</strong>:</p>
<ul>
<li>Unlike a <strong>relational database</strong>, SnelDB doesn’t model changing rows. It treats data as a log of things that happened.</li>
<li>Unlike a <strong>message queue</strong>, it’s built for storage and querying, not just delivery.</li>
<li>Unlike a <strong>data warehouse</strong>, it’s lightweight and easy to embed in everyday applications.</li>
</ul>
<p>Think of it as a <strong>database that embraces time and immutability as first-class concepts</strong>.</p>
<h2 id="a-mental-model"><a class="header" href="#a-mental-model">A mental model</a></h2>
<p>The easiest way to think about SnelDB is:</p>
<ul>
<li><strong>A notebook for your system’s history</strong>: every line is a fact, recorded once, never erased.</li>
<li><strong>A timeline you can slice and filter</strong>: events are grouped by type, context, and time, so you can quickly zoom in.</li>
<li><strong>A replay button</strong>: if you need to reconstruct a past sequence, you can ask SnelDB to play it back in order.</li>
</ul>
<h2 id="a-simple-example"><a class="header" href="#a-simple-example">A simple example</a></h2>
<p>Imagine you’re building a payments system.</p>
<p>You might store events like:</p>
<pre><code class="language-json">{ "event_type": "payment_initiated", "context_id": "user_123",  "payload" : { "amount": 100 }, "timestamp": "2025-08-20T09:30:00Z" }
{ "event_type": "payment_verified",  "context_id": "user_123",  "payload" : { "amount": 100 }, "timestamp": "2025-08-20T09:31:00Z" }
{ "event_type": "payment_settled",   "context_id": "user_123", "payload" : { "amount": 100 }, "timestamp": "2025-08-20T09:35:00Z" }
</code></pre>
<p>Later, you might want to:</p>
<ul>
<li>Fetch all payment_initiated events from last week</li>
<li>Replay all events for <code>user_123</code> in order</li>
<li>Filter for verified payments over <code>$500</code></li>
</ul>
<p>And maybe even more:</p>
<ul>
<li>Compare the average settlement time for all payments last month</li>
<li>Find all users who initiated a payment but never settled</li>
<li>Retrieve the full sequence of events for a disputed transaction</li>
<li>Generate a distribution of payment amounts across different countries</li>
<li>Train a model using all past transactions, keeping the exact order of events intact</li>
</ul>
<p>In a traditional setup, you’d stitch together logs, SQL queries, and custom scripts.
With SnelDB, these queries are <strong>first-class citizens</strong>. For example:</p>
<pre><code class="language-sneldb">QUERY payment_initiated SINCE 2025-08-01
</code></pre>
<p>or:</p>
<pre><code class="language-sneldb">REPLAY FOR user_123
</code></pre>
<p>even like:</p>
<pre><code class="language-sneldb">QUERY payment_verified WHERE amount &gt; 500
</code></pre>
<p>or find sequences:</p>
<pre><code class="language-sneldb">QUERY payment_initiated FOLLOWED BY payment_settled LINKED BY user_id
</code></pre>
<p>Instead of thinking in terms of tables and joins, you think in terms of events.
SnelDB is designed so the way you ask matches the way you think: “What happened? When? For whom? What happened next?”</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h1>
<p>SnelDB is small in surface area but powerful in practice.
Here are the highlights that make it different:</p>
<h2 id="1-append-only-storage"><a class="header" href="#1-append-only-storage">1. Append-only storage</a></h2>
<p>Events are <strong>immutable</strong>.
Once stored, they’re never updated or deleted — which means:</p>
<ul>
<li>Perfect audit trails</li>
<li>Predictable replay of past behavior</li>
<li>No risk of hidden mutations breaking analysis</li>
</ul>
<h2 id="2-simple-human-readable-commands"><a class="header" href="#2-simple-human-readable-commands">2. Simple, human-readable commands</a></h2>
<p>No SQL boilerplate. No obscure APIs.
SnelDB has a compact command language that reads like plain English:</p>
<pre><code class="language-sneldb">DEFINE payment FIELDS { "amount": "int", "status": "string" }
STORE payment FOR user-123 PAYLOAD {"amount": 250, "status":"verified"}
QUERY payment WHERE status="verified"
REPLAY FOR user-123
</code></pre>
<p>Fast to learn. Easy to remember.
Case-insensitive and JSON-native.</p>
<h2 id="3-fast-queries-at-scale"><a class="header" href="#3-fast-queries-at-scale">3. Fast queries at scale</a></h2>
<p>Under the hood, SnelDB uses an LSM-tree design with:</p>
<ul>
<li>Shards for parallelism</li>
<li>Zones and filters to skip irrelevant data</li>
<li>Compaction to keep reads efficient over time</li>
</ul>
<p>The result: queries stay snappy whether you have thousands or billions of events.</p>
<h2 id="4-replay-built-in"><a class="header" href="#4-replay-built-in">4. Replay built in</a></h2>
<p>You don’t just query — you can replay events in order:</p>
<pre><code class="language-sneldb">REPLAY order_created FOR customer-42
</code></pre>
<p>This makes debugging, time-travel analysis, and sequence modeling natural parts of the workflow.</p>
<h2 id="45-sequence-matching"><a class="header" href="#45-sequence-matching">4.5. Sequence matching</a></h2>
<p>Find events that occur in a specific order for the same entity:</p>
<pre><code class="language-sneldb">QUERY page_view FOLLOWED BY order_created LINKED BY user_id WHERE page_view.page="/checkout"
</code></pre>
<p>Perfect for funnel analysis, conversion tracking, and understanding event dependencies. SnelDB uses efficient columnar processing and two-pointer algorithms to match sequences without materializing events.</p>
<h2 id="5-flexible-schemas"><a class="header" href="#5-flexible-schemas">5. Flexible schemas</a></h2>
<p>SnelDB supports schema definitions per event type, with:</p>
<ul>
<li>Strict validation: payloads must match fields</li>
<li>Optional fields: declared as string | null</li>
<li>Clear errors when something doesn’t line up</li>
</ul>
<p>This keeps data trustworthy without slowing you down.</p>
<h2 id="6-designed-for-ai--analytics"><a class="header" href="#6-designed-for-ai--analytics">6. Designed for AI &amp; analytics</a></h2>
<p>Because events are ordered, immutable, and replayable, SnelDB is a natural fit for:</p>
<ul>
<li>Training models on real-world sequences</li>
<li>Feeding pipelines with reproducible datasets</li>
<li>Analyzing behavior over time without complex joins</li>
<li>Auditing decision processes with confidence</li>
</ul>
<h2 id="7-lightweight--embeddable"><a class="header" href="#7-lightweight--embeddable">7. Lightweight &amp; embeddable</a></h2>
<p>SnelDB is written in Rust with minimal dependencies.
It runs anywhere — from a laptop dev setup to production servers — without heavyweight orchestration.</p>
<p>You can drop it into your stack as a focused, reliable event database.</p>
<h3 id="built-in-playground"><a class="header" href="#built-in-playground">Built-in Playground</a></h3>
<p>SnelDB ships with a tiny single-page Playground so you can try commands without any client code.
Start the server and open <code>http://127.0.0.1:8085/</code> to experiment with <code>DEFINE</code>, <code>STORE</code>, <code>QUERY</code>, and <code>REPLAY</code> instantly.</p>
<h2 id="8-safety-by-design"><a class="header" href="#8-safety-by-design">8. Safety by design</a></h2>
<p>SnelDB is built in Rust, which brings <strong>memory safety, thread safety, and performance</strong> without garbage collection.</p>
<p>This means:</p>
<ul>
<li>No segfaults or memory leaks corrupting your data</li>
<li>Concurrency without data races</li>
<li>Predictable performance, even under load</li>
</ul>
<p>When you’re storing critical events, <strong>safety is not optional</strong> — and Rust helps guarantee it from the ground up.</p>
<hr>
<p>In short:
SnelDB is designed to be small but sharp — a tool that does one thing well:
<strong>make working with immutable events simple, fast, and reliable.</strong></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="commands"><a class="header" href="#commands">Commands</a></h1>
<p>SnelDB has a compact, human-friendly command language. <strong>Keywords are case-insensitive</strong> (<code>store</code>, <code>STORE</code>, <code>StOrE</code> all work). <strong>Event type names and context IDs are case-preserving</strong>.</p>
<p>Core verbs:</p>
<ul>
<li><code>DEFINE</code> — declare a schema for an event type</li>
<li><code>STORE</code> — append a new event with a JSON payload</li>
<li><code>QUERY</code> — filter events</li>
<li><code>REPLAY</code> — stream events in original order (per context, optionally per type)</li>
<li><code>FLUSH</code> — force a memtable → segment flush</li>
<li><code>PING</code> — health check</li>
</ul>
<p>User management:</p>
<ul>
<li><code>CREATE USER</code> — create a new authentication user</li>
<li><code>REVOKE KEY</code> — revoke a user’s authentication key</li>
<li><code>LIST USERS</code> — list all registered users</li>
</ul>
<p>If a command returns no rows, you’ll see: <code>No matching events found</code>.</p>
<p>See pages below for full syntax and examples.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="syntax--operators"><a class="header" href="#syntax--operators">Syntax &amp; Operators</a></h1>
<h2 id="command-forms"><a class="header" href="#command-forms">Command forms</a></h2>
<pre><code class="language-sneldb">DEFINE &lt;event_type&gt; FIELDS { "key": "type", … }

STORE  &lt;event_type&gt; FOR &lt;context_id&gt; PAYLOAD &lt;json_object&gt;

QUERY  &lt;event_type&gt; [FOR &lt;context_id&gt;] [SINCE &lt;ts&gt;] [USING &lt;time_field&gt;] [WHERE &lt;expr&gt;] [LIMIT &lt;n&gt;]
QUERY  &lt;event_type_a&gt; [FOLLOWED BY|PRECEDED BY] &lt;event_type_b&gt; LINKED BY &lt;link_field&gt; [WHERE &lt;expr&gt;] [LIMIT &lt;n&gt;]

REPLAY [&lt;event_type&gt;] FOR &lt;context_id&gt; [SINCE &lt;ts&gt;] [USING &lt;time_field&gt;]

FLUSH
</code></pre>
<ul>
<li><strong>Keywords</strong>: case-insensitive.</li>
<li><strong>Literals</strong>:
<ul>
<li>Strings: double-quoted (<code>"NL"</code>, <code>"a string"</code>).</li>
<li>Numbers: unquoted (<code>42</code>, <code>3</code>, <code>900</code>).</li>
<li>Booleans: <code>true</code>, <code>false</code> (unquoted).</li>
</ul>
</li>
<li><strong>WHERE operators</strong>: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>AND</code>, <code>OR</code>, <code>NOT</code>.</li>
<li><strong>Precedence</strong>: <code>NOT</code> &gt; <code>AND</code> &gt; <code>OR</code>. Use parentheses sparingly by structuring conditions; (parentheses not required in current grammar).</li>
<li><strong>LIMIT</strong>: positive integer; caps returned rows.</li>
<li><strong>SINCE</strong>: ISO-8601 timestamp string (e.g., <code>2025-08-01T00:00:00Z</code>) or numeric epoch (s/ms/µs/ns). Parsed and normalized to epoch seconds (fractional parts truncated to whole seconds).</li>
<li><strong>USING</strong>: Selects the time field used by SINCE and bucketing; defaults to core <code>timestamp</code>. Common choices: a schema field like <code>created_at</code> declared as <code>"datetime"</code>.</li>
</ul>
<h3 id="mini-grammar-informal"><a class="header" href="#mini-grammar-informal">Mini-grammar (informal)</a></h3>
<pre><code class="language-bash">expr      := cmp | NOT expr | expr AND expr | expr OR expr
cmp       :=
op        := = | != | &gt; | &gt;= | &lt; | &lt;=
value     := string | number | boolean
</code></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<pre><code class="language-sneldb">DEFINE order_created AS 1 FIELDS {
  id: "uuid",
  amount: "float",
  currency: "string",
  created_at: "datetime"
}

STORE order_created FOR ctx_123 PAYLOAD {
  "id": "a1-b2",
  "amount": 42.5,
  "currency": "EUR",
  "created_at": "2025-09-07T12:00:00Z"
}

QUERY order_created FOR "ctx_123" SINCE "2025-08-01T00:00:00Z" USING created_at
WHERE amount &gt;= 40 AND currency = "EUR"
LIMIT 100

QUERY page_view FOLLOWED BY order_created LINKED BY user_id WHERE page_view.page="/checkout"

REPLAY order_created FOR ctx_123 SINCE "2025-08-01T00:00:00Z" USING created_at
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="define"><a class="header" href="#define">DEFINE</a></h1>
<h2 id="purpose"><a class="header" href="#purpose">Purpose</a></h2>
<p>Register the schema for an event type. <code>STORE</code> payloads must conform to this schema.</p>
<h2 id="form"><a class="header" href="#form">Form</a></h2>
<pre><code class="language-sneldb">DEFINE &lt;event_type:WORD&gt; [ AS &lt;version:NUMBER&gt; ] FIELDS { "key_1": "type_1", ... }
</code></pre>
<h2 id="constraints"><a class="header" href="#constraints">Constraints</a></h2>
<ul>
<li>Requires authentication and admin role.</li>
</ul>
<h2 id="field-pairs"><a class="header" href="#field-pairs">Field pairs</a></h2>
<ul>
<li>Keys can be STRING or WORD. The parser will quote WORD keys when converting to JSON.</li>
<li>Values (types) can be:
<ul>
<li>STRING literals, for example: “int”, “string”, “string | null”</li>
<li>Special logical time types:
<ul>
<li>“datetime” → event time instant; payload accepts ISO-8601 strings or epoch (s/ms/µs/ns) and is normalized to epoch seconds</li>
<li>“date” → calendar date; payload accepts “YYYY-MM-DD” (midnight UTC) or epoch and is normalized to epoch seconds</li>
</ul>
</li>
<li>ARRAY of strings to define an enum, for example: [“pro”, “basic”]
<ul>
<li>Enum variants are case-sensitive (“Pro” != “pro”)</li>
</ul>
</li>
</ul>
</li>
<li>Schema must be flat (no nested objects).</li>
</ul>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<pre><code class="language-sneldb">DEFINE order_created FIELDS { "order_id": "int", "status": "string" }
</code></pre>
<pre><code class="language-sneldb">DEFINE review FIELDS { rating: "int", verified: "bool" }
</code></pre>
<pre><code class="language-sneldb">DEFINE order_created AS 2 FIELDS { order_id: "int", status: "string", note: "string | null" }
</code></pre>
<pre><code class="language-sneldb">DEFINE subscription FIELDS { plan: ["pro", "basic"] }
</code></pre>
<pre><code class="language-sneldb">DEFINE product FIELDS { name: "string", created_at: "datetime", release_date: "date" }
</code></pre>
<h2 id="errors"><a class="header" href="#errors">Errors</a></h2>
<ul>
<li><code>Authentication required</code>: No user ID provided or authentication failed.</li>
<li><code>Only admin users can define schemas</code>: The authenticated user is not an admin.</li>
</ul>
<h2 id="typical-validation-errors-raised-during-store"><a class="header" href="#typical-validation-errors-raised-during-store">Typical validation errors raised during STORE</a></h2>
<ul>
<li>No schema defined</li>
<li>Missing field <code>status</code> in payload</li>
<li>Field <code>order_id</code> is expected to be one of <code>int</code>, but got <code>String</code></li>
<li>Payload contains fields not defined in schema: invalid_field</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="store"><a class="header" href="#store">Store</a></h1>
<h2 id="purpose-1"><a class="header" href="#purpose-1">Purpose</a></h2>
<p>Append an event for a specific context.</p>
<h2 id="form-1"><a class="header" href="#form-1">Form</a></h2>
<pre><code class="language-sneldb">STORE &lt;event_type:WORD&gt; FOR &lt;context_id:WORD or STRING&gt; PAYLOAD {"key":"value", ...}
</code></pre>
<h2 id="constraints-1"><a class="header" href="#constraints-1">Constraints</a></h2>
<ul>
<li><code>&lt;context_id&gt;</code> can be a WORD (example: user-1) or a quoted STRING.</li>
<li><code>PAYLOAD</code> must be a flat JSON object (no nested objects).</li>
<li><code>PAYLOAD</code> must follow schema defined using <code>DEFINE</code> command.</li>
<li>Requires authentication and write permission for the event type (or appropriate role: <code>admin</code>, <code>editor</code>, or <code>write-only</code>).</li>
</ul>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<pre><code class="language-sneldb">STORE order_created FOR customer-1 PAYLOAD {"order_id":123,"status":"confirmed"}
</code></pre>
<pre><code class="language-sneldb">STORE review FOR "user:ext:42" PAYLOAD {"rating":5,"verified":true}
</code></pre>
<pre><code class="language-sneldb">STORE login FOR user-7 PAYLOAD {"device":"android"}
</code></pre>
<h2 id="behavior"><a class="header" href="#behavior">Behavior</a></h2>
<ul>
<li>Validates payload against the schema of the event type.</li>
<li>Rejects missing or extra fields and type mismatches.</li>
<li>Durability-first: once acknowledged, the event will survive crashes.</li>
</ul>
<h2 id="errors-1"><a class="header" href="#errors-1">Errors</a></h2>
<ul>
<li><code>&lt;event_type&gt;</code> cannot be empty</li>
<li><code>&lt;context_id&gt;</code> cannot be empty</li>
<li>Schema validation errors (see <code>DEFINE</code>)</li>
<li><code>Authentication required</code>: No user ID provided or authentication failed</li>
<li><code>Write permission denied for event type '&lt;event_type&gt;'</code>: User lacks write permission for the event type and does not have an appropriate role (<code>admin</code>, <code>editor</code>, or <code>write-only</code>)</li>
<li>Overload/backpressure (rare): Shard is busy, try again later</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="query"><a class="header" href="#query">QUERY</a></h1>
<h2 id="purpose-2"><a class="header" href="#purpose-2">Purpose</a></h2>
<p>Filter events by type, optionally by context, time, predicate, and limit.</p>
<h2 id="form-2"><a class="header" href="#form-2">Form</a></h2>
<pre><code class="language-sneldb">QUERY &lt;event_type:WORD&gt;
  [ FOR &lt;context_id:WORD or STRING&gt; ]
  [ SINCE &lt;timestamp:STRING_OR_NUMBER&gt; ]
  [ USING &lt;time_field:WORD&gt; ]
  [ RETURN [ &lt;field:WORD or STRING&gt;, ... ] ]
  [ WHERE &lt;expr&gt; ]
  [ &lt;aggregations&gt; ]
  [ PER &lt;time_granularity: HOUR|DAY|WEEK|MONTH&gt; [ USING &lt;time_field:WORD&gt; ] ]
  [ BY &lt;field&gt; [, &lt;field&gt; ...] [ USING &lt;time_field:WORD&gt; ] ]
  [ LIMIT &lt;n:NUMBER&gt; ]
</code></pre>
<h2 id="constraints-2"><a class="header" href="#constraints-2">Constraints</a></h2>
<ul>
<li>Requires authentication and read permission for the event type (or appropriate role: <code>admin</code>, <code>read-only</code>/<code>viewer</code>, or <code>editor</code>).</li>
</ul>
<h2 id="examples-3"><a class="header" href="#examples-3">Examples</a></h2>
<pre><code class="language-sneldb">QUERY order_created WHERE status="confirmed"
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE status=confirmed
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE id &gt; 13 AND id &lt; 15
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE country!="NL"
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE country="NL" OR country="FR"
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE id IN (1, 2, 3)
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE (status = "active" OR status = "pending") AND priority &gt; 5
</code></pre>
<pre><code class="language-sneldb">QUERY order_created WHERE NOT status = "cancelled"
</code></pre>
<pre><code class="language-sneldb">QUERY payment SINCE "2025-08-01T00:00:00Z" WHERE amount &gt;= 500 LIMIT 100
</code></pre>
<pre><code class="language-sneldb">QUERY orders SINCE 1735689600000 USING created_at WHERE amount &gt;= 10
# SINCE accepts ISO-8601 strings or numeric epoch in s/ms/µs/ns; all normalized to seconds
</code></pre>
<pre><code class="language-sneldb">QUERY product RETURN [name, "price"] WHERE price &gt; 10
</code></pre>
<h3 id="aggregations"><a class="header" href="#aggregations">Aggregations</a></h3>
<pre><code class="language-sneldb"># Count all orders
QUERY orders COUNT

# Count unique contexts (users) per country
QUERY orders COUNT UNIQUE context_id BY country

# Sum and average amount by day using created_at field
QUERY orders TOTAL amount, AVG amount PER DAY USING created_at

# Multiple metrics with grouping
QUERY orders COUNT, TOTAL amount, AVG amount BY country

# Min/Max over comparable fields
QUERY orders MIN amount, MAX amount BY country
</code></pre>
<h2 id="notes"><a class="header" href="#notes">Notes</a></h2>
<ul>
<li><code>SINCE</code> accepts ISO-8601 strings (e.g., <code>2025-01-01T00:00:00Z</code>) or numeric epoch in seconds, milliseconds, microseconds, or nanoseconds. Inputs are normalized to epoch seconds.</li>
<li><code>USING &lt;time_field&gt;</code> makes <code>SINCE</code> and temporal pruning use a payload datetime field (e.g., <code>created_at</code>). Defaults to the core <code>timestamp</code> field.</li>
<li><code>RETURN [ ... ]</code> limits the payload fields included in results. Omit to return all payload fields. An empty list <code>RETURN []</code> also returns all payload fields.</li>
<li>Field names in <code>RETURN</code> can be bare words or quoted strings.</li>
<li>Works across in-memory and on-disk segments.</li>
<li>If nothing matches, returns: No matching events found.</li>
<li><code>IN</code> operator: <code>WHERE id IN (1, 2, 3)</code> is equivalent to <code>WHERE id = 1 OR id = 2 OR id = 3</code>. Each value uses zone indexes for efficient pruning.</li>
<li>Parentheses: Complex WHERE clauses with parentheses are supported. Example: <code>WHERE (status = "active" OR status = "pending") AND priority &gt; 5</code>.</li>
<li><code>NOT</code> operator: <code>WHERE NOT status = "cancelled"</code> returns all events except those matching the condition. Supports De Morgan’s laws for complex expressions like <code>NOT (A AND B)</code> and <code>NOT (A OR B)</code>.</li>
</ul>
<h3 id="aggregation-notes"><a class="header" href="#aggregation-notes">Aggregation notes</a></h3>
<ul>
<li>Aggregations are requested via one or more of: <code>COUNT</code>, <code>COUNT UNIQUE &lt;field&gt;</code>, <code>COUNT &lt;field&gt;</code>, <code>TOTAL &lt;field&gt;</code>, <code>AVG &lt;field&gt;</code>, <code>MIN &lt;field&gt;</code>, <code>MAX &lt;field&gt;</code>.</li>
<li>Optional <code>BY &lt;fields...&gt;</code> groups results by one or more payload fields.</li>
<li>Optional <code>PER &lt;HOUR|DAY|WEEK|MONTH&gt;</code> buckets results by the chosen time field. You can select the time field for bucketing with <code>USING &lt;time_field&gt;</code>; default is <code>timestamp</code>.</li>
<li><code>LIMIT</code> on aggregation caps the number of distinct groups produced (it does not limit events scanned within those groups).</li>
<li>Aggregations return a tabular result with columns: optional <code>bucket</code>, grouped fields, followed by metric columns like <code>count</code>, <code>total_&lt;field&gt;</code>, <code>avg_&lt;field&gt;</code>, <code>min_&lt;field&gt;</code>, <code>max_&lt;field&gt;</code>.</li>
</ul>
<h2 id="sequence-queries"><a class="header" href="#sequence-queries">Sequence Queries</a></h2>
<p>SnelDB supports sequence matching queries that find events that occur in a specific order for the same entity. This is perfect for funnel analysis, conversion tracking, and understanding event dependencies.</p>
<h3 id="basic-form"><a class="header" href="#basic-form">Basic Form</a></h3>
<pre><code class="language-sneldb">QUERY &lt;event_type_a&gt; FOLLOWED BY &lt;event_type_b&gt; LINKED BY &lt;link_field&gt;
QUERY &lt;event_type_a&gt; PRECEDED BY &lt;event_type_b&gt; LINKED BY &lt;link_field&gt;
</code></pre>
<h3 id="concepts"><a class="header" href="#concepts">Concepts</a></h3>
<ul>
<li><strong>FOLLOWED BY</strong>: Finds events where <code>event_type_b</code> occurs after <code>event_type_a</code> in time</li>
<li><strong>PRECEDED BY</strong>: Finds events where <code>event_type_b</code> occurred before <code>event_type_a</code> in time</li>
<li><strong>LINKED BY</strong>: Defines the field that connects events together (e.g., <code>user_id</code>, <code>order_id</code>, <code>session_id</code>)</li>
</ul>
<h3 id="examples-1-1"><a class="header" href="#examples-1-1">Examples</a></h3>
<p><strong>Funnel analysis</strong>: Find users who viewed the checkout page and then created an order:</p>
<pre><code class="language-sneldb">QUERY page_view FOLLOWED BY order_created LINKED BY user_id
</code></pre>
<p><strong>With WHERE clause</strong>: Only count checkout page views:</p>
<pre><code class="language-sneldb">QUERY page_view FOLLOWED BY order_created LINKED BY user_id WHERE page_view.page="/checkout"
</code></pre>
<p><strong>Event-specific filters</strong>: Filter both events in the sequence:</p>
<pre><code class="language-sneldb">QUERY page_view FOLLOWED BY order_created LINKED BY user_id
WHERE page_view.page="/checkout" AND order_created.status="done"
</code></pre>
<p><strong>PRECEDED BY</strong>: Find orders that were preceded by a payment failure:</p>
<pre><code class="language-sneldb">QUERY order_created PRECEDED BY payment_failed LINKED BY user_id WHERE order_created.status="done"
</code></pre>
<p><strong>Avoiding ambiguity</strong>: If both event types have the same field name, use event-prefixed fields:</p>
<pre><code class="language-sneldb"># This will return 400 Bad Request if both order_created and payment_failed have a "status" field
QUERY order_created PRECEDED BY payment_failed LINKED BY user_id WHERE status="done"

# Use event-prefixed fields to disambiguate
QUERY order_created PRECEDED BY payment_failed LINKED BY user_id WHERE order_created.status="done"
</code></pre>
<p><strong>Different link fields</strong>: Use order_id instead of user_id:</p>
<pre><code class="language-sneldb">QUERY order_created FOLLOWED BY order_cancelled LINKED BY order_id
</code></pre>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li><strong>Grouping</strong>: Events are grouped by the <code>link_field</code> value (e.g., all events for <code>user_id="u1"</code> are grouped together)</li>
<li><strong>Sorting</strong>: Within each group, events are sorted by timestamp</li>
<li><strong>Matching</strong>: The two-pointer algorithm finds matching sequences efficiently</li>
<li><strong>Filtering</strong>: WHERE clauses are applied before matching to reduce the search space</li>
</ol>
<h3 id="where-clause-behavior"><a class="header" href="#where-clause-behavior">WHERE Clause Behavior</a></h3>
<ul>
<li><strong>Event-prefixed fields</strong>: Use <code>event_type.field</code> to filter specific events (e.g., <code>page_view.page="/checkout"</code>)</li>
<li><strong>Common fields</strong>: Fields without a prefix apply to all events (e.g., <code>timestamp &gt; 1000</code>)</li>
<li><strong>Combined</strong>: You can mix event-specific and common filters with <code>AND</code>/<code>OR</code></li>
<li><strong>Ambiguity detection</strong>: If a common field (without event prefix) exists in multiple event types within the sequence, the query will return a <code>400 Bad Request</code> error. Use event-prefixed fields to disambiguate (e.g., <code>order_created.status="done"</code> instead of <code>status="done"</code> when both <code>order_created</code> and <code>payment_failed</code> have a <code>status</code> field)</li>
</ul>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>Sequence queries are optimized for performance:</p>
<ul>
<li><strong>Columnar processing</strong>: Events are processed in columnar format without materialization</li>
<li><strong>Early filtering</strong>: WHERE clauses are applied before grouping and matching</li>
<li><strong>Parallel collection</strong>: Zones for different event types are collected in parallel</li>
<li><strong>Index usage</strong>: Existing indexes on <code>link_field</code> and <code>event_type</code> are leveraged</li>
</ul>
<h3 id="notes-1"><a class="header" href="#notes-1">Notes</a></h3>
<ul>
<li>Both events in the sequence must have the same value for the <code>link_field</code></li>
<li>For <code>FOLLOWED BY</code>, <code>event_type_b</code> must occur at the same timestamp or later than <code>event_type_a</code></li>
<li>For <code>PRECEDED BY</code>, <code>event_type_b</code> must occur strictly before <code>event_type_a</code> (same timestamp does not match)</li>
<li>The query returns both events from each matched sequence</li>
<li><code>LIMIT</code> applies to the number of matched sequences, not individual events</li>
</ul>
<h2 id="errors-2"><a class="header" href="#errors-2">Errors</a></h2>
<ul>
<li><code>Authentication required</code>: No user ID provided or authentication failed.</li>
<li><code>Read permission denied for event type '&lt;event_type&gt;'</code>: User lacks read permission for the event type.</li>
</ul>
<h2 id="gotchas"><a class="header" href="#gotchas">Gotchas</a></h2>
<ul>
<li>Field names used in <code>WHERE</code> must exist in the schema for that event type.</li>
<li>Strings must be double-quoted when you need explicit string literals.</li>
<li>Unknown fields in <code>RETURN</code> are ignored; only schema-defined payload fields (plus core fields <code>context_id</code>, <code>event_type</code>, <code>timestamp</code>) are returned.</li>
<li>Temporal literals in <code>WHERE</code> (e.g., `created_at = “2025-01-01T00:00:01Z”) are parsed and normalized to epoch seconds. Fractional seconds are truncated; ranges using only sub-second differences may collapse to empty after normalization.</li>
<li>In sequence queries, the <code>link_field</code> must exist in both event types’ schemas.</li>
<li>In sequence queries, if a WHERE clause uses a common field (without event prefix) that exists in multiple event types, you must use event-prefixed fields to disambiguate. For example, if both <code>order_created</code> and <code>payment_failed</code> have a <code>status</code> field, use <code>order_created.status="done"</code> instead of <code>status="done"</code> to avoid ambiguity errors.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="replay"><a class="header" href="#replay">REPLAY</a></h1>
<h2 id="purpose-3"><a class="header" href="#purpose-3">Purpose</a></h2>
<p>Stream events back in their original append order for a context, optionally restricted to one event type.</p>
<h2 id="form-3"><a class="header" href="#form-3">Form</a></h2>
<pre><code class="language-sneldb">REPLAY [ &lt;event_type:WORD&gt; ]
  FOR &lt;context_id:WORD or STRING&gt;
  [ SINCE &lt;timestamp:STRING&gt; ]
  [ RETURN [ &lt;field:WORD or STRING&gt;, ... ] ]
</code></pre>
<h2 id="variants"><a class="header" href="#variants">Variants</a></h2>
<ul>
<li>All event types:</li>
</ul>
<pre><code class="language-sneldb">REPLAY FOR &lt;context_id&gt;
</code></pre>
<ul>
<li>Only specific event types:</li>
</ul>
<pre><code class="language-sneldb">REPLAY &lt;event_type&gt; FOR &lt;context_id&gt;
</code></pre>
<h2 id="examples-4"><a class="header" href="#examples-4">Examples</a></h2>
<pre><code class="language-sneldb">REPLAY FOR alice
</code></pre>
<pre><code class="language-sneldb">REPLAY order_shipped FOR customer-99
</code></pre>
<pre><code class="language-sneldb">REPLAY FOR "user:ext:42" SINCE "2025-08-20T09:00:00Z"
</code></pre>
<pre><code class="language-sneldb">REPLAY product FOR user-1 RETURN ["name"]
</code></pre>
<h2 id="behavior-1"><a class="header" href="#behavior-1">Behavior</a></h2>
<ul>
<li>Routes to the shard owning the context ID.</li>
<li>Preserves original order.</li>
<li>If nothing matches: No matching events found.</li>
<li><code>RETURN [ ... ]</code> limits payload fields in the replayed events. Omit or use <code>RETURN []</code> to include all payload fields. Unknown fields are ignored; core fields (<code>context_id</code>, <code>event_type</code>, <code>timestamp</code>) are always present.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flush"><a class="header" href="#flush">Flush</a></h1>
<h2 id="purpose-4"><a class="header" href="#purpose-4">Purpose</a></h2>
<p>Force a memtable flush into an immutable segment.</p>
<h2 id="form-4"><a class="header" href="#form-4">Form</a></h2>
<pre><code class="language-sneldb">FLUSH
</code></pre>
<h2 id="notes-2"><a class="header" href="#notes-2">Notes</a></h2>
<p>Useful for tests, checkpoints, or when you want on-disk segments immediately. Not required for correctness; ingestion continues during flush.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="remember"><a class="header" href="#remember">Remember</a></h1>
<h2 id="purpose-5"><a class="header" href="#purpose-5">Purpose</a></h2>
<p>Materialize the results of a streaming-compatible <code>QUERY</code> under a durable alias so future readers can replay the stored snapshot without re-scanning the cluster.</p>
<h2 id="form-5"><a class="header" href="#form-5">Form</a></h2>
<pre><code class="language-sneldb">REMEMBER QUERY &lt;query-expr&gt; AS &lt;name&gt;
</code></pre>
<ul>
<li><code>&lt;query-expr&gt;</code> must be a plain <code>QUERY</code> command that already works at the shell prompt.</li>
<li><code>&lt;name&gt;</code> becomes the filename-friendly alias used under <code>materializations/&lt;name&gt;/</code>.</li>
</ul>
<h2 id="constraints-3"><a class="header" href="#constraints-3">Constraints</a></h2>
<ul>
<li>Aliases may contain ASCII letters, digits, <code>_</code>, and <code>-</code> only.</li>
<li>Only selection queries without aggregates, grouping, or event sequences can be remembered (the same restriction as streaming queries).</li>
<li>The first run performs a full scan; ensure the backend has enough disk for the snapshot.</li>
</ul>
<h2 id="behavior-2"><a class="header" href="#behavior-2">Behavior</a></h2>
<ol>
<li>The query plan is compiled and executed once through the streaming pipeline.</li>
<li>Batches are persisted to <code>materializations/&lt;name&gt;/frames/NNNNNN.mat</code> using the same column ordering and types as the live query.</li>
<li>A catalog entry (<code>materializations/catalog.bin</code>) records:
<ul>
<li>Canonical query hash and serialized query spec.</li>
<li>Stored schema snapshot.</li>
<li>Current row and byte totals.</li>
<li>High-water mark (timestamp + event_id) used for future incremental refreshes.</li>
<li>Last append deltas (rows/bytes) and timestamps.</li>
<li>Optional retention policy placeholder (future feature).</li>
</ul>
</li>
<li>A short summary (rows stored, bytes, watermark age) is returned to the caller.</li>
</ol>
<h2 id="retention"><a class="header" href="#retention">Retention</a></h2>
<p>Each remembered query can optionally track a retention policy (max rows or max age). Policies are recorded in the catalog for future use; the current implementation records the fields and prunes frames when they are set programmatically.</p>
<h2 id="diagnostics--telemetry"><a class="header" href="#diagnostics--telemetry">Diagnostics &amp; Telemetry</a></h2>
<ul>
<li>Successful runs log a <code>sneldb::remember</code> event with alias, total rows, rows appended, and watermark details.</li>
<li>You can inspect <code>materializations/catalog.bin</code> (bincode + JSON-encoded spec) to review metadata, or issue <code>SHOW &lt;name&gt;</code> to fetch both the stored snapshot and the latest delta.</li>
</ul>
<h2 id="errors-3"><a class="header" href="#errors-3">Errors</a></h2>
<ul>
<li>Alias already exists.</li>
<li>Query is not streaming-compatible.</li>
<li>Engine is unable to write the materialization directory (disk full / permission).</li>
<li>Catalog persistence failure (corrupted header or serialization error).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="show"><a class="header" href="#show">Show</a></h1>
<h2 id="purpose-6"><a class="header" href="#purpose-6">Purpose</a></h2>
<p>Stream the materialized results of a remembered query, replaying the stored snapshot and appending the latest delta in a single response.</p>
<h2 id="form-6"><a class="header" href="#form-6">Form</a></h2>
<pre><code class="language-sneldb">SHOW &lt;name&gt;
</code></pre>
<ul>
<li><code>&lt;name&gt;</code> must correspond to an existing materialization created with <code>REMEMBER QUERY … AS &lt;name&gt;</code>.</li>
</ul>
<h2 id="behavior-3"><a class="header" href="#behavior-3">Behavior</a></h2>
<ol>
<li>Loads the catalog entry and opens <code>materializations/&lt;name&gt;/</code>.</li>
<li>Streams previously stored frames into the response using the same column layout recorded at remember-time.</li>
<li>Builds an incremental query by appending <code>WHERE &lt;time_field&gt; &gt; last_timestamp OR (&lt;time_field&gt; = last_timestamp AND event_id &gt; last_event_id)</code>, where <code>&lt;time_field&gt;</code> defaults to <code>timestamp</code> unless the original query specified <code>USING &lt;time_field&gt;</code>.</li>
<li>Runs the incremental query through the streaming pipeline.</li>
<li>Forks each delta batch to the client response and to the materialized store, extending the snapshot on disk.</li>
<li>Updates the catalog with the new high-water mark, total rows/bytes, and last append deltas.</li>
<li>Logs a <code>sneldb::show</code> telemetry event summarizing counts, bytes, and watermark age.</li>
</ol>
<h2 id="output-format"><a class="header" href="#output-format">Output Format</a></h2>
<p><code>SHOW</code> reuses the streaming response format (schema header + row fragments) used by <code>QUERY</code> when streaming is enabled. Any client capable of consuming streaming query output can process a <code>SHOW</code> response without modification.</p>
<h2 id="retention-1"><a class="header" href="#retention-1">Retention</a></h2>
<p>If a retention policy (max rows / max age) is recorded in the catalog, the store will prune older frames after the delta append completes. Policies can be set programmatically via admin tooling; placeholders are stored for future command-level configuration.</p>
<h2 id="errors-4"><a class="header" href="#errors-4">Errors</a></h2>
<ul>
<li>Unknown materialization name.</li>
<li>Stored schema missing or corrupted.</li>
<li>Disk I/O failure while reading existing frames or appending delta batches.</li>
<li>Incremental query fails (e.g., schema evolution removed required fields).</li>
</ul>
<h2 id="operational-notes"><a class="header" href="#operational-notes">Operational Notes</a></h2>
<ul>
<li>The catalog (<code>materializations/catalog.bin</code>) is persisted with a binary header and JSON-encoded query spec. Deleting the file removes all metadata; individual materializations can be dropped by removing both the directory and the catalog entry.</li>
<li>High-water mark age is included in logs to help detect stale materializations that are not being refreshed.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="user-management"><a class="header" href="#user-management">User Management</a></h1>
<h2 id="purpose-7"><a class="header" href="#purpose-7">Purpose</a></h2>
<p>SnelDB provides authentication and authorization through HMAC-based signatures and session tokens. User management commands allow you to create users, revoke their access keys, list all registered users, and manage fine-grained permissions for event types.</p>
<p>SnelDB uses a two-tier access control system:</p>
<ul>
<li><strong>Roles</strong>: Broad privileges (e.g., “can read everything”, “can write everything”)</li>
<li><strong>Permissions</strong>: Fine-grained access per event type (e.g., “can read orders”, “can write payments”)</li>
</ul>
<p><strong>Important:</strong> Permissions override roles, but the behavior depends on what the permission grants or denies. If you have a <code>read-only</code> role but are granted write permission on a specific event type, you can write to that event type while still reading from your role. See <a href="#roles-and-permissions">Roles and Permissions</a> for detailed examples.</p>
<p>All commands require authentication. SnelDB supports multiple authentication methods:</p>
<ul>
<li><strong>Session tokens</strong> (recommended for high-throughput): Authenticate once with <code>AUTH</code>, receive a token, then use it for subsequent commands</li>
<li><strong>HMAC-SHA256 signatures</strong>: Sign each command with a user’s secret key</li>
<li><strong>Connection-scoped authentication</strong>: Authenticate once per connection, then send signed commands</li>
</ul>
<p>User management commands (CREATE USER, REVOKE KEY, LIST USERS) and permission management commands (GRANT, REVOKE, SHOW PERMISSIONS) require admin privileges. This ensures that only authorized administrators can manage users and permissions.</p>
<h2 id="authentication-overview"><a class="header" href="#authentication-overview">Authentication Overview</a></h2>
<p>SnelDB supports multiple authentication methods to suit different use cases:</p>
<ul>
<li>
<p><strong>Session Token Authentication</strong>: After authenticating with the <code>AUTH</code> command, you receive a session token that can be reused for multiple commands without re-signing. This is optimized for high-throughput scenarios, especially WebSocket connections.</p>
</li>
<li>
<p><strong>HMAC-SHA256 Signature Authentication</strong>: Each user has a secret key that is used to sign commands. The signature proves that the command was issued by someone who knows the secret key. This can be done per-command (inline format) or connection-scoped (after AUTH command).</p>
</li>
</ul>
<h3 id="authentication-formats"><a class="header" href="#authentication-formats">Authentication Formats</a></h3>
<p><strong>1. Session Token Authentication (Recommended for high-throughput):</strong></p>
<p>After authenticating with the <code>AUTH</code> command, you receive a session token that can be used for subsequent commands:</p>
<pre><code class="language-sneldb">AUTH user_id:signature
OK TOKEN &lt;session_token&gt;
</code></pre>
<p>Then use the token with subsequent commands:</p>
<pre><code class="language-sneldb">STORE event_type FOR context_id PAYLOAD {...} TOKEN &lt;session_token&gt;
QUERY event_type WHERE field=value TOKEN &lt;session_token&gt;
</code></pre>
<ul>
<li>Tokens are session-based and expire after a configurable time (default: 5 minutes, configurable via <code>auth.session_token_expiry_seconds</code>)</li>
<li>Tokens are 64-character hexadecimal strings (32 bytes)</li>
<li>The token must be appended at the end of the command after <code>TOKEN</code></li>
<li>This method is optimized for high-throughput scenarios, especially WebSocket connections</li>
<li>If a token is invalid or expired, the system falls back to other authentication methods</li>
</ul>
<p><strong>2. TCP/UNIX/WebSocket (after AUTH command, connection-scoped):</strong></p>
<pre><code class="language-sneldb">AUTH user_id:signature
signature:STORE event_type FOR context_id PAYLOAD {...}
</code></pre>
<p><strong>3. TCP/UNIX/WebSocket (inline format, per-command):</strong></p>
<pre><code class="language-sneldb">user_id:signature:STORE event_type FOR context_id PAYLOAD {...}
</code></pre>
<p><strong>4. HTTP (header-based):</strong></p>
<pre><code>X-Auth-User: user_id
X-Auth-Signature: signature
</code></pre>
<p>The signature is computed as: <code>HMAC-SHA256(secret_key, message)</code> where <code>message</code> is the command string being executed.</p>
<p><strong>Note:</strong> WebSocket connections support all authentication formats (token, AUTH command, and inline format). Commands are sent as text messages over the WebSocket connection.</p>
<h2 id="create-user"><a class="header" href="#create-user">CREATE USER</a></h2>
<h3 id="purpose-1-1"><a class="header" href="#purpose-1-1">Purpose</a></h3>
<p>Create a new user with authentication credentials and optional roles. The user will receive a secret key that can be used to sign commands. Roles provide broad access privileges, while permissions provide fine-grained control per event type.</p>
<h3 id="form-7"><a class="header" href="#form-7">Form</a></h3>
<pre><code class="language-sneldb">CREATE USER &lt;user_id:WORD or STRING&gt; [ WITH KEY &lt;secret_key:STRING&gt; ] [ WITH ROLES [&lt;role:STRING&gt;[,&lt;role:STRING&gt;...]] ]
</code></pre>
<h3 id="constraints-4"><a class="header" href="#constraints-4">Constraints</a></h3>
<ul>
<li><code>&lt;user_id&gt;</code> must be non-empty and contain only alphanumeric characters, underscores, or hyphens.</li>
<li><code>&lt;user_id&gt;</code> is case-sensitive (e.g., <code>user1</code> ≠ <code>User1</code>).</li>
<li>If <code>WITH KEY</code> is omitted, a random 64-character hexadecimal secret key is generated.</li>
<li>If <code>WITH KEY</code> is provided, the secret key can contain any characters.</li>
<li><code>WITH ROLES</code> is optional. If omitted, the user has no roles (access controlled only by permissions).</li>
<li>Roles can be specified as string literals (e.g., <code>"admin"</code>) or word identifiers (e.g., <code>admin</code>).</li>
<li>Multiple roles can be specified in the array.</li>
<li><code>WITH KEY</code> and <code>WITH ROLES</code> can be specified in any order.</li>
<li>Requires admin authentication.</li>
</ul>
<h3 id="supported-roles"><a class="header" href="#supported-roles">Supported Roles</a></h3>
<p>SnelDB supports the following roles:</p>
<ul>
<li><strong><code>admin</code></strong>: Full system access. Can read/write all event types and manage users/permissions.</li>
<li><strong><code>read-only</code></strong> or <strong><code>viewer</code></strong>: Can read all event types, but cannot write. Useful for monitoring and analytics users.</li>
<li><strong><code>editor</code></strong>: Can read and write all event types, but cannot manage users or permissions. Useful for data entry users.</li>
<li><strong><code>write-only</code></strong>: Can write all event types, but cannot read. Useful for data ingestion services.</li>
</ul>
<p><strong>Note:</strong> Roles provide broad access, but specific permissions can override role-based access (see <a href="#roles-and-permissions">Roles and Permissions</a> below).</p>
<h3 id="examples-5"><a class="header" href="#examples-5">Examples</a></h3>
<pre><code class="language-sneldb">CREATE USER api_client
</code></pre>
<p>Creates a user named <code>api_client</code> with no roles. Access is controlled entirely by permissions.</p>
<pre><code class="language-sneldb">CREATE USER "service-account" WITH KEY "my_custom_secret_key_12345"
</code></pre>
<p>Creates a user with a custom secret key and no roles.</p>
<pre><code class="language-sneldb">CREATE USER monitoring_service WITH KEY monitoring_key_2024 WITH ROLES ["read-only"]
</code></pre>
<p>Creates a read-only user for monitoring purposes. This user can read all event types but cannot write.</p>
<pre><code class="language-sneldb">CREATE USER data_entry WITH ROLES ["editor"]
</code></pre>
<p>Creates an editor user who can read and write all event types.</p>
<pre><code class="language-sneldb">CREATE USER admin_user WITH ROLES ["admin"]
</code></pre>
<p>Creates an admin user with full system access.</p>
<pre><code class="language-sneldb">CREATE USER viewer WITH ROLES ["viewer"]
</code></pre>
<p>Creates a viewer user (alias for read-only role).</p>
<pre><code class="language-sneldb">CREATE USER writer WITH ROLES ["write-only"]
</code></pre>
<p>Creates a write-only user who can write but not read.</p>
<pre><code class="language-sneldb">CREATE USER multi_role WITH ROLES ["admin", "read-only"]
</code></pre>
<p>Creates a user with multiple roles. Admin role takes precedence.</p>
<pre><code class="language-sneldb">CREATE USER api_client WITH KEY "secret" WITH ROLES ["read-only"]
</code></pre>
<p>Creates a user with both a custom key and a role. Order doesn’t matter.</p>
<pre><code class="language-sneldb">CREATE USER api_client WITH ROLES ["read-only"] WITH KEY "secret"
</code></pre>
<p>Same as above, with roles and key in different order.</p>
<h3 id="behavior-4"><a class="header" href="#behavior-4">Behavior</a></h3>
<ul>
<li>Validates the user ID format before creation.</li>
<li>Checks if the user already exists (returns error if duplicate).</li>
<li>Stores the user in the protected auth store (backed by <code>AuthWalStorage</code>, a dedicated encrypted WAL file).</li>
<li>Caches the user credentials in memory for fast authentication lookups (auth checks are O(1)).</li>
<li>Returns the secret key in the response (only shown once during creation).</li>
</ul>
<h3 id="response-format"><a class="header" href="#response-format">Response Format</a></h3>
<pre><code>200 OK
User 'api_client' created
Secret key: a1b2c3d4e5f6...
</code></pre>
<h3 id="errors-5"><a class="header" href="#errors-5">Errors</a></h3>
<ul>
<li><code>Invalid user ID format</code>: User ID contains invalid characters or is empty.</li>
<li><code>User already exists: &lt;user_id&gt;</code>: A user with this ID already exists.</li>
</ul>
<h2 id="revoke-key"><a class="header" href="#revoke-key">REVOKE KEY</a></h2>
<h3 id="purpose-2-1"><a class="header" href="#purpose-2-1">Purpose</a></h3>
<p>Revoke a user’s authentication key by marking it as inactive. The user will no longer be able to authenticate commands, but their user record remains in the system.</p>
<h3 id="form-1-1"><a class="header" href="#form-1-1">Form</a></h3>
<pre><code class="language-sneldb">REVOKE KEY &lt;user_id:WORD or STRING&gt;
</code></pre>
<h3 id="examples-1-2"><a class="header" href="#examples-1-2">Examples</a></h3>
<pre><code class="language-sneldb">REVOKE KEY api_client
</code></pre>
<p>Revokes access for the <code>api_client</code> user.</p>
<pre><code class="language-sneldb">REVOKE KEY "service-account"
</code></pre>
<p>Revokes access for a user with a hyphenated name (quotes required).</p>
<h3 id="behavior-1-1"><a class="header" href="#behavior-1-1">Behavior</a></h3>
<ul>
<li>Marks the user’s key as inactive in both the database and in-memory cache.</li>
<li>Previously authenticated connections may continue to work until they disconnect.</li>
<li>The user record remains in the system for audit purposes.</li>
<li>To restore access, you must create a new user with a different user ID (or implement key rotation in a future version).</li>
<li>Requires admin authentication.</li>
</ul>
<h3 id="response-format-1"><a class="header" href="#response-format-1">Response Format</a></h3>
<pre><code>200 OK
Key revoked for user 'api_client'
</code></pre>
<h3 id="errors-1-1"><a class="header" href="#errors-1-1">Errors</a></h3>
<ul>
<li><code>User not found: &lt;user_id&gt;</code>: No user exists with the specified user ID.</li>
</ul>
<h2 id="list-users"><a class="header" href="#list-users">LIST USERS</a></h2>
<h3 id="purpose-3-1"><a class="header" href="#purpose-3-1">Purpose</a></h3>
<p>List all registered users and their current status (active or inactive).</p>
<h3 id="form-2-1"><a class="header" href="#form-2-1">Form</a></h3>
<pre><code class="language-sneldb">LIST USERS
</code></pre>
<h3 id="examples-2-1"><a class="header" href="#examples-2-1">Examples</a></h3>
<pre><code class="language-sneldb">LIST USERS
</code></pre>
<h3 id="behavior-2-1"><a class="header" href="#behavior-2-1">Behavior</a></h3>
<ul>
<li>Returns all users registered in the system.</li>
<li>Shows each user’s ID and active status.</li>
<li>Does not return secret keys (for security reasons).</li>
<li>Results are returned from the in-memory cache for fast access.</li>
<li>Requires admin authentication.</li>
</ul>
<h2 id="auth-storage-and-durability"><a class="header" href="#auth-storage-and-durability">Auth Storage and Durability</a></h2>
<ul>
<li><strong>Isolation:</strong> User data is stored in a dedicated encrypted WAL file (<code>AuthWalStorage</code>) and is not queryable through regular commands. The HTTP dispatcher rejects any <code>__system_*</code> context from user input to prevent access to system contexts.</li>
<li><strong>Hot path:</strong> Authentication and authorization are served from in-memory caches (<code>UserCache</code>, <code>PermissionCache</code>) for O(1) checks.</li>
<li><strong>Durability:</strong> Mutations (create/revoke/grant/revoke permissions) append to a dedicated secured WAL (<code>.swal</code>). Frames are binary, length-prefixed, CRC-checked, encrypted with ChaCha20Poly1305, and versioned with the standard storage header. Corrupted frames are skipped during replay instead of failing startup.</li>
<li><strong>Replay:</strong> On startup, the auth WAL is replayed with “latest timestamp wins” semantics to rebuild the caches. The WAL is small because auth mutations are rare compared to data events.</li>
<li><strong>Configurable fsync cadence:</strong> The auth WAL flushes each write and fsyncs periodically; you can tune fsync frequency via the WAL settings (or override per storage instance) to balance throughput and durability.</li>
<li><strong>Encryption:</strong> Auth WAL payloads are encrypted (AEAD). Provide a 32-byte master key via <code>SNELDB_AUTH_WAL_KEY</code> (64-char hex). If unset, a key is derived from the server <code>auth_token</code>.</li>
</ul>
<h3 id="response-format-2"><a class="header" href="#response-format-2">Response Format</a></h3>
<pre><code>200 OK
api_client: active
service-account: active
old_client: inactive
</code></pre>
<p>If no users exist:</p>
<pre><code>200 OK
No users found
</code></pre>
<h3 id="notes-3"><a class="header" href="#notes-3">Notes</a></h3>
<ul>
<li>Secret keys are never returned by this command.</li>
<li>The list includes both active and inactive users.</li>
<li>Results are ordered by user ID (implementation-dependent).</li>
<li>Requires admin authentication.</li>
</ul>
<h2 id="grant"><a class="header" href="#grant">GRANT</a></h2>
<h3 id="purpose-4-1"><a class="header" href="#purpose-4-1">Purpose</a></h3>
<p>Grant read and/or write permissions to a user for specific event types. Permissions control which users can query (read) or store (write) events of a given type.</p>
<h3 id="form-3-1"><a class="header" href="#form-3-1">Form</a></h3>
<pre><code class="language-sneldb">GRANT &lt;permissions:READ[,WRITE] or WRITE[,READ]&gt; ON &lt;event_type:WORD or STRING&gt;[,&lt;event_type:WORD or STRING&gt;...] TO &lt;user_id:WORD or STRING&gt;
</code></pre>
<h3 id="constraints-1-1"><a class="header" href="#constraints-1-1">Constraints</a></h3>
<ul>
<li>Permissions must be <code>READ</code>, <code>WRITE</code>, or both (<code>READ,WRITE</code> or <code>WRITE,READ</code>).</li>
<li>Event types must be defined using the <code>DEFINE</code> command before permissions can be granted.</li>
<li>Multiple event types can be specified, separated by commas.</li>
<li>Only admin users can grant permissions.</li>
</ul>
<h3 id="examples-3-1"><a class="header" href="#examples-3-1">Examples</a></h3>
<pre><code class="language-sneldb">GRANT READ ON orders TO api_client
</code></pre>
<p>Grants read-only access to the <code>orders</code> event type for <code>api_client</code>.</p>
<pre><code class="language-sneldb">GRANT WRITE ON orders TO api_client
</code></pre>
<p>Grants write-only access to the <code>orders</code> event type for <code>api_client</code>.</p>
<pre><code class="language-sneldb">GRANT READ, WRITE ON orders TO api_client
</code></pre>
<p>Grants both read and write access to the <code>orders</code> event type for <code>api_client</code>.</p>
<pre><code class="language-sneldb">GRANT READ, WRITE ON orders, products TO api_client
</code></pre>
<p>Grants read and write access to both <code>orders</code> and <code>products</code> event types for <code>api_client</code>.</p>
<h3 id="behavior-3-1"><a class="header" href="#behavior-3-1">Behavior</a></h3>
<ul>
<li>Validates that the event type exists in the schema registry.</li>
<li>Merges with existing permissions (grant adds permissions, doesn’t remove existing ones).</li>
<li>Updates permissions in both the database and in-memory cache for fast lookups.</li>
<li>Permissions take effect immediately for new commands.</li>
</ul>
<h3 id="response-format-3"><a class="header" href="#response-format-3">Response Format</a></h3>
<pre><code>200 OK
Permissions granted to user 'api_client'
</code></pre>
<h3 id="errors-2-1"><a class="header" href="#errors-2-1">Errors</a></h3>
<ul>
<li><code>Authentication required</code>: No user ID provided or authentication failed.</li>
<li><code>Only admin users can manage permissions</code>: The authenticated user is not an admin.</li>
<li><code>Invalid permission: &lt;perm&gt;</code>. Must be ‘read’ or ‘write’`: Invalid permission name specified.</li>
<li><code>No schema defined for event type '&lt;event_type&gt;'</code>: The event type must be defined before permissions can be granted.</li>
</ul>
<h2 id="revoke-permissions"><a class="header" href="#revoke-permissions">REVOKE (Permissions)</a></h2>
<h3 id="purpose-5-1"><a class="header" href="#purpose-5-1">Purpose</a></h3>
<p>Revoke read and/or write permissions from a user for specific event types. If no permissions are specified, all permissions for the event types are revoked.</p>
<h3 id="form-4-1"><a class="header" href="#form-4-1">Form</a></h3>
<pre><code class="language-sneldb">REVOKE [&lt;permissions:READ[,WRITE] or WRITE[,READ]&gt;] ON &lt;event_type:WORD or STRING&gt;[,&lt;event_type:WORD or STRING&gt;...] FROM &lt;user_id:WORD or STRING&gt;
</code></pre>
<h3 id="constraints-2-1"><a class="header" href="#constraints-2-1">Constraints</a></h3>
<ul>
<li>Permissions are optional. If omitted, all permissions for the specified event types are revoked.</li>
<li>If permissions are specified, only those permissions are revoked (e.g., <code>REVOKE WRITE</code> only revokes write permission, leaving read permission intact).</li>
<li>Multiple event types can be specified, separated by commas.</li>
<li>Only admin users can revoke permissions.</li>
</ul>
<h3 id="examples-4-1"><a class="header" href="#examples-4-1">Examples</a></h3>
<pre><code class="language-sneldb">REVOKE READ ON orders FROM api_client
</code></pre>
<p>Revokes read permission for the <code>orders</code> event type from <code>api_client</code>, leaving write permission intact if it exists.</p>
<pre><code class="language-sneldb">REVOKE WRITE ON orders FROM api_client
</code></pre>
<p>Revokes write permission for the <code>orders</code> event type from <code>api_client</code>, leaving read permission intact if it exists.</p>
<pre><code class="language-sneldb">REVOKE READ, WRITE ON orders FROM api_client
</code></pre>
<p>Revokes both read and write permissions for the <code>orders</code> event type from <code>api_client</code>.</p>
<pre><code class="language-sneldb">REVOKE ON orders FROM api_client
</code></pre>
<p>Revokes all permissions (both read and write) for the <code>orders</code> event type from <code>api_client</code>.</p>
<pre><code class="language-sneldb">REVOKE ON orders, products FROM api_client
</code></pre>
<p>Revokes all permissions for both <code>orders</code> and <code>products</code> event types from <code>api_client</code>.</p>
<h3 id="behavior-4-1"><a class="header" href="#behavior-4-1">Behavior</a></h3>
<ul>
<li>Revokes specified permissions for the given event types.</li>
<li>If all permissions are revoked for an event type, the permission entry is removed entirely.</li>
<li>Updates permissions in both the database and in-memory cache.</li>
<li>Changes take effect immediately for new commands.</li>
</ul>
<h3 id="response-format-4"><a class="header" href="#response-format-4">Response Format</a></h3>
<pre><code>200 OK
Permissions revoked from user 'api_client'
</code></pre>
<h3 id="errors-3-1"><a class="header" href="#errors-3-1">Errors</a></h3>
<ul>
<li><code>Authentication required</code>: No user ID provided or authentication failed.</li>
<li><code>Only admin users can manage permissions</code>: The authenticated user is not an admin.</li>
<li><code>Invalid permission: &lt;perm&gt;</code>. Must be ‘read’ or ‘write’`: Invalid permission name specified.</li>
</ul>
<h2 id="show-permissions"><a class="header" href="#show-permissions">SHOW PERMISSIONS</a></h2>
<h3 id="purpose-6-1"><a class="header" href="#purpose-6-1">Purpose</a></h3>
<p>Display all permissions granted to a specific user, showing which event types they can read and/or write.</p>
<h3 id="form-5-1"><a class="header" href="#form-5-1">Form</a></h3>
<pre><code class="language-sneldb">SHOW PERMISSIONS FOR &lt;user_id:WORD or STRING&gt;
</code></pre>
<h3 id="examples-5-1"><a class="header" href="#examples-5-1">Examples</a></h3>
<pre><code class="language-sneldb">SHOW PERMISSIONS FOR api_client
</code></pre>
<p>Shows all permissions for <code>api_client</code>.</p>
<pre><code class="language-sneldb">SHOW PERMISSIONS FOR "service-account"
</code></pre>
<p>Shows all permissions for a user with a hyphenated name.</p>
<h3 id="behavior-5"><a class="header" href="#behavior-5">Behavior</a></h3>
<ul>
<li>Returns all permissions for the specified user.</li>
<li>Shows each event type and the permissions (read, write, or both).</li>
<li>Results are returned from the in-memory cache for fast access.</li>
<li>Requires admin authentication.</li>
</ul>
<h3 id="response-format-5"><a class="header" href="#response-format-5">Response Format</a></h3>
<pre><code>200 OK
Permissions for user 'api_client':
  orders: read, write
  products: read
  users: write
</code></pre>
<p>If the user has no permissions:</p>
<pre><code>200 OK
Permissions for user 'api_client':
  (has no permissions)
</code></pre>
<h3 id="errors-4-1"><a class="header" href="#errors-4-1">Errors</a></h3>
<ul>
<li><code>Authentication required</code>: No user ID provided or authentication failed.</li>
<li><code>Only admin users can manage permissions</code>: The authenticated user is not an admin.</li>
<li><code>Failed to show permissions</code>: Internal error retrieving permissions.</li>
</ul>
<h2 id="authentication-flow"><a class="header" href="#authentication-flow">Authentication Flow</a></h2>
<h3 id="initial-setup"><a class="header" href="#initial-setup">Initial Setup</a></h3>
<ol>
<li>
<p><strong>Create a user:</strong></p>
<pre><code class="language-sneldb">CREATE USER my_client
</code></pre>
<p>Save the returned secret key securely.</p>
</li>
<li>
<p><strong>Authenticate (TCP/UNIX/WebSocket):</strong></p>
<p><strong>Option A: Session Token (Recommended for high-throughput):</strong></p>
<pre><code class="language-sneldb">AUTH my_client:&lt;signature&gt;
</code></pre>
<p>Where <code>&lt;signature&gt;</code> = <code>HMAC-SHA256(secret_key, "my_client")</code></p>
<p>Response:</p>
<pre><code>OK TOKEN &lt;session_token&gt;
</code></pre>
<p>Then use the token for subsequent commands:</p>
<pre><code class="language-sneldb">STORE order_created FOR user-123 PAYLOAD {"id": 456} TOKEN &lt;session_token&gt;
</code></pre>
<p><strong>Option B: Connection-scoped authentication:</strong></p>
<pre><code class="language-sneldb">AUTH my_client:&lt;signature&gt;
</code></pre>
<p>Then send signed commands:</p>
<pre><code class="language-sneldb">&lt;signature&gt;:STORE order_created FOR user-123 PAYLOAD {"id": 456}
</code></pre>
<p>Where <code>&lt;signature&gt;</code> = <code>HMAC-SHA256(secret_key, "STORE order_created FOR user-123 PAYLOAD {\"id\": 456}")</code></p>
<p><strong>Option C: Inline format (per-command):</strong></p>
<pre><code class="language-sneldb">my_client:&lt;signature&gt;:STORE order_created FOR user-123 PAYLOAD {"id": 456}
</code></pre>
<p>Where <code>&lt;signature&gt;</code> = <code>HMAC-SHA256(secret_key, "STORE order_created FOR user-123 PAYLOAD {\"id\": 456}")</code></p>
<p><strong>Note:</strong> For WebSocket connections, send these commands as text messages over the WebSocket connection.</p>
</li>
</ol>
<h3 id="http-authentication"><a class="header" href="#http-authentication">HTTP Authentication</a></h3>
<p>For HTTP requests, include authentication headers:</p>
<pre><code>POST /command
X-Auth-User: my_client
X-Auth-Signature: &lt;signature&gt;
Content-Type: application/json

STORE order_created FOR user-123 PAYLOAD {"id": 456}
</code></pre>
<p>Where <code>&lt;signature&gt;</code> = <code>HMAC-SHA256(secret_key, "STORE order_created FOR user-123 PAYLOAD {\"id\": 456}")</code></p>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<ul>
<li><strong>Secret keys are sensitive</strong>: Store them securely and never log them.</li>
<li><strong>Session tokens are sensitive</strong>: Session tokens provide access to the system and should be protected. Treat them like passwords:
<ul>
<li>Never log tokens or expose them in error messages</li>
<li>Use secure channels (TLS/SSL) when transmitting tokens over the network</li>
<li>Tokens are stored in-memory only and are lost on server restart (this is a security feature, not a bug)</li>
<li>Tokens expire automatically after the configured time (default: 5 minutes)</li>
</ul>
</li>
<li><strong>Key rotation</strong>: Currently, revoking a key requires creating a new user. Future versions will support key rotation.</li>
<li><strong>Token revocation</strong>: There is no user-facing command to revoke session tokens. Tokens expire automatically, but cannot be manually revoked before expiration.</li>
<li><strong>User enumeration</strong>: Error messages may reveal whether a user exists. This is a known limitation.</li>
<li><strong>Rate limiting</strong>: Not currently implemented. Consider implementing rate limiting at the network layer.</li>
<li><strong>Key storage</strong>: Secret keys are stored in plaintext in SnelDB’s internal storage. Ensure proper access controls on the database files.</li>
<li><strong>Token storage</strong>: Session tokens are stored in-memory only (not persisted to disk), which means they are lost on server restart but also cannot be recovered from disk if the server is compromised.</li>
</ul>
<h2 id="critical-issues"><a class="header" href="#critical-issues">Critical Issues</a></h2>
<p>The following critical security issues need to be addressed:</p>
<ul>
<li><input disabled="" type="checkbox"> <strong>Secret key exposure</strong>: Secret keys are returned in command responses and may be logged or exposed in network traces.</li>
<li><input disabled="" type="checkbox"> <strong>Session token exposure</strong>: Session tokens are returned in <code>AUTH</code> command responses and may be logged or exposed in network traces. Tokens sent with commands (<code>COMMAND ... TOKEN &lt;token&gt;</code>) may also be logged.</li>
<li><input disabled="" type="checkbox"> <strong>No token revocation command</strong>: There is no user-facing command to revoke session tokens. Tokens can only be revoked by waiting for expiration or server restart.</li>
<li><input disabled="" type="checkbox"> <strong>User enumeration</strong>: Error messages reveal whether a user exists (<code>UserNotFound</code> vs <code>UserExists</code>), enabling user enumeration attacks.</li>
<li><input disabled="" type="checkbox"> <strong>Weak constant-time comparison</strong>: The current constant-time comparison implementation has an early return that leaks timing information about signature length.</li>
<li><input disabled="" type="checkbox"> <strong>No rate limiting</strong>: Missing rate limiting allows brute-force attacks on signatures, user creation, and token validation attempts.</li>
<li><input disabled="" type="checkbox"> <strong>Plaintext key storage</strong>: Secret keys are stored in plaintext in the database, exposing all keys if the database is compromised.</li>
<li><input disabled="" type="checkbox"> <strong>Error message leakage</strong>: Detailed error messages reveal internal system details to potential attackers.</li>
<li><input disabled="" type="checkbox"> <strong>No key rotation</strong>: Once compromised, keys cannot be rotated without creating a new user account.</li>
<li><input disabled="" type="checkbox"> <strong>AUTH command signature verification</strong>: The AUTH command signature verification may not match the documented format.</li>
<li><input disabled="" type="checkbox"> <strong>No input length limits</strong>: Missing input length validation allows potential denial-of-service attacks via oversized inputs.</li>
<li><input disabled="" type="checkbox"> <strong>Token validation timing</strong>: Token validation uses hash lookup (O(1)), but error messages may leak information about token existence.</li>
</ul>
<h2 id="roles-and-permissions"><a class="header" href="#roles-and-permissions">Roles and Permissions</a></h2>
<p>SnelDB implements a two-tier access control system combining <strong>roles</strong> (broad privileges) and <strong>permissions</strong> (fine-grained access per event type).</p>
<h3 id="understanding-roles-vs-permissions"><a class="header" href="#understanding-roles-vs-permissions">Understanding Roles vs Permissions</a></h3>
<ul>
<li><strong>Roles</strong>: Provide broad, organization-wide access privileges (e.g., “can read everything”, “can write everything”)</li>
<li><strong>Permissions</strong>: Provide specific access to individual event types (e.g., “can read orders”, “can write payments”)</li>
</ul>
<h3 id="access-control-priority"><a class="header" href="#access-control-priority">Access Control Priority</a></h3>
<p>When checking access, SnelDB uses the following priority order:</p>
<ol>
<li><strong>Admin role</strong> → Full access (highest priority)</li>
<li><strong>Specific permissions</strong> → Override roles (most granular)</li>
<li><strong>Roles</strong> → Apply when no specific permissions exist (broader access)</li>
<li><strong>Deny</strong> → If no permissions and no roles</li>
</ol>
<p><strong>Key Principle:</strong> Permissions override roles, but only for the specific permission type being checked. This allows for flexible access control where roles provide defaults and permissions provide exceptions.</p>
<h4 id="how-permission-override-works"><a class="header" href="#how-permission-override-works">How Permission Override Works</a></h4>
<p>The permission override logic works differently for READ and WRITE checks:</p>
<p><strong>For READ access:</strong></p>
<ul>
<li>If a permission set <strong>grants READ</strong> (<code>read=true</code>), use it (permission overrides role)</li>
<li>If a permission set <strong>denies READ</strong> (<code>read=false</code>) but grants WRITE (<code>write=true</code>), fall back to role for READ
<ul>
<li><strong>Rationale:</strong> <code>GRANT WRITE</code> is additive—it adds write capability without removing existing read access from roles. If you want write-only access, explicitly <code>REVOKE READ</code> first.</li>
</ul>
</li>
<li>If a permission set <strong>explicitly denies both</strong> (<code>read=false, write=false</code>), deny access completely (override role)</li>
<li>If <strong>no permission set exists</strong>, use role</li>
</ul>
<p><strong>For WRITE access:</strong></p>
<ul>
<li>If a permission set <strong>exists for the event type</strong>, it completely overrides the role (both granting and denying)</li>
<li>If <strong>no permission set exists</strong>, use role</li>
</ul>
<p>This design ensures that:</p>
<ul>
<li>Granting WRITE permission doesn’t remove READ access from roles</li>
<li>Revoking WRITE permission explicitly denies WRITE even if role would grant it</li>
<li>Revoking all permissions creates an explicit denial that overrides roles</li>
</ul>
<h3 id="roles"><a class="header" href="#roles">Roles</a></h3>
<p>SnelDB supports the following roles:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Role</th><th>Read All Events</th><th>Write All Events</th><th>Admin Functions</th></tr>
</thead>
<tbody>
<tr><td><code>admin</code></td><td>✅</td><td>✅</td><td>✅</td></tr>
<tr><td><code>read-only</code> / <code>viewer</code></td><td>✅</td><td>❌</td><td>❌</td></tr>
<tr><td><code>editor</code></td><td>✅</td><td>✅</td><td>❌</td></tr>
<tr><td><code>write-only</code></td><td>❌</td><td>✅</td><td>❌</td></tr>
</tbody>
</table>
</div>
<p><strong>Role Behavior:</strong></p>
<ul>
<li><strong>Admin</strong>: Full system access. Can manage users, grant permissions, define schemas, and access all event types.</li>
<li><strong>Read-only / Viewer</strong>: Can read all event types by default, but cannot write. Useful for monitoring, analytics, or reporting users.</li>
<li><strong>Editor</strong>: Can read and write all event types by default, but cannot manage users or permissions. Useful for data entry or ETL processes.</li>
<li><strong>Write-only</strong>: Can write all event types by default, but cannot read. Useful for data ingestion services that only need to store events.</li>
</ul>
<h3 id="permissions"><a class="header" href="#permissions">Permissions</a></h3>
<p>Permissions provide fine-grained access control at the event type level:</p>
<ul>
<li><strong>Read permission</strong>: Allows users to query events of the specified event type.</li>
<li><strong>Write permission</strong>: Allows users to store events of the specified event type.</li>
</ul>
<p>Permissions can be granted per event type, allowing precise control over what each user can access.</p>
<h3 id="how-roles-and-permissions-work-together"><a class="header" href="#how-roles-and-permissions-work-together">How Roles and Permissions Work Together</a></h3>
<p><strong>Example 1: Read-only role with write permission</strong></p>
<pre><code class="language-sneldb">CREATE USER analyst WITH ROLES ["read-only"]
GRANT WRITE ON special_events TO analyst
</code></pre>
<p>Result:</p>
<ul>
<li>✅ Can read all event types (read-only role provides READ)</li>
<li>✅ Can write to <code>special_events</code> (permission grants WRITE)</li>
<li>❌ Cannot write to other event types (read-only role denies WRITE)</li>
</ul>
<p><strong>Why:</strong> The permission grants WRITE, so it overrides the role for WRITE. But since the permission doesn’t grant READ (<code>read=false</code>), the system falls back to the role for READ access, which the read-only role provides.</p>
<p><strong>Example 2: Editor role with restrictive permission</strong></p>
<pre><code class="language-sneldb">CREATE USER editor_user WITH ROLES ["editor"]
GRANT READ ON sensitive_data TO editor_user
REVOKE WRITE ON sensitive_data FROM editor_user
</code></pre>
<p>Result:</p>
<ul>
<li>✅ Can read/write most event types (editor role)</li>
<li>✅ Can read <code>sensitive_data</code> (permission grants READ)</li>
<li>❌ Cannot write to <code>sensitive_data</code> (permission denies WRITE, overrides role)</li>
</ul>
<p><strong>Why:</strong> The permission set exists for <code>sensitive_data</code> with <code>read=true, write=false</code>. For READ, the permission grants it. For WRITE, the permission explicitly denies it, which overrides the editor role’s ability to write.</p>
<p><strong>Example 3: Write-only role with read permission</strong></p>
<pre><code class="language-sneldb">CREATE USER ingester WITH ROLES ["write-only"]
GRANT READ ON status_events TO ingester
</code></pre>
<p>Result:</p>
<ul>
<li>✅ Can write all event types (write-only role)</li>
<li>✅ Can read <code>status_events</code> (permission grants READ)</li>
<li>❌ Cannot read other event types (write-only role denies READ)</li>
</ul>
<p><strong>Why:</strong> The permission grants READ for <code>status_events</code>, so it overrides the role. For other event types, no permission exists, so the write-only role applies (can write, cannot read).</p>
<p><strong>Example 4: Revoking all permissions</strong></p>
<pre><code class="language-sneldb">CREATE USER readonly_user WITH ROLES ["read-only"]
GRANT READ, WRITE ON orders TO readonly_user
REVOKE READ, WRITE ON orders FROM readonly_user
</code></pre>
<p>Result:</p>
<ul>
<li>❌ Cannot read <code>orders</code> (explicit denial overrides role)</li>
<li>❌ Cannot write <code>orders</code> (explicit denial overrides role)</li>
<li>✅ Can read other event types (read-only role applies)</li>
</ul>
<p><strong>Why:</strong> When all permissions are revoked, a permission set with <code>read=false, write=false</code> is created. This explicit denial overrides the role completely for that event type.</p>
<p><strong>Example 5: No role, permissions only</strong></p>
<pre><code class="language-sneldb">CREATE USER api_client
GRANT READ, WRITE ON orders TO api_client
GRANT READ ON products TO api_client
</code></pre>
<p>Result:</p>
<ul>
<li>✅ Can read/write <code>orders</code> (permission)</li>
<li>✅ Can read <code>products</code> (permission)</li>
<li>❌ Cannot access other event types (no role, no permission)</li>
</ul>
<p><strong>Why:</strong> Without a role, access is controlled entirely by permissions. No permission means no access.</p>
<p><strong>Example 6: Permission grants only one type</strong></p>
<pre><code class="language-sneldb">CREATE USER readonly_user WITH ROLES ["read-only"]
GRANT WRITE ON events TO readonly_user
</code></pre>
<p>Result:</p>
<ul>
<li>✅ Can read <code>events</code> (role provides READ, permission doesn’t grant it so falls back to role)</li>
<li>✅ Can write <code>events</code> (permission grants WRITE)</li>
<li>✅ Can read other event types (read-only role)</li>
<li>❌ Cannot write other event types (read-only role)</li>
</ul>
<p><strong>Why:</strong> The permission grants WRITE but not READ (<code>read=false, write=true</code>). For READ, since the permission doesn’t grant it, the system falls back to the role, which provides READ. For WRITE, the permission grants it, overriding the role’s denial.</p>
<h3 id="permission-checking"><a class="header" href="#permission-checking">Permission Checking</a></h3>
<p>Permissions are checked at command execution time:</p>
<ul>
<li><code>STORE</code> commands require write permission for the event type (or appropriate role).</li>
<li><code>QUERY</code> commands require read permission for the event type (or appropriate role).</li>
<li><code>DEFINE</code> commands require admin role.</li>
<li>User management commands (<code>CREATE USER</code>, <code>REVOKE KEY</code>, <code>LIST USERS</code>) require admin role.</li>
<li>Permission management commands (<code>GRANT</code>, <code>REVOKE</code>, <code>SHOW PERMISSIONS</code>) require admin role.</li>
</ul>
<p>Permissions take effect immediately when granted or revoked. Changes apply to new commands; commands already in progress are not affected.</p>
<h3 id="admin-users"><a class="header" href="#admin-users">Admin Users</a></h3>
<p>Admin users are created with the <code>admin</code> role. They have full system access and can:</p>
<ul>
<li>Create and manage users</li>
<li>Grant and revoke permissions</li>
<li>Define event schemas</li>
<li>Access all event types regardless of permissions or other roles</li>
</ul>
<p>The initial admin user can be configured via the <code>initial_admin_user</code> and <code>initial_admin_key</code> configuration options, which automatically creates an admin user on first startup if no users exist.</p>
<h3 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h3>
<ol>
<li>
<p><strong>Use roles for broad access patterns</strong>: Assign roles like <code>read-only</code> or <code>editor</code> when users need consistent access across many event types.</p>
</li>
<li>
<p><strong>Use permissions for exceptions</strong>: Grant specific permissions when you need to override role-based access for particular event types.</p>
</li>
<li>
<p><strong>Combine roles and permissions</strong>: Use roles as defaults and permissions as exceptions. For example, give most users a <code>read-only</code> role, then grant write permissions only on specific event types they need to modify.</p>
</li>
<li>
<p><strong>Start restrictive</strong>: Create users without roles initially, then grant specific permissions. Add roles only when users need broader access.</p>
</li>
<li>
<p><strong>Document access patterns</strong>: Keep track of which users have which roles and permissions to maintain security and compliance.</p>
</li>
</ol>
<h2 id="future-work"><a class="header" href="#future-work">Future Work</a></h2>
<p>The following improvements are planned for user management:</p>
<ul>
<li><strong>Key rotation</strong>: Allow users to rotate their secret keys without creating a new user account.</li>
<li><strong>Key expiration</strong>: Support time-based key expiration and automatic rotation.</li>
<li><strong>Audit logging</strong>: Log all authentication attempts, user creation, key revocation, and permission changes for security auditing.</li>
<li><strong>Rate limiting</strong>: Implement per-user rate limiting to prevent abuse and brute-force attacks.</li>
<li><strong>Key encryption at rest</strong>: Encrypt secret keys in the database using a master encryption key.</li>
<li><strong>Multi-factor authentication</strong>: Support additional authentication factors beyond HMAC signatures.</li>
<li><strong>User metadata</strong>: Store additional user information (email, description, created date, last access date).</li>
<li><strong>Bulk operations</strong>: Support creating or revoking multiple users in a single command.</li>
<li><strong>Key strength validation</strong>: Enforce minimum key length and complexity requirements.</li>
<li><strong>Session management</strong>: Track active sessions and allow session invalidation. Add a command to revoke session tokens.</li>
<li><strong>Token security improvements</strong>: Implement token rotation, token binding to IP addresses, and token refresh mechanisms.</li>
<li><strong>Password reset flow</strong>: Implement secure password reset mechanisms for user accounts.</li>
<li><strong>User groups</strong>: Organize users into groups for easier management and permission assignment.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design"><a class="header" href="#design">Design</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<h2 id="what-this-section-is"><a class="header" href="#what-this-section-is">What this section is</a></h2>
<ul>
<li>A short tour of how SnelDB works inside: the big components and how data flows between them.</li>
<li>Enough context for contributors to find their bearings without reading the whole codebase first.</li>
</ul>
<h2 id="the-big-picture"><a class="header" href="#the-big-picture">The big picture</a></h2>
<ul>
<li>Commands enter via frontends (TCP/UNIX/HTTP/WebSocket) and are parsed, validated, and dispatched.</li>
<li>Writes go through a WAL for durability, land in an in-memory table, and get flushed into immutable segments on disk.</li>
<li>Reads (query/replay) scan the in-memory table and segments, skipping as much as possible using zone metadata and filters.</li>
<li>Background compaction keeps segments tidy so read performance stays predictable.</li>
<li>Sharding by <code>context_id</code> spreads work and makes per-context replay cheap.</li>
</ul>
<h2 id="lifecycle-at-a-glance"><a class="header" href="#lifecycle-at-a-glance">Lifecycle at a glance</a></h2>
<ul>
<li><code>DEFINE</code>: register or update the schema for an event type (used to validate STORE).</li>
<li><code>STORE</code>: validate payload → append to WAL → apply to MemTable → later flush to a new segment.</li>
<li><code>QUERY</code>: fan out to shards, prune zones and project only needed columns, evaluate predicates, merge results.</li>
<li><code>REPLAY</code>: route to the shard for the context_id, stream events in original append order (optionally narrowed by event type).</li>
<li><code>FLUSH</code>: force a MemTable flush to produce a new immutable segment (useful in tests/checkpoints).</li>
</ul>
<h2 id="what-runs-where"><a class="header" href="#what-runs-where">What runs where</a></h2>
<ul>
<li>Commands and flow control: command/parser, command/dispatcher, command/handlers.</li>
<li>Storage engine: engine/core/* for WAL, memory, segments, zones, filters; engine/store, engine/query, engine/replay.</li>
<li>Sharding and concurrency: engine/shard/* (manager, worker, messages).</li>
<li>Background work: engine/compactor/* for segment merging and cleanup.</li>
<li>Wiring and I/O: frontend/_ listeners; shared/_ for config, responses, logging.</li>
</ul>
<h2 id="key-guarantees-high-level"><a class="header" href="#key-guarantees-high-level">Key guarantees (high level)</a></h2>
<ul>
<li>Durability once a <code>STORE</code> is acknowledged (WAL first).</li>
<li>Immutability of events and on-disk segments (compaction replaces whole files, never edits in place).</li>
<li>Ordered replay per <code>context_id</code>.</li>
<li>Schema-validated payloads (strict by default, optional fields via union types).</li>
<li>Bounded memory via shard-local backpressure.</li>
</ul>
<h2 id="what-this-section-doesnt-do"><a class="header" href="#what-this-section-doesnt-do">What this section doesn’t do</a></h2>
<ul>
<li>It won’t dive into file formats or algorithmic details; those live in the focused pages that follow.</li>
<li>It won’t prescribe ops/production practices; see the development/operations parts of the book.</li>
</ul>
<h2 id="how-to-use-this-section"><a class="header" href="#how-to-use-this-section">How to use this section</a></h2>
<p>Skim this page, then jump to the piece you’re touching:</p>
<ul>
<li><a href="commands">Changing parsing or adding a command</a></li>
<li><a href="#storage-engine">Touching durability/flush/segment files</a></li>
<li><a href="#sharding">Threading, channels, and routing</a></li>
<li><a href="#query-and-replay">Anything read-path related</a></li>
<li><a href="#compaction">Background merging/policies</a></li>
<li><a href="#infrastructure">Config, responses, logging, tests</a></li>
</ul>
<p>That’s the map. Next pages keep the same tone and size: just enough to guide you to the right code.</p>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core concepts</a></h2>
<ul>
<li>Event: time-stamped, immutable fact with a typed payload</li>
<li>Event type &amp; schema: defined via DEFINE, validates payload shape</li>
<li>Context: groups related events under a context_id</li>
<li>Shard: independent pipeline — WAL → MemTable → Flush → Segments</li>
<li>WAL: per-shard durability log; replayed on startup</li>
<li>MemTable: in-memory buffer; flushed when full</li>
<li>Segment: immutable on-disk unit with columns, zones, filters, indexes</li>
<li>Zone: fixed-size block inside a segment with pruning metadata</li>
<li>Compaction: merges small segments to keep reads predictable</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="storage-engine"><a class="header" href="#storage-engine">Storage Engine</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>The storage engine turns incoming events into durable, immutable data you can query quickly. It’s built around append-only writes, in-memory buffering, and on-disk segments that are efficient to scan and easy to skip.</p>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<ul>
<li><strong>WAL (write-ahead log)</strong>: Per-shard durability log. Every accepted event is appended here first.</li>
<li><strong>MemTable</strong>: In-memory buffer for recent events. Fast inserts; swapped out when full.</li>
<li><strong>Flush worker</strong>: Converts a full MemTable into an immutable on-disk segment in the background.</li>
<li><strong>Segments</strong>: On-disk building blocks (columns, zone metadata, filters, lightweight indexes, index catalogs).</li>
<li><strong>Snapshots</strong>: Optional utility files (<code>.snp</code> events, <code>.smt</code> metadata) for export/replay and range bookkeeping.</li>
<li><strong>Materializations</strong>: Per-alias snapshots written by <code>REMEMBER QUERY</code> (catalog + frame files for remembered queries).</li>
<li><strong>Compactor</strong> (covered later): Merges small segments into larger ones to keep reads predictable.</li>
</ul>
<h2 id="write-path-at-a-glance"><a class="header" href="#write-path-at-a-glance">Write Path (At a Glance)</a></h2>
<ol>
<li>Validate payload against the event type schema.</li>
<li>Append to the WAL (durability point).</li>
<li>Apply to the MemTable (fast in-memory structure).</li>
<li>When the MemTable hits a threshold, swap it out and enqueue a background flush.</li>
<li>Flush worker writes a new segment and publishes it atomically.</li>
</ol>
<p>See the diagram below:</p>
<p><img src="design/storage.svg" alt="Write path"></p>
<h2 id="write-path-in-depth"><a class="header" href="#write-path-in-depth">Write Path (In Depth)</a></h2>
<h3 id="0-validate-the-event"><a class="header" href="#0-validate-the-event">0) Validate the event</a></h3>
<ul>
<li><strong>What</strong>: Check the incoming payload against the registered schema for its <code>event_type</code>.</li>
<li><strong>Why</strong>: Ensures only well-formed data enters the system so downstream files and indexes remain consistent.</li>
</ul>
<p>Example:</p>
<pre><code class="language-json">{
  "timestamp": 1700000000,
  "event_type": "signup",
  "context_id": "user-42",
  "payload": { "plan": "pro", "country": "US" }
}
</code></pre>
<p>Equivalent command:</p>
<pre><code class="language-sneldb">STORE signup FOR user-42 PAYLOAD {"plan":"pro","country":"US"}
</code></pre>
<p>Validation ensures required fields exist and types are correct (for example, the <code>event_type</code> is known and a “plan” is provided in the payload).</p>
<h3 id="1-append-to-the-wal-durability-point"><a class="header" href="#1-append-to-the-wal-durability-point">1) Append to the WAL (durability point)</a></h3>
<ul>
<li><strong>What</strong>: Append the validated event to the per-shard Write-Ahead Log (WAL).</li>
<li><strong>Why</strong>: Once the append returns, the event will survive a crash. On restart, the system replays WAL entries to rebuild in-memory state and complete any interrupted flushes.</li>
<li><strong>Notes</strong>:
<ul>
<li>WAL records are lightweight, line-oriented appends (JSON-serialized per line).</li>
<li>WAL files rotate in sync with the MemTable flush threshold (<code>engine.flush_threshold</code>), so replay windows are bounded by flush points. After a successful flush, older WAL files up to that cutoff can be pruned.</li>
<li>Behavior is tunable via config: <code>[wal] enabled, dir, buffered, buffer_size, flush_each_write, fsync, fsync_every_n</code> and <code>[engine] flush_threshold</code>.</li>
</ul>
</li>
</ul>
<p>Crash safety example:</p>
<ul>
<li>If the process crashes after the WAL append but before the event hits memory, recovery will re-insert it into the MemTable on startup.</li>
</ul>
<h3 id="2-insert-into-the-memtable-fast-in-memory-apply"><a class="header" href="#2-insert-into-the-memtable-fast-in-memory-apply">2) Insert into the MemTable (fast in-memory apply)</a></h3>
<ul>
<li><strong>What</strong>: Place the event into the in-memory, append-friendly, queryable buffer (MemTable).</li>
<li><strong>Why</strong>: Absorb writes in memory to batch them into large, sequential segment writes (avoids random I/O), maintain backpressure with bounded memory, and maximize ingest throughput. As a secondary benefit, new events are immediately visible to queries.</li>
<li><strong>Behavior</strong>:
<ul>
<li>The MemTable is sized by <code>flush_threshold</code> (config). When it reaches capacity, it triggers a swap and a background flush.</li>
<li>Inserts are grouped by context so the flusher can scan them quickly.</li>
</ul>
</li>
</ul>
<p>Small example:</p>
<ul>
<li><code>flush_threshold = 4</code></li>
<li>Incoming events (in order): A, B, C, D, E
<ul>
<li>A, B, C, D go into the active MemTable. After D, the MemTable is full.</li>
<li>A background flush is enqueued for these four; a fresh MemTable becomes active.</li>
<li>E enters the new MemTable immediately (no blocking on the background flush).</li>
</ul>
</li>
</ul>
<h3 id="3-swap-and-enqueue-a-background-flush"><a class="header" href="#3-swap-and-enqueue-a-background-flush">3) Swap and enqueue a background flush</a></h3>
<ul>
<li><strong>What</strong>: When the active MemTable is full, it’s atomically swapped for a fresh, empty one, and the full snapshot is queued for flushing.</li>
<li><strong>Why</strong>: Writers remain responsive (no long I/O in the foreground) and the system maintains bounded memory.</li>
<li><strong>Details</strong>:
<ul>
<li>The passive MemTable (now immutable) is handed off to the flush worker.</li>
<li>Writes proceed into the newly created active MemTable.</li>
</ul>
</li>
</ul>
<h3 id="4-flush-worker-writes-a-new-immutable-segment"><a class="header" href="#4-flush-worker-writes-a-new-immutable-segment">4) Flush worker writes a new immutable segment</a></h3>
<ul>
<li><strong>What</strong>: The background worker turns the passive MemTable into an on-disk segment directory (for example, <code>00042/</code>).</li>
<li>Inside the segment:
<ul>
<li><strong>Column files</strong>: One file per field, optimized for sequential appends and later memory-mapped (mmap) access. Naming: <code>&lt;uid&gt;_&lt;field&gt;.col</code>. Example: <code>u01_event_id.col</code>, <code>u01_timestamp.col</code>, <code>u01_event_type.col</code>, <code>u01_context_id.col</code>, <code>u01_plan.col</code>, <code>u01_country.col</code>. Where <code>&lt;uid&gt;</code> is defined per event type.</li>
<li><strong>Zone metadata</strong>: Per-zone min/max timestamps, row ranges, and presence stats for pruning.</li>
<li><strong>Filters/Indexes</strong> (policy-driven):
<ul>
<li>XOR: <code>&lt;uid&gt;_&lt;field&gt;.xf</code> (approximate membership)</li>
<li>Enum Bitmap (EBM): <code>&lt;uid&gt;_&lt;field&gt;.ebm</code> (eq/neq for enums)</li>
<li>Zone SuRF: <code>&lt;uid&gt;_&lt;field&gt;.zsrf</code> (succinct range filter for <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>)</li>
<li>Zone XOR Index: <code>&lt;uid&gt;_&lt;field&gt;.zxf</code> (per-zone XOR index for equality pruning)</li>
<li>Temporal Calendar: <code>&lt;uid&gt;_&lt;field&gt;.cal</code> (per-field day/hour → zone ids)</li>
<li>Temporal Index (slab): <code>&lt;uid&gt;_&lt;field&gt;.tfi</code> (per-field slab of per-zone temporal indexes)</li>
</ul>
</li>
<li><strong>Index Catalog</strong>: <code>{uid}.icx</code> describing which <code>IndexKind</code>s exist per field and globally for this segment.</li>
<li><strong>Filters/Indexes</strong> (policy-driven):
<ul>
<li>XOR: <code>&lt;uid&gt;_&lt;field&gt;.xf</code></li>
<li>Enum Bitmap (EBM): <code>&lt;uid&gt;_&lt;field&gt;.ebm</code></li>
<li>Zone SuRF: <code>&lt;uid&gt;_&lt;field&gt;.zsrf</code></li>
<li>Zone XOR Index: <code>&lt;uid&gt;_&lt;field&gt;.zxf</code></li>
</ul>
</li>
<li><strong>Index Catalog</strong>: <code>{uid}.icx</code> (binary header + bincode <code>SegmentIndexCatalog</code>) recording available <code>IndexKind</code>s per field and globally for the segment.</li>
<li><strong>Offsets/Index</strong>: Per-zone compressed offsets (<code>.zfc</code> files) describing compressed block ranges and in-block offsets.</li>
</ul>
</li>
<li>
<ul>
<li><strong>Snapshots</strong> (optional):</li>
</ul>
</li>
<li>
<ul>
<li>Event Snapshots (<code>.snp</code>): portable arrays of events with a binary header + length‑prefixed JSON entries.</li>
</ul>
</li>
<li>
<ul>
<li>Snapshot Metadata (<code>.smt</code>): arrays of <code>{uid, context_id, from_ts, to_ts}</code> entries with a binary header + length‑prefixed JSON.</li>
</ul>
</li>
<li><strong>Publication</strong>: Segment creation is atomic at the directory level; once complete, readers can discover and scan it.</li>
</ul>
<p>See the diagram below:</p>
<p><img src="design/uid.svg" alt="UID storage example"></p>
<p>Sizing example:</p>
<ul>
<li><code>flush_threshold = 32_768</code></li>
<li><code>events_per_zone = 2_048</code></li>
<li>A full flush of 32,768 events creates exactly 16 zones. Each zone has its own metadata and contributes field values to the filter files. Larger <code>events_per_zone</code> values reduce metadata overhead but offer coarser pruning; smaller values increase pruning precision at the cost of more metadata.</li>
</ul>
<h3 id="5-cleanup-and-wal-compaction"><a class="header" href="#5-cleanup-and-wal-compaction">5) Cleanup and WAL compaction</a></h3>
<ul>
<li><strong>What</strong>: After a successful flush, the system can prune or rotate old WAL files up to a cutoff corresponding to flushed data.</li>
<li><strong>Why</strong>: Keeps recovery time short and disk usage bounded.</li>
</ul>
<h3 id="end-to-end-write-example"><a class="header" href="#end-to-end-write-example">End-to-end write example</a></h3>
<ol>
<li>Client sends <code>STORE signup ...</code> with a valid payload.</li>
<li>The engine validates the event against the <code>signup</code> schema.</li>
<li>The event is appended to the WAL for shard 3 (durability).</li>
<li>The event is inserted into shard 3’s active MemTable.</li>
<li>When the MemTable reaches <code>flush_threshold</code>, it is swapped and the old one is queued for the background flush.</li>
<li>The flush worker writes <code>00137/</code> with column files, 16 zones (if 32,768/2,048), zone metadata, policy-selected filters/indexes (XF/EBM/ZSf/ZXF), an Index Catalog <code>{uid}.icx</code>, and offsets/index.</li>
<li>Once published, queries immediately see the segment alongside any newer in-memory events.</li>
<li>The WAL up to (and including) the flushed range is now safe to compact or rotate.</li>
</ol>
<h3 id="failure-model-write-path"><a class="header" href="#failure-model-write-path">Failure model (write path)</a></h3>
<ul>
<li><strong>Crash before WAL append</strong>: The event is lost (not acknowledged).</li>
<li><strong>Crash after WAL append but before MemTable insert</strong>: The event is recovered from the WAL and re-applied on startup.</li>
<li><strong>Crash after MemTable insert but before flush</strong>: The event is not yet in a segment, but it is durable in the WAL. On restart, WAL replay restores it to the MemTable; if a swap occurred and a passive MemTable existed, its contents are reconstructed from WAL as well. No data loss; no duplicate segments.</li>
<li><strong>Crash during flush</strong>: The WAL still contains the flushed events; on restart, the system replays or completes the flush. Partially written segments are ignored until a valid, fully published segment is present.</li>
</ul>
<h3 id="tuning-the-write-path"><a class="header" href="#tuning-the-write-path">Tuning the write path</a></h3>
<ul>
<li><strong>shards</strong>: More shards increase parallelism of WAL, MemTable, and flush pipelines (at the cost of more intense CPU and RAM usage and more files and directories).</li>
<li><strong><code>flush_threshold</code></strong>: Controls MemTable size. Higher values reduce flush frequency (bigger segments) but increase peak memory and WAL replay cost.</li>
<li><strong><code>events_per_zone</code></strong>: Smaller values improve pruning for reads but increase metadata and filter counts. Pick based on query selectivity and typical field cardinalities.</li>
</ul>
<h2 id="durability--recovery"><a class="header" href="#durability--recovery">Durability &amp; Recovery</a></h2>
<ul>
<li>Covered in the write path: WAL append is the durability point; replay restores MemTables; WAL rotation keeps recovery bounded. See Failure model above.</li>
</ul>
<h2 id="backpressure--safety"><a class="header" href="#backpressure--safety">Backpressure &amp; Safety</a></h2>
<ul>
<li><strong>Bounded channels</strong> between components provide backpressure under load (writers slow down instead of exhausting memory).</li>
<li><strong>Async workers</strong> (flush and compaction) are throttled so foreground writes and reads stay responsive.</li>
</ul>
<p>This is the spine of the engine: durable append, fast memory, immutable segments with rich metadata, and just enough background work to keep reads snappy as data grows.</p>
<h2 id="policy-driven-index-build-write-time"><a class="header" href="#policy-driven-index-build-write-time">Policy-driven index build (write-time)</a></h2>
<ul>
<li>Index builds are determined by an <code>IndexBuildPolicy</code> and an <code>IndexBuildPlanner</code> that produce a per-field <code>BuildPlan</code> of <code>IndexKind</code> bitflags and global kinds.</li>
<li><code>ZoneWriter</code> consumes the plan to build only the requested artifacts; legacy catch‑all builders were removed in favor of filtered builders (e.g., <code>build_all_filtered</code>).</li>
<li>RLTE (if enabled) is included via the policy and emitted best‑effort.</li>
</ul>
<h2 id="read-time-catalogs-and-planning"><a class="header" href="#read-time-catalogs-and-planning">Read-time catalogs and planning</a></h2>
<ul>
<li>Each segment’s <code>{uid}.icx</code> is loaded (and cached) into a <code>SegmentIndexCatalog</code>.</li>
<li><code>IndexRegistry</code> aggregates catalogs across segments; <code>IndexPlanner</code> chooses an explicit <code>IndexStrategy</code> per filter based on available kinds and the schema.</li>
<li>Strategy selection uses a representative segment that actually has a catalog; if no catalog/kinds exist for a field/segment, the planner chooses <code>FullScan</code> to avoid filesystem probing.
<ul>
<li>Temporal strategies are field-aware: <code>TemporalEq { field }</code> and <code>TemporalRange { field }</code> use the per-field calendar and slabbed temporal index for both the fixed <code>timestamp</code> and payload <code>datetime</code> fields (e.g., <code>created_at</code>).</li>
</ul>
</li>
</ul>
<h2 id="read-time-projection--column-pruning"><a class="header" href="#read-time-projection--column-pruning">Read-time Projection &amp; Column Pruning</a></h2>
<ul>
<li>The query planner derives a minimal column set to load based on:
<ul>
<li>Core fields: <code>context_id</code>, <code>event_type</code>, <code>timestamp</code> (always loaded)</li>
<li>Filter fields used in <code>WHERE</code></li>
<li>Requested payload fields from <code>RETURN [ ... ]</code> (if provided)</li>
</ul>
</li>
<li>If <code>RETURN</code> is omitted or empty (<code>RETURN []</code>), all payload fields are considered eligible.</li>
<li>Unknown fields in <code>RETURN</code> are ignored (schema-driven).</li>
<li>Only the selected columns are mmap’d and read; others are skipped entirely, reducing I/O and memory.</li>
<li>Projection decisions are logged under the <code>query::projection</code> target for debugging.</li>
</ul>
<h2 id="materialized-query-store"><a class="header" href="#materialized-query-store">Materialized Query Store</a></h2>
<ul>
<li><strong>Materialized Query Store</strong>
<ul>
<li>Directory structure: <code>materializations/catalog.bin</code> + <code>materializations/&lt;alias&gt;/frames/NNNNNN.mat</code> + <code>manifest.bin</code>.</li>
<li>Catalog entries store the serialized query spec (JSON in bincode), schema snapshot, byte/row counters, high-water mark, retention hints, and telemetry deltas.</li>
<li>Each frame is typed row storage with a shared header + LZ4-compressed payload (null bitmap + typed values) so it can be streamed back verbatim.</li>
<li>SHOW streams existing frames, runs a delta pipeline using the stored high-water mark, appends new frames, and updates catalog metadata.</li>
<li>Retention policies (max rows / max age) are persisted but currently optional; when present, frames are pruned after each append.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flush-pipeline"><a class="header" href="#flush-pipeline">Flush Pipeline</a></h1>
<h2 id="why-it-matters"><a class="header" href="#why-it-matters">Why it matters</a></h2>
<p>Every event that lands in SnelDB takes a short trip: it is accepted into memory, logged for durability, written to disk later, then published for queries. The flush pipeline is the bridge between “in memory” and “on disk”. Recent changes (segment lifecycle tracking, flush progress IDs, SHOW barriers) tightened the guarantees in this path. This doc walks through the flow in plain language.</p>
<h2 id="cast-of-components"><a class="header" href="#cast-of-components">Cast of components</a></h2>
<ul>
<li><strong>MemTable</strong> – In-memory buffer per shard. Fast inserts, bounded by <code>flush_threshold</code>.</li>
<li><strong>PassiveBufferSet</strong> – Holds recently rotated MemTables so reads can still see them while they flush.</li>
<li><strong>FlushProgress</strong> – Hands out monotonic “flush tickets” and records when each one finishes.</li>
<li><strong>FlushManager</strong> – Lightweight dispatcher that queues flush jobs onto a shard-local worker.</li>
<li><strong>FlushWorker</strong> – Async worker that writes segment files, verifies them, and cleans up.</li>
<li><strong>SegmentLifecycleTracker</strong> – Tracks each flushing segment (Flushing → Written → Verified) and owns the associated passive buffer.</li>
<li><strong>segment_ids</strong> – Shared list of published segments per shard; queries rely on it to discover data.</li>
<li><strong>SHOW / AwaitFlush</strong> – The consumer of the new barrier logic; waits for specific flush tickets to complete before running delta queries.</li>
</ul>
<h2 id="timeline-high-level"><a class="header" href="#timeline-high-level">Timeline (high level)</a></h2>
<ol>
<li><strong>MemTable rotation</strong> – Inserts keep filling the active MemTable. When it hits capacity we clone it into the passive set, reserve the next segment ID, and request a flush ticket from <code>FlushProgress</code>.</li>
<li><strong>Queue the job</strong> – <code>FlushManager::queue_for_flush</code> sends the passive buffer, schema handle, segment ID, and ticket to the shard’s <code>FlushWorker</code>.</li>
<li><strong>Write &amp; verify</strong> – The worker registers the flush with <code>SegmentLifecycleTracker</code>, writes the segment, verifies it, and only then appends the zero-padded segment name to <code>segment_ids</code>.</li>
<li><strong>Lifecycle cleanup</strong> – Once verification succeeds, the tracker marks the segment <code>Verified</code>, returns the passive buffer, and WAL files up to that point are pruned.</li>
<li><strong>Ticket complete</strong> – The worker calls <code>flush_progress.mark_completed(ticket)</code> and notifies any waiter (explicit FLUSH command, tests, SHOW barrier).</li>
<li><strong>SHOW barrier</strong> – When SHOW asks every shard to <code>AwaitFlush</code>, the worker snapshots the latest ticket number and waits only until <code>completed()</code> catches up. New writes can keep issuing higher tickets without blocking the wait.</li>
</ol>
<h2 id="1-memtable-rotation--ticketing"><a class="header" href="#1-memtable-rotation--ticketing">1. MemTable rotation &amp; ticketing</a></h2>
<ul>
<li>The rotation happens inside <code>insert_and_maybe_flush</code>.</li>
<li>Key steps:
<ol>
<li>Copy the full MemTable into an <code>Arc&lt;Mutex&lt;MemTable&gt;&gt;</code> inside <code>PassiveBufferSet</code>.</li>
<li>Swap in a fresh MemTable so inserts never block on disk.</li>
<li>Fetch <code>flush_id = ctx.flush_progress.next_id()</code> – this is the ticket tied to the current passive buffer.</li>
<li>Enqueue the job via <code>FlushManager</code>.</li>
</ol>
</li>
<li>Why tickets? They let downstream consumers refer to “all flushes that were already in flight when I started waiting” without affecting future writes.</li>
</ul>
<h2 id="2-dispatch-via-flushmanager"><a class="header" href="#2-dispatch-via-flushmanager">2. Dispatch via FlushManager</a></h2>
<ul>
<li>Each shard owns a <code>FlushManager</code> with a bounded async channel to its <code>FlushWorker</code>.</li>
<li><code>queue_for_flush</code> only logs intent and pushes the payload (segment id, memtable, passive buffer, schema handle, ticket, optional completion sender).</li>
<li>No disk work happens here, which keeps ingestion latency low.</li>
</ul>
<h2 id="3-flushworker-responsibilities"><a class="header" href="#3-flushworker-responsibilities">3. FlushWorker responsibilities</a></h2>
<ol>
<li>
<p><strong>Register lifecycle</strong></p>
<ul>
<li>For non-empty memtables we call <code>SegmentLifecycleTracker::register_flush(segment_id, passive_buffer)</code> and mark the phase as <code>Flushing</code>.</li>
</ul>
</li>
<li>
<p><strong>Write the segment</strong></p>
<ul>
<li>A <code>Flusher</code> writes column files, filters, metadata, etc. This runs under a shard-level <code>flush_coordination_lock</code> so index catalogs stay consistent.</li>
</ul>
</li>
<li>
<p><strong>Verify before publishing</strong></p>
<ul>
<li><code>SegmentVerifier</code> tries to open the just-written files (with retries). Only once verification succeeds do we:
<ul>
<li><code>segment_ids.write().push(format!("{:05}", segment_id))</code></li>
<li><code>SegmentLifecycleTracker::mark_written</code> followed by <code>mark_verified</code></li>
<li><code>SegmentLifecycleTracker::clear_and_complete</code> to flush the passive buffer copy</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Cleanup</strong></p>
<ul>
<li><code>WalCleaner</code> prunes WAL files up to the segment’s boundary.</li>
</ul>
</li>
<li>
<p><strong>Error handling</strong></p>
<ul>
<li>Failures skip the <code>segment_ids</code> update and leave the passive buffer registered so the data can be retried after restart. The WAL still contains the events, so nothing is lost.</li>
</ul>
</li>
<li>
<p><strong>Ticket completion</strong></p>
<ul>
<li>Regardless of success, <code>flush_progress.mark_completed(ticket)</code> fires, then any optional oneshot completion channel is notified. Tickets therefore form a strict happens-after chain for SHOW barriers.</li>
</ul>
</li>
</ol>
<h2 id="4-segmentlifecycletracker-in-practice"><a class="header" href="#4-segmentlifecycletracker-in-practice">4. SegmentLifecycleTracker in practice</a></h2>
<ul>
<li>Lifecycle phases:
<ul>
<li><code>Flushing</code> – passive buffer is registered; files are being written.</li>
<li><code>Written</code> – files exist on disk but haven’t been verified.</li>
<li><code>Verified</code> – files passed verification; passive buffer can be cleared.</li>
</ul>
</li>
<li>Why we need it:
<ul>
<li>Queries can safely merge results from passive buffers and published segments without seeing double or missing rows.</li>
<li>Passive buffers only disappear once the durable copy is proven healthy.</li>
<li>If a flush fails midway, the tracker still owns the buffer so it can be retried or replayed from WAL.</li>
</ul>
</li>
</ul>
<h2 id="5-flushprogress--show-barriers"><a class="header" href="#5-flushprogress--show-barriers">5. FlushProgress &amp; SHOW barriers</a></h2>
<ul>
<li><code>FlushProgress</code> holds two atomics: <code>submitted</code> (last ticket handed out) and <code>completed</code> (highest ticket confirmed finished).</li>
<li>When SHOW wants a consistent snapshot before running its delta query:
<ol>
<li>Each shard receives <code>ShardMessage::AwaitFlush</code>.</li>
<li>The worker snapshots <code>target = flush_progress.snapshot()</code>.</li>
<li>It waits (polling) until <code>flush_progress.completed() &gt;= target</code>.</li>
<li>If the shard was idle, the wait returns immediately.</li>
</ol>
</li>
<li>Advantages of this model:
<ul>
<li>SHOW doesn’t force new flushes; it simply waits for the already queued ones.</li>
<li>Continuous ingest doesn’t starve the wait, because any tickets issued <em>after</em> the snapshot are ignored for that request.</li>
<li>Any other subsystem can reuse the same barrier semantics (e.g., shutdown, diagnostics).</li>
</ul>
</li>
</ul>
<h2 id="6-failure-resilience-recap"><a class="header" href="#6-failure-resilience-recap">6. Failure resilience recap</a></h2>
<ul>
<li>WAL keeps every event durable until its segment is safely written and verified; only then do we prune.</li>
<li><code>segment_ids</code> only references directories that already exist, which prevents CI-only races where fast readers tried to mmap files that weren’t there yet.</li>
<li><code>FlushProgress</code> ensures explicit waits and user-facing commands can deterministically block on flushes without busy-looping or issuing redundant work.</li>
<li><code>SegmentLifecycleTracker</code> is the guardrail that keeps passive buffers alive until their on-disk counterparts are proven good.</li>
</ul>
<h2 id="7-reader-side-guards-while-indexes-settle"><a class="header" href="#7-reader-side-guards-while-indexes-settle">7. Reader-side guards while indexes settle</a></h2>
<p>Once the writer path is safe, we still need to shield queries from “half-published” segments:</p>
<ul>
<li><strong>InflightSegments</strong> – every segment ID goes into an inflight tracker the moment it’s queued for flush and drops out only after verification. Zone planning merges these inflight IDs with the published <code>segment_ids</code> list so readers don’t miss freshly flushed data.</li>
<li><strong>UID-aware fallbacks</strong> – selectors (<code>collect_zones_for_scope</code>, <code>FieldSelector</code>, <code>IndexZoneSelector</code>) now check whether a segment actually hosts the requested event-type UID (via index catalogs, <code>.zones</code> metadata, or inflight status) before falling back to “scan all zones”. Segments that never contained that UID are skipped entirely; they no longer fabricate empty candidate zones that lead to <code>row_count: 0</code>.</li>
<li><strong>Targeted full scans</strong> – when an index file is genuinely missing for a <em>real</em> segment (e.g., the flush is still writing <code>.idx</code>), we still honor the configured <code>MissingIndexPolicy</code>, but only after the UID check passes. This keeps the safety net for in-progress flushes without paying the cost on unrelated shards.</li>
</ul>
<p>Together these guards mean readers either (a) see the verified files, (b) rely on the passive buffer via inflight detection, or (c) briefly scan real zones for that UID. They never waste work hydrating segments that could not possibly contain the event type.</p>
<h2 id="mental-model"><a class="header" href="#mental-model">Mental model</a></h2>
<p>Think of each memtable rotation as handing a “package” plus a numbered receipt to the flush conveyor belt. The worker assembles the package, checks it, places it on the shelf (segment_ids), cleans the workbench (WAL), and finally stamps the receipt as done. Any customer (SHOW) can walk up to the desk, note the latest receipt number, and wait until all packages up to that number are on the shelf. New packages keep flowing without blocking anyone.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="query-and-replay"><a class="header" href="#query-and-replay">Query and Replay</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>SnelDB reads come in two flavors:</p>
<ul>
<li><code>QUERY</code>: filter one event type by predicates, time, and optional <code>context_id</code>; may span shards.</li>
<li><code>REPLAY</code>: stream all events for one <code>context_id</code> (optionally one type) in original append order; single shard.</li>
</ul>
<p>Both use the same internals as the write path: in‑memory MemTable, on‑disk immutable segments, per‑segment zones, and compact per‑field filters.</p>
<h2 id="when-to-use-which"><a class="header" href="#when-to-use-which">When to Use Which</a></h2>
<ul>
<li>Use <code>QUERY</code> for analytics, debugging slices, and ad‑hoc filters across many contexts.</li>
<li>Use <code>REPLAY</code> to rebuild state or audit the exact sequence for one context.</li>
</ul>
<h3 id="examples-6"><a class="header" href="#examples-6">Examples</a></h3>
<ul>
<li>
<p>QUERY</p>
<ul>
<li>Investigate: “All <code>order_created</code> over $100 in the last 24h across all users”</li>
<li>Dashboard: “Errors by type this week”</li>
<li>Debug: “Sessions with <code>status = 'pending'</code> and <code>retries &gt; 3</code>”</li>
</ul>
</li>
<li>
<p>REPLAY</p>
<ul>
<li>Operational debugging (incident timeline)
<pre><code class="language-sneldb">REPLAY system_event FOR host-123 SINCE "2024-05-01T00:00:00Z"
</code></pre>
</li>
<li>Auditing/compliance (full account trail)
<pre><code class="language-sneldb">REPLAY FOR account-42 SINCE "2024-01-01T00:00:00Z"
</code></pre>
</li>
<li>ML pipelines (rebuild a customer’s transaction sequence)
<pre><code class="language-sneldb">REPLAY transaction FOR user-456 SINCE "2023-01-01T00:00:00Z"
</code></pre>
</li>
<li>Product journey (single user or session in order)
<pre><code class="language-sneldb">REPLAY FOR user-123
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="command-cheatsheet"><a class="header" href="#command-cheatsheet">Command Cheatsheet</a></h2>
<pre><code class="language-sneldb">QUERY &lt;event_type&gt; [FOR &lt;context_id&gt;] [SINCE &lt;ts&gt;] [WHERE &lt;expr&gt;] [LIMIT &lt;n&gt;]
</code></pre>
<pre><code class="language-sneldb">REPLAY [&lt;event_type&gt;] FOR &lt;context_id&gt; [SINCE &lt;ts&gt;]
</code></pre>
<p>More examples: <a href="#query">Query</a> and <a href="#replay">Replay</a></p>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<h3 id="query-stepbystep"><a class="header" href="#query-stepbystep">QUERY (step‑by‑step)</a></h3>
<ol>
<li>
<p>Parse and validate inputs.</p>
</li>
<li>
<p>Plan shard tasks (fan‑out unless narrowed by <code>context_id</code>).</p>
</li>
<li>
<p>Per shard, scan MemTable and pick relevant segments.</p>
</li>
<li>
<p>Prune zones by time and per‑field filters; read only needed columns.</p>
<ul>
<li>Range predicates (<code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>) are pruned using Zone SuRF (<code>{uid}_{field}.zsrf</code>) when present, falling back to XOR/EBM only if unavailable. SuRF is an order‑preserving trie using succinct arrays for fast range overlap checks.</li>
<li>Equality predicates (<code>=</code>, <code>IN</code>) use Zone XOR indexes (<code>{uid}_{field}.zxf</code>) for fast zone lookup.</li>
<li>Complex WHERE clauses with parentheses, AND/OR/NOT are transformed into a FilterGroup tree, and zones are combined using set operations (intersection for AND, union for OR, complement for NOT). See <a href="#filter-architecture-and-zone-collection">Filter Architecture</a> for details.</li>
</ul>
</li>
<li>
<p>Evaluate predicates and apply <code>WHERE</code> condition.</p>
</li>
<li>
<p>If aggregations are present:</p>
<ul>
<li>Build an aggregation plan (ops, optional group_by, optional time bucket and selected time field).</li>
<li>In each shard, update aggregators from both MemTable (row path) and segments (columnar path). Segment scans project only needed columns (filters, group_by, time field, agg inputs).</li>
<li>Group keys combine optional time bucket with <code>group_by</code> values; a fast prehash accelerates hashmap grouping.</li>
<li>Merge partial aggregation states across shards; finalize into a table (bucket? + group columns + metric columns). <code>LIMIT</code> caps distinct groups.</li>
</ul>
<p>Otherwise (selection path):</p>
<ul>
<li>Merge rows; apply global <code>LIMIT</code> if set.</li>
</ul>
</li>
</ol>
<h3 id="sequence-queries-stepbystep"><a class="header" href="#sequence-queries-stepbystep">Sequence Queries (step‑by‑step)</a></h3>
<p>Sequence queries (<code>FOLLOWED BY</code>, <code>PRECEDED BY</code>, <code>LINKED BY</code>) follow a specialized path optimized for finding ordered event pairs:</p>
<ol>
<li><strong>Parse sequence</strong>: Extract event types, link field, and sequence operator from the query.</li>
<li><strong>Parallel zone collection</strong>: Collect zones for all event types in parallel across shards. Each event type gets its own query plan with transformed WHERE clauses (event-prefixed fields like <code>page_view.page</code> become <code>page</code> for the <code>page_view</code> plan).</li>
<li><strong>Index strategy assignment</strong>: Assign index strategies to filter plans so zone XOR indexes are used for field filters.</li>
<li><strong>Zone hydration</strong>: Load column values (including the <code>link_field</code>) without materializing events.</li>
<li><strong>Grouping</strong>: Group row indices by <code>link_field</code> value using columnar data. Within each group, sort by timestamp.</li>
<li><strong>Matching</strong>: Apply the two-pointer algorithm to find matching sequences:
<ul>
<li>For <code>FOLLOWED BY</code>: find events where <code>event_type_b</code> occurs at the same timestamp or later</li>
<li>For <code>PRECEDED BY</code>: find events where <code>event_type_b</code> occurred strictly before</li>
<li>Apply WHERE clause filters during matching to avoid materializing non-matching events</li>
</ul>
</li>
<li><strong>Materialization</strong>: Only materialize events from matched sequences, using <code>EventBuilder</code> and <code>PreparedAccessor</code> for efficient construction.</li>
</ol>
<p><strong>Performance optimizations</strong>:</p>
<ul>
<li>Columnar processing avoids premature event materialization</li>
<li>Early filtering reduces the search space before grouping</li>
<li>Parallel zone collection for different event types</li>
<li>Index usage for <code>link_field</code> and <code>event_type</code> filters</li>
<li>Limit short-circuiting stops processing once enough matches are found</li>
</ul>
<h3 id="replay-stepbystep"><a class="header" href="#replay-stepbystep">REPLAY (step‑by‑step)</a></h3>
<ol>
<li>Parse and validate inputs.</li>
<li>Route to the shard owning the <code>context_id</code>.</li>
<li>Scan MemTable and relevant segments for that context.</li>
<li>Apply optional <code>event_type</code> and <code>SINCE</code> filters.</li>
<li>Stream events in original append order.</li>
</ol>
<p>See the diagram:</p>
<p><img src="design/query.svg" alt="Query and Replay flow"></p>
<h2 id="what-you-get"><a class="header" href="#what-you-get">What You Get</a></h2>
<ul>
<li><strong>Visibility</strong>: fresh writes are visible from <code>MemTable</code> before flush.</li>
<li><strong>Ordering</strong>: <code>REPLAY</code> preserves append order (single shard). <code>QUERY</code> has no global ordering unless you explicitly sort at merge (costly) or scope the query narrowly.</li>
<li><strong>LIMIT</strong> (<code>QUERY</code>): short‑circuit per shard when possible; always cap globally during merge.</li>
</ul>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<ul>
<li><strong>Prune early</strong>: favor <code>event_type</code>, <code>context_id</code>, and <code>SINCE</code> to skip zones fast.</li>
<li><strong>Shard wisely</strong>: more shards increase scan parallelism but cost more on fan‑out.</li>
</ul>
<h2 id="tuning"><a class="header" href="#tuning">Tuning</a></h2>
<ul>
<li><code>events_per_zone</code>: smaller zones = better pruning, more metadata; larger zones = fewer skips, less metadata.</li>
<li><code>flush_threshold</code>: affects how much is in memory vs on disk, and segment cadence.</li>
<li>Shard count: match to CPU and expected concurrency.</li>
</ul>
<h2 id="invariants"><a class="header" href="#invariants">Invariants</a></h2>
<ul>
<li>Immutability: events and segments are never edited in place.</li>
<li>Single‑shard replay: each <code>context_id</code> maps to exactly one shard.</li>
<li>Schema validity: stored payloads conform to their event type schema.</li>
<li>Atomic publication: new segments become visible all‑or‑nothing.</li>
</ul>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li><a href="#storage-engine">Read flow overview</a></li>
<li><a href="#filter-architecture-and-zone-collection">Filter Architecture and Zone Collection</a></li>
<li><a href="architecture/segments_zones.html">Segments and zones</a></li>
</ul>
<p>SnelDB’s read path is simple to reason about: prune aggressively, read only what you need, and merge efficiently—whether you’re slicing across many contexts or replaying one.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="filter-architecture-and-zone-collection"><a class="header" href="#filter-architecture-and-zone-collection">Filter Architecture and Zone Collection</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>SnelDB’s filter system efficiently prunes zones (fixed-size data blocks) before reading column data, dramatically reducing I/O for complex queries. The architecture transforms user queries into a logical filter tree, collects candidate zones for each filter, and combines them using set operations (intersection for AND, union for OR) to determine which zones to scan.</p>
<h2 id="core-concepts-1"><a class="header" href="#core-concepts-1">Core Concepts</a></h2>
<ul>
<li><strong>FilterGroup</strong>: A tree structure representing the logical structure of WHERE clauses (AND, OR, NOT, individual filters)</li>
<li><strong>Zone Collection</strong>: The process of identifying candidate zones that might contain matching data</li>
<li><strong>Zone Combination</strong>: Set operations (intersection/union) to combine zones from multiple filters</li>
<li><strong>Index Strategy</strong>: How a filter is applied (ZoneXorIndex for equality, ZoneSuRF for ranges, FullScan as fallback)</li>
</ul>
<h2 id="filtergroup-structure"><a class="header" href="#filtergroup-structure">FilterGroup Structure</a></h2>
<p>The <code>FilterGroup</code> enum preserves the logical structure from WHERE clauses:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum FilterGroup {
    Filter { column, operation, value, ... },
    And(Vec&lt;FilterGroup&gt;),
    Or(Vec&lt;FilterGroup&gt;),
    Not(Box&lt;FilterGroup&gt;),
}
<span class="boring">}</span></code></pre>
<p><strong>Example</strong>: The query <code>WHERE (status = "active" OR status = "pending") AND priority &gt; 4</code> becomes:</p>
<pre><code>And([
    Or([
        Filter { column: "status", operation: Eq, value: "active" },
        Filter { column: "status", operation: Eq, value: "pending" }
    ]),
    Filter { column: "priority", operation: Gt, value: 4 }
])
</code></pre>
<h2 id="query-transformation-pipeline"><a class="header" href="#query-transformation-pipeline">Query Transformation Pipeline</a></h2>
<h3 id="1-expression-parsing"><a class="header" href="#1-expression-parsing">1. Expression Parsing</a></h3>
<p>The PEG parser converts the WHERE clause into an <code>Expr</code> tree:</p>
<pre><code class="language-sneldb">QUERY orders WHERE id IN (1, 2, 3) AND status = "active"
</code></pre>
<p>Becomes:</p>
<pre><code>And(
    In { field: "id", values: [1, 2, 3] },
    Compare { field: "status", op: Eq, value: "active" }
)
</code></pre>
<h3 id="2-filtergroup-building"><a class="header" href="#2-filtergroup-building">2. FilterGroup Building</a></h3>
<p><code>FilterGroupBuilder</code> transforms <code>Expr</code> → <code>FilterGroup</code> with optimizations:</p>
<h4 id="in-operator-expansion"><a class="header" href="#in-operator-expansion">IN Operator Expansion</a></h4>
<p><code>IN</code> operators are expanded into <code>OR</code> of equality filters for efficient zone collection:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// id IN (1, 2, 3) becomes:
Or([
    Filter { column: "id", operation: Eq, value: 1 },
    Filter { column: "id", operation: Eq, value: 2 },
    Filter { column: "id", operation: Eq, value: 3 }
])
<span class="boring">}</span></code></pre>
<p><strong>Why</strong>: Each equality can use <code>ZoneXorIndex</code> for fast zone lookup, then zones are unioned.</p>
<h4 id="or-equality-expansion"><a class="header" href="#or-equality-expansion">OR Equality Expansion</a></h4>
<p>Multiple equality comparisons on the same field are automatically expanded:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// status = "active" OR status = "pending" becomes:
Or([
    Filter { column: "status", operation: Eq, value: "active" },
    Filter { column: "status", operation: Eq, value: "pending" }
])
<span class="boring">}</span></code></pre>
<p><strong>Why</strong>: Same optimization as IN—each equality uses an index, then union.</p>
<h4 id="or-flattening"><a class="header" href="#or-flattening">OR Flattening</a></h4>
<p>Nested OR structures are flattened to avoid unnecessary tree depth:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// OR(A, OR(B, C)) becomes OR(A, B, C)
<span class="boring">}</span></code></pre>
<p><strong>Why</strong>: Simplifies zone combination logic and improves performance.</p>
<h3 id="3-zone-collection"><a class="header" href="#3-zone-collection">3. Zone Collection</a></h3>
<p><code>ZoneCollector</code> orchestrates zone collection:</p>
<ol>
<li><strong>Extract unique filters</strong>: Deduplicate filters to avoid redundant zone lookups</li>
<li><strong>Build zone cache</strong>: For each unique filter, collect candidate zones from all segments</li>
<li><strong>Combine zones</strong>: Use <code>ZoneGroupCollector</code> to traverse the FilterGroup tree and combine zones</li>
</ol>
<p><strong>Example</strong>: For <code>(status = "active" OR status = "pending") AND priority &gt; 4</code>:</p>
<ul>
<li>Collect zones for <code>status = "active"</code> → <code>[zone_1, zone_3]</code></li>
<li>Collect zones for <code>status = "pending"</code> → <code>[zone_2, zone_4]</code></li>
<li>Collect zones for <code>priority &gt; 4</code> → <code>[zone_2, zone_3, zone_5]</code></li>
<li>Combine: <code>OR([zone_1, zone_3], [zone_2, zone_4])</code> = <code>[zone_1, zone_2, zone_3, zone_4]</code></li>
<li>Then: <code>AND([zone_1, zone_2, zone_3, zone_4], [zone_2, zone_3, zone_5])</code> = <code>[zone_2, zone_3]</code></li>
</ul>
<h2 id="smart-not-handling"><a class="header" href="#smart-not-handling">Smart NOT Handling</a></h2>
<p>NOT operations require special handling because “NOT matching” means “all zones except matching zones.”</p>
<h3 id="notfilter"><a class="header" href="#notfilter">NOT(Filter)</a></h3>
<p>For a single filter, compute the complement:</p>
<ol>
<li>Get all zones for all segments in the query</li>
<li>Collect zones matching the filter</li>
<li>Return: <code>all_zones - matching_zones</code></li>
</ol>
<p><strong>Example</strong>: <code>NOT status = "active"</code> returns all zones except those containing <code>status = "active"</code>.</p>
<h3 id="notand---de-morgans-law"><a class="header" href="#notand---de-morgans-law">NOT(AND) - De Morgan’s Law</a></h3>
<p>Transform using De Morgan’s law: <code>NOT(A AND B)</code> = <code>NOT A OR NOT B</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// NOT(status = "active" AND priority &gt; 4) becomes:
Or([
    Not(Filter { status = "active" }),
    Not(Filter { priority &gt; 4 })
])
<span class="boring">}</span></code></pre>
<p>Then each <code>NOT(Filter)</code> computes its complement, and zones are unioned.</p>
<h3 id="notor---de-morgans-law"><a class="header" href="#notor---de-morgans-law">NOT(OR) - De Morgan’s Law</a></h3>
<p>Transform using De Morgan’s law: <code>NOT(A OR B)</code> = <code>NOT A AND NOT B</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// NOT(status = "active" OR status = "pending") becomes:
And([
    Not(Filter { status = "active" }),
    Not(Filter { status = "pending" })
])
<span class="boring">}</span></code></pre>
<p>Then each <code>NOT(Filter)</code> computes its complement, and zones are intersected.</p>
<h3 id="notnot-x---double-negation"><a class="header" href="#notnot-x---double-negation">NOT(NOT X) - Double Negation</a></h3>
<p>Double negation is eliminated: <code>NOT NOT X</code> = <code>X</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// NOT NOT status = "active" becomes:
Filter { status = "active" }
<span class="boring">}</span></code></pre>
<h2 id="zone-combination-logic"><a class="header" href="#zone-combination-logic">Zone Combination Logic</a></h2>
<p><code>ZoneGroupCollector</code> recursively traverses the FilterGroup tree:</p>
<h3 id="and-combination"><a class="header" href="#and-combination">AND Combination</a></h3>
<p>Intersect zones from all children:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// AND(A, B, C): intersect zones from A, B, and C
// Early exit: if any child has no zones, return empty
<span class="boring">}</span></code></pre>
<p><strong>Example</strong>: <code>AND([zone_1, zone_2], [zone_2, zone_3])</code> = <code>[zone_2]</code></p>
<h3 id="or-combination"><a class="header" href="#or-combination">OR Combination</a></h3>
<p>Union zones from all children:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// OR(A, B, C): union zones from A, B, and C
<span class="boring">}</span></code></pre>
<p><strong>Example</strong>: <code>OR([zone_1, zone_2], [zone_2, zone_3])</code> = <code>[zone_1, zone_2, zone_3]</code> (deduplicated)</p>
<h3 id="not-combination"><a class="header" href="#not-combination">NOT Combination</a></h3>
<p>Compute complement (see Smart NOT Handling above).</p>
<h2 id="index-strategies"><a class="header" href="#index-strategies">Index Strategies</a></h2>
<p>Each filter is assigned an index strategy based on the operation and field type:</p>
<ul>
<li><strong>ZoneXorIndex</strong>: Equality comparisons (<code>=</code>, <code>IN</code>) on indexed fields</li>
<li><strong>ZoneSuRF</strong>: Range comparisons (<code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>) on indexed fields</li>
<li><strong>FullScan</strong>: Fallback when no index is available</li>
</ul>
<p><strong>Example</strong>: <code>priority &gt; 4</code> uses <code>ZoneSuRF</code> to find zones with <code>priority</code> values greater than 4.</p>
<h2 id="performance-optimizations"><a class="header" href="#performance-optimizations">Performance Optimizations</a></h2>
<h3 id="filter-deduplication"><a class="header" href="#filter-deduplication">Filter Deduplication</a></h3>
<p>Duplicate filters are collected only once:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// WHERE status = "active" AND status = "active"
// Only collects zones once for status = "active"
<span class="boring">}</span></code></pre>
<h3 id="zone-cache"><a class="header" href="#zone-cache">Zone Cache</a></h3>
<p>Zones are cached per filter key to avoid redundant lookups:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cache key: "status:Eq:active"
// Zones: [zone_1, zone_3]
<span class="boring">}</span></code></pre>
<h3 id="early-exit-for-and"><a class="header" href="#early-exit-for-and">Early Exit for AND</a></h3>
<p>If any child of an AND has no zones, return empty immediately:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// AND(A, B) where A has no zones → return [] immediately
// No need to collect zones for B
<span class="boring">}</span></code></pre>
<h3 id="cross-segment-zone-intersection"><a class="header" href="#cross-segment-zone-intersection">Cross-Segment Zone Intersection</a></h3>
<p>AND operations intersect zones by both <code>zone_id</code> and <code>segment_id</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Zone from segment_1, zone_2 AND zone from segment_2, zone_2
// Do NOT intersect (different segments)
<span class="boring">}</span></code></pre>
<h2 id="examples-7"><a class="header" href="#examples-7">Examples</a></h2>
<h3 id="simple-and"><a class="header" href="#simple-and">Simple AND</a></h3>
<pre><code class="language-sneldb">QUERY orders WHERE status = "active" AND priority &gt; 5
</code></pre>
<ol>
<li>Build FilterGroup: <code>And([Filter(status="active"), Filter(priority&gt;5)])</code></li>
<li>Collect zones: <code>status="active"</code> → <code>[zone_1, zone_3]</code>, <code>priority&gt;5</code> → <code>[zone_2, zone_3]</code></li>
<li>Intersect: <code>[zone_3]</code></li>
<li>Scan only <code>zone_3</code> for both filters</li>
</ol>
<h3 id="in-with-and"><a class="header" href="#in-with-and">IN with AND</a></h3>
<pre><code class="language-sneldb">QUERY orders WHERE id IN (1, 2, 3) AND status = "active"
</code></pre>
<ol>
<li>Expand IN: <code>Or([Filter(id=1), Filter(id=2), Filter(id=3)])</code></li>
<li>Build FilterGroup: <code>And([Or([...]), Filter(status="active")])</code></li>
<li>Collect zones: <code>id=1</code> → <code>[zone_1]</code>, <code>id=2</code> → <code>[zone_2]</code>, <code>id=3</code> → <code>[zone_3]</code>, <code>status="active"</code> → <code>[zone_1, zone_3]</code></li>
<li>Union IN zones: <code>[zone_1, zone_2, zone_3]</code></li>
<li>Intersect with status: <code>[zone_1, zone_3]</code></li>
</ol>
<h3 id="complex-parentheses"><a class="header" href="#complex-parentheses">Complex Parentheses</a></h3>
<pre><code class="language-sneldb">QUERY orders WHERE ((status = "active" OR status = "pending") AND priority &gt; 4) OR category = "A"
</code></pre>
<ol>
<li>Build FilterGroup:
<pre><code>Or([
    And([
        Or([Filter(status="active"), Filter(status="pending")]),
        Filter(priority&gt;4)
    ]),
    Filter(category="A")
])
</code></pre>
</li>
<li>Collect zones for each filter</li>
<li>Combine: Inner OR → union, then AND → intersect, then outer OR → union</li>
</ol>
<h3 id="not-operation"><a class="header" href="#not-operation">NOT Operation</a></h3>
<pre><code class="language-sneldb">QUERY orders WHERE NOT status = "active"
</code></pre>
<ol>
<li>Build FilterGroup: <code>Not(Filter(status="active"))</code></li>
<li>Get all zones: <code>[zone_0, zone_1, zone_2, zone_3]</code></li>
<li>Get matching zones: <code>status="active"</code> → <code>[zone_1, zone_3]</code></li>
<li>Compute complement: <code>[zone_0, zone_2]</code></li>
</ol>
<h2 id="invariants-1"><a class="header" href="#invariants-1">Invariants</a></h2>
<ul>
<li><strong>Zone uniqueness</strong>: Zones are deduplicated by <code>(zone_id, segment_id)</code> before combination</li>
<li><strong>Filter deduplication</strong>: Identical filters (same column, operation, value) are collected only once</li>
<li><strong>Early exit</strong>: AND operations return empty immediately if any child has no zones</li>
<li><strong>Complement correctness</strong>: NOT operations correctly compute all zones minus matching zones</li>
<li><strong>De Morgan’s laws</strong>: NOT(AND) and NOT(OR) are correctly transformed</li>
</ul>
<h2 id="further-reading-1"><a class="header" href="#further-reading-1">Further Reading</a></h2>
<ul>
<li><a href="#query-and-replay">Query and Replay</a> - High-level query execution flow</li>
<li><a href="#storage-engine">Storage Engine</a> - Zone structure and segment layout</li>
<li><a href="engine/core/read/index_strategy.rs">Index Strategies</a> - How filters use indexes</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="streaming-query-flow"><a class="header" href="#streaming-query-flow">Streaming Query Flow</a></h1>
<h2 id="what-this-page-is"><a class="header" href="#what-this-page-is">What this page is</a></h2>
<ul>
<li>A walk-through of the new streaming read path, from command dispatch to shard merges.</li>
<li>Enough detail to map runtime behavior to the coordinating and shard-side code.</li>
</ul>
<h2 id="when-we-stream"><a class="header" href="#when-we-stream">When we stream</a></h2>
<ul>
<li>Plain <code>QUERY</code> commands including aggregations, grouping, and time buckets (aggregate queries always use streaming).</li>
<li>Sequence queries use a specialized streaming path.</li>
<li>Triggered by the HTTP/TCP/WebSocket <code>QUERY</code> handler when the caller asks for streaming (e.g. client-side backpressure, large scans where batching the whole result is impractical).</li>
<li>Falls back to the existing batch pipeline only for non-aggregate selection queries when streaming is disabled or unavailable.</li>
</ul>
<h2 id="pipeline-overview"><a class="header" href="#pipeline-overview">Pipeline overview</a></h2>
<ol>
<li>
<p><strong>Coordinator</strong> (<code>QueryExecutionPipeline::execute_streaming</code>)</p>
<ul>
<li>Planner builds a <code>PlanOutcome</code> exactly like the batch path (zone picks, per-shard filters, limits).</li>
<li><code>StreamingShardDispatcher</code> fans out a <code>ShardMessage::QueryStream</code> to every shard, bundling the plan fragments.</li>
</ul>
</li>
<li>
<p><strong>Shard execution</strong> (<code>scan_streaming</code> → <code>StreamingScan</code>)</p>
<ul>
<li>Each shard rebuilds a <code>QueryPlan</code>, then initializes a <code>StreamingContext</code> (plan, passive snapshot, caches, <code>FlowContext</code>, effective limit = <code>limit + offset</code>).</li>
<li><code>FlowBuilders</code> produces up to two flows:
<ul>
<li><code>memtable_flow</code> wraps the active memtable plus passive buffers via <code>MemTableQueryRunner::stream</code>.</li>
<li><code>segment_flow</code> calls <code>build_segment_stream</code> to launch a background <code>SegmentQueryRunner</code> streaming columnar batches.</li>
</ul>
</li>
<li><code>ShardFlowMerger</code> fuses those flows. If the command carries an <code>ORDER BY</code>, it spawns an ordered heap merge; otherwise, it fan-ins the channels. The result is a <code>ShardFlowHandle</code> (receiver + schema + background tasks).</li>
</ul>
</li>
<li>
<p><strong>Coordinator merge &amp; delivery</strong></p>
<ul>
<li>The dispatcher hands the <code>ShardFlowHandle</code>s to the merge layer (<code>StreamMergerKind</code>).</li>
<li>For aggregate queries: <code>AggregateStreamMerger</code> collects partial aggregate batches from all shards, merges them by group key, applies ORDER BY/LIMIT/OFFSET, and emits finalized results.</li>
<li><code>OrderedStreamMerger</code> uses the flow-level ordered merger to respect <code>ORDER BY field [DESC]</code>, honouring <code>LIMIT/OFFSET</code> at the coordinator (for non-aggregate queries).</li>
<li><code>UnorderedStreamMerger</code> forwards batches as they arrive when no ordering is requested (for non-aggregate queries).</li>
<li><code>QueryBatchStream</code> wraps the merged receiver. Dropping it aborts all shard/background tasks to avoid leaks.</li>
</ul>
</li>
</ol>
<h2 id="where-to-look-in-code"><a class="header" href="#where-to-look-in-code">Where to look in code</a></h2>
<ul>
<li>Coordinator entry: <code>src/command/handlers/mod.rs</code>, <code>query/orchestrator.rs</code> (<code>execute_streaming</code>).</li>
<li>Dispatch: <code>src/command/handlers/query/dispatch/streaming.rs</code>.</li>
<li>Merge: <code>src/command/handlers/query/merge/streaming.rs</code>, <code>src/command/handlers/query/merge/aggregate_stream.rs</code>, <code>query_batch_stream.rs</code>.</li>
<li>Shard message + worker: <code>src/engine/shard/message.rs</code>, <code>src/engine/shard/worker.rs</code>.</li>
<li>Shard read pipeline: <code>src/engine/query/streaming/{scan.rs,context.rs,builders.rs,merger.rs}</code>.</li>
<li>Flow primitives (channels, batches, ordered merge): <code>src/engine/core/read/flow/</code> (notably <code>context.rs</code>, <code>channel.rs</code>, <code>ordered_merger.rs</code>, <code>shard_pipeline.rs</code>).</li>
</ul>
<h2 id="operational-notes-1"><a class="header" href="#operational-notes-1">Operational notes</a></h2>
<ul>
<li><strong>Aggregate queries always use the streaming path</strong> - they cannot fall back to batch execution. Each shard produces partial aggregates via <code>AggregateOp</code> that are merged at the coordinator using <code>AggregateStreamMerger</code>.</li>
<li>AVG aggregations preserve sum and count throughout the streaming pipeline, ensuring accurate merging across shards/segments. The average is only finalized at the coordinator when emitting results.</li>
<li>COUNT UNIQUE aggregations preserve the actual unique values (as JSON array strings) throughout the streaming pipeline, ensuring accurate merging across shards/segments. The count is only finalized at the coordinator when emitting results.</li>
<li><code>StreamingContext</code> snapshots passive buffers at creation; long-lived streams do not see newer passive flushes until a new stream is opened.</li>
<li>Flow channels are bounded (default 32k rows per batch) to provide natural backpressure; coordinator-side consumers should <code>recv</code> promptly.</li>
<li>If any shard fails while constructing the stream, the dispatcher surfaces a shard-specific error and aborts the entire streaming request.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aggregations-1"><a class="header" href="#aggregations-1">Aggregations</a></h1>
<h2 id="what-this-page-is-1"><a class="header" href="#what-this-page-is-1">What this page is</a></h2>
<ul>
<li>A focused description of how aggregation queries are parsed, planned, and executed.</li>
<li>Enough detail to find code and reason about behavior without reading internals first.</li>
</ul>
<h2 id="command-surface"><a class="header" href="#command-surface">Command surface</a></h2>
<ul>
<li>Metrics: <code>COUNT</code>, <code>COUNT UNIQUE &lt;field&gt;</code>, <code>COUNT &lt;field&gt;</code>, <code>TOTAL &lt;field&gt;</code>, <code>AVG &lt;field&gt;</code>, <code>MIN &lt;field&gt;</code>, <code>MAX &lt;field&gt;</code></li>
<li>Grouping: <code>BY &lt;field&gt; [, &lt;field&gt; ...]</code></li>
<li>Time bucketing: <code>PER HOUR|DAY|WEEK|MONTH [USING &lt;time_field&gt;]</code></li>
<li>Time selection: <code>USING &lt;time_field&gt;</code> (also affects SINCE and pruning)</li>
<li>Limit groups: <code>LIMIT &lt;n&gt;</code> caps distinct groups emitted</li>
</ul>
<p>See: Commands → Query for examples.</p>
<h2 id="flow-stepbystep"><a class="header" href="#flow-stepbystep">Flow (step‑by‑step)</a></h2>
<ol>
<li>
<p>Parse: PEG grammar recognizes aggregation specs, grouping, and optional time bucketing/USING.</p>
</li>
<li>
<p>Plan: <code>AggregatePlan</code> captures ops, <code>group_by</code>, <code>time_bucket</code>.</p>
<ul>
<li>If aggregating, the implicit SINCE filter may be removed from filter plans to avoid truncating buckets; explicit SINCE remains honored.</li>
<li>Projection strategy adds: non-core filter columns, <code>group_by</code> fields, selected time field for bucketing, and input fields for requested metrics.</li>
</ul>
</li>
<li>
<p>Execute per shard (streaming path):</p>
<ul>
<li>MemTable events: streamed via <code>MemTableSource</code> → batches → <code>AggregateOp</code> → <code>AggregateSink</code>.</li>
<li>Segments: <code>SegmentQueryRunner</code> streams columnar batches → <code>AggregateOp</code> → <code>AggregateSink</code>.</li>
<li>Group key = (optional time bucket(ts, granularity, using time_field), ordered group_by values). A precomputed hash accelerates grouping.</li>
<li>Optional group limit prevents creating new groups beyond <code>LIMIT</code> but continues to update existing ones.</li>
<li>Each shard emits partial aggregate batches (intermediate schema with sum/count for AVG, JSON arrays for COUNT UNIQUE).</li>
</ul>
</li>
<li>
<p>Merge and finalize:</p>
<ul>
<li><code>AggregateStreamMerger</code> collects partial aggregate batches from all shards.</li>
<li>Partial states are merged across shards per group key using <code>AggState::merge</code>.</li>
<li>Final table columns: optional <code>bucket</code>, group_by fields, then metric columns (e.g., <code>count</code>, <code>count_unique*&lt;field&gt;</code>, <code>total*&lt;field&gt;</code>, <code>avg*&lt;field&gt;</code>, <code>min*&lt;field&gt;</code>, <code>max*&lt;field&gt;</code>).</li>
<li>AVG aggregations preserve sum and count throughout the pipeline (as <code>avg_{field}_sum</code> and <code>avg_{field}_count</code> columns) and only finalize to an average at the coordinator, ensuring accurate merging across shards/segments.</li>
<li>COUNT UNIQUE aggregations preserve the actual unique values (as JSON array strings) throughout the pipeline and only finalize the count at the coordinator.</li>
<li>ORDER BY and LIMIT/OFFSET are applied at the coordinator after merging all shard results.</li>
</ul>
</li>
</ol>
<h2 id="where-to-look-in-code-1"><a class="header" href="#where-to-look-in-code-1">Where to look in code</a></h2>
<ul>
<li>Parse: <code>src/command/parser/commands/query.rs</code> (agg_clause, group_clause, time_clause)</li>
<li>Plan: <code>src/engine/core/read/query_plan.rs</code>, <code>src/engine/core/read/aggregate/plan.rs</code></li>
<li>Projection: <code>src/engine/core/read/projection/strategies.rs</code> (AggregationProjection)</li>
<li>Execution (streaming): <code>src/engine/core/read/flow/operators/aggregate.rs</code> (<code>AggregateOp</code>), <code>src/engine/core/read/flow/shard_pipeline.rs</code> (<code>build_segment_stream</code>)</li>
<li>Sink (grouping): <code>src/engine/core/read/sink/aggregate/</code> (<code>sink.rs</code>, <code>group_key.rs</code>, <code>time_bucketing.rs</code>)</li>
<li>Merge/finalize: <code>src/command/handlers/query/merge/aggregate_stream.rs</code> (<code>AggregateStreamMerger</code>)</li>
<li>Result formatting: <code>src/engine/core/read/result.rs</code> (<code>AggregateResult::finalize</code>)</li>
</ul>
<h2 id="invariants--notes"><a class="header" href="#invariants--notes">Invariants &amp; notes</a></h2>
<ul>
<li>Time bucketing uses calendar-aware or naive bucketing (configurable) for stable edges.</li>
<li>COUNT ALL-only queries still load a core column to determine zone sizes.</li>
<li>LIMIT on aggregation limits group cardinality, not scanned events.</li>
<li>Aggregate queries always use the streaming execution path for efficient processing and accurate merging across shards.</li>
<li>AVG aggregations maintain sum and count separately during shard processing and merge these partial states accurately at the coordinator before finalizing to the average value.</li>
<li>COUNT UNIQUE aggregations maintain the actual unique values (as JSON arrays) during shard processing and merge these sets accurately at the coordinator before finalizing to the count.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sharding"><a class="header" href="#sharding">Sharding</a></h1>
<h2 id="what-it-is-1"><a class="header" href="#what-it-is-1">What it is</a></h2>
<p>Sharding is how SnelDB scales ingestion and keeps per-context replay efficient. Instead of one big pipeline, the system runs multiple shard workers side by side. Each <code>context_id</code> is deterministically mapped to a shard, so all events for that context live together.</p>
<h2 id="core-pieces"><a class="header" href="#core-pieces">Core pieces</a></h2>
<ul>
<li>Shard Manager — owns all shards and routes work to them by hashing <code>context_id</code>.</li>
<li>Shard (worker) — long‑lived task that owns a WAL, active/passive MemTables, a flush queue, and the shard’s segment list. Processes Store, Query, Replay, and Flush messages.</li>
<li>Messages — typed messages delivered to each shard: Store, Query, Replay, Flush.</li>
<li>Backpressure — each shard has a bounded mailbox; when it fills, senders wait. Hot shards slow down independently without affecting others.</li>
</ul>
<p><img src="design/sharding.svg" alt="Sharding overview"></p>
<h2 id="how-it-works-2"><a class="header" href="#how-it-works-2">How it works</a></h2>
<ul>
<li>
<p>Startup</p>
<ul>
<li>The manager creates <code>N</code> shards (configurable) and starts one worker per shard.</li>
<li>Each shard ensures its storage directories exist, recovers its MemTable from its WAL, loads existing segment IDs, and starts background services (flush, compaction).</li>
</ul>
</li>
<li>
<p>Store</p>
<ul>
<li>Hash <code>context_id</code> → pick shard → send Store.</li>
<li>The shard appends to its WAL, updates the in‑memory MemTable, and, when the MemTable reaches its threshold, rotates it to a passive buffer and enqueues a flush.</li>
</ul>
</li>
<li>
<p>Query</p>
<ul>
<li>Broadcast to all shards. Each shard scans its in‑memory state and on‑disk segments and returns matches. Results are merged.</li>
</ul>
</li>
<li>
<p>Replay</p>
<ul>
<li>Single‑shard. The manager routes to the shard that owns the <code>context_id</code>. The shard streams events in order for that context.</li>
</ul>
</li>
<li>
<p>Flush</p>
<ul>
<li>Manual <code>Flush</code> is broadcast to all shards. Each shard rotates its active MemTable and enqueues a flush to create a new segment.</li>
<li>Automatic flush also occurs when a shard’s MemTable reaches its configured threshold during ingestion.</li>
</ul>
</li>
</ul>
<h2 id="why-this-design"><a class="header" href="#why-this-design">Why this design</a></h2>
<ul>
<li><strong>Locality</strong>: all events for a <code>context_id</code> stay on one shard → fast, single‑shard replay.</li>
<li><strong>Parallelism</strong>: shards work independently → ingestion and queries scale with cores.</li>
<li><strong>Isolation</strong>: hot shards apply backpressure locally without stalling the whole system.</li>
<li><strong>Simplicity</strong>: shards don’t coordinate directly; only query results are merged.</li>
</ul>
<h2 id="invariants-2"><a class="header" href="#invariants-2">Invariants</a></h2>
<ul>
<li>Same <code>context_id</code> → always the same shard.</li>
<li>Within a shard, event order per <code>context_id</code> is preserved.</li>
<li>Shards never share mutable state; cross‑shard communication happens via message passing and result merging.</li>
</ul>
<h2 id="operational-notes-2"><a class="header" href="#operational-notes-2">Operational notes</a></h2>
<ul>
<li>Number of shards controls parallelism; increase to utilize more CPU cores.</li>
<li>Flush threshold tunes memory usage vs. write amplification; lower values flush more often.</li>
<li>On startup, shards recover from their WALs before serving traffic; compaction runs in the background to control segment growth.</li>
</ul>
<h2 id="further-reading-2"><a class="header" href="#further-reading-2">Further Reading</a></h2>
<ul>
<li>A deep dive into WAL or flush internals (see <a href="#storage-engine">Storage Engine</a>).</li>
<li>Query planning details (see <a href="#query-and-replay">Query &amp; Replay</a>).</li>
<li>Compaction policies (see <a href="#compaction">Compaction</a>).</li>
</ul>
<p>Sharding is the concurrency backbone: it divides the work, keeps replay cheap, and prevents overload by applying backpressure shard by shard.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="infrastructure"><a class="header" href="#infrastructure">Infrastructure</a></h1>
<p>SnelDB isn’t just a storage engine — it needs the scaffolding around it to feel safe, predictable, and easy to integrate. That’s what the infrastructure layer provides.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>Every system needs a single source of truth for its settings.
SnelDB loads a configuration once at startup and makes it available everywhere. This means:</p>
<ul>
<li>Consistency — all components (server, engine, WAL, logging) read from the same snapshot.</li>
<li>Flexibility — settings can be changed through a config file or environment variable without recompiling.</li>
<li>Safety — startup fails fast if something critical is missing or invalid.</li>
</ul>
<p>Think of it as the contract between how you run SnelDB and how the engine behaves.</p>
<h2 id="logging"><a class="header" href="#logging">Logging</a></h2>
<p>Logs are the “black box recorder” of SnelDB. They serve two purposes:</p>
<ul>
<li>For operators: real-time feedback in the console (levels like info/debug/warn).</li>
<li>For long-term visibility: structured logs rotated daily on disk.</li>
</ul>
<p>The philosophy is simple: logs should be human-readable, lightweight, and always available when you need to explain “what just happened.”</p>
<h2 id="responses"><a class="header" href="#responses">Responses</a></h2>
<p>Every command produces a response. SnelDB keeps them minimal and predictable:</p>
<ul>
<li>A clear status code (OK, BadRequest, NotFound, InternalError).</li>
<li>A message for humans.</li>
<li>A body that can be either lines (for CLI-like tools) or structured JSON arrays (for programmatic use).</li>
</ul>
<p>Two renderers handle the output: one friendly for terminals, one clean for machines. This way, SnelDB speaks both languages without complicating the core.</p>
<h2 id="why-it-matters-1"><a class="header" href="#why-it-matters-1">Why it matters</a></h2>
<p>These pieces aren’t “extra code” — they’re the glue that makes SnelDB usable in the real world:</p>
<ul>
<li>Configuration means you can run the same binary in development, staging, and production with confidence.</li>
<li>Logging means you can trust the system to tell you what it’s doing, even when things go wrong.</li>
<li>Responses mean every client, from shell scripts to dashboards, gets consistent feedback.</li>
</ul>
<p>Together, they provide the operational safety net: when you store events, you know how to configure it, you see what’s happening, and you get a clear answer back.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="compaction"><a class="header" href="#compaction">Compaction</a></h1>
<h2 id="what-it-is-2"><a class="header" href="#what-it-is-2">What it is</a></h2>
<p>Compaction keeps reads predictable as data grows. Instead of editing files in place, SnelDB periodically merges small, freshly-flushed segments into larger, cleaner ones. This reduces file count, tightens zone metadata, and improves pruning—without touching the logical history of events.</p>
<h2 id="why-it-matters-2"><a class="header" href="#why-it-matters-2">Why it matters</a></h2>
<ul>
<li>Fewer segments → fewer seeks and better cache behavior.</li>
<li>Larger, well-formed zones → more “skip work” during queries.</li>
<li>Stable tail latencies as ingestion continues day after day.</li>
</ul>
<h2 id="how-it-runs-big-picture"><a class="header" href="#how-it-runs-big-picture">How it runs (big picture)</a></h2>
<ul>
<li>One background task per process coordinates compaction across shards with a global concurrency limit (configurable). Shards are compacted concurrently up to the limit; within a shard, work runs serially.</li>
<li>Periodically checks system IO pressure; if the system is busy, it skips.</li>
<li>Uses a policy to plan compaction (k-way by uid, multi-level); if the policy yields plans, a worker runs them and publishes new segments atomically.</li>
<li>Plans are grouped by shared input segments to enable efficient multi-UID compaction in a single pass.</li>
</ul>
<h2 id="shard-local-by-design"><a class="header" href="#shard-local-by-design">Shard-local by design</a></h2>
<p>Each shard compacts its own segments. This keeps the work isolated, prevents cross-shard coordination, and preserves the “all events for a context live together” property.</p>
<h2 id="when-it-triggers"><a class="header" href="#when-it-triggers">When it triggers</a></h2>
<ul>
<li>Only if the k-way policy finds any merge plans for the shard (no threshold counter anymore).</li>
<li>Skips entirely if IO pressure is high to avoid hurting foreground work.</li>
</ul>
<h2 id="safety--correctness"><a class="header" href="#safety--correctness">Safety &amp; correctness</a></h2>
<ul>
<li>Segments are immutable; compaction writes new files and then swaps pointers in one step.</li>
<li>If a run fails, nothing is partially applied; the old segments remain authoritative.</li>
<li>Reads continue throughout—queries see either the old set or the new set, never a half state.</li>
<li>Replay order and event immutability are unaffected.</li>
</ul>
<h2 id="resource-awareness"><a class="header" href="#resource-awareness">Resource awareness</a></h2>
<ul>
<li>The loop samples system state (disks/IO) before running.</li>
<li>Under pressure, the compactor yields to ingestion and queries.</li>
<li>This protects P99 read latencies and avoids “compaction storms.”</li>
</ul>
<h2 id="what-the-worker-does-conceptually"><a class="header" href="#what-the-worker-does-conceptually">What the worker does (conceptually)</a></h2>
<ul>
<li>Groups segments by UID and level, then chunks them into batches of size <code>k</code> (config).</li>
<li>For each batch, processes all UIDs from the same input segments together in a single pass (multi-UID compaction).</li>
<li>Performs k-way merges of events sorted by <code>context_id</code> for each UID.</li>
<li>Rebuilds zones at a level-aware target size: <code>events_per_zone * fill_factor * (level+1)</code>.</li>
<li>Emits new segments at the next level (L0→L1, L1→L2, etc.) with correct naming, updates the segment index, and removes inputs from the index.</li>
<li>Leftover segments (those that don’t form a complete batch of <code>k</code>) accumulate across cycles rather than being force-compacted immediately.</li>
</ul>
<h2 id="operator-knobs"><a class="header" href="#operator-knobs">Operator knobs</a></h2>
<ul>
<li><code>segments_per_merge</code>: number of segments to merge per output batch (applies to all levels).</li>
<li><code>compaction_max_shard_concurrency</code>: max shards compacted simultaneously (default 1 = serial across shards).</li>
<li><code>sys_io_threshold</code> (and related IO heuristics): how conservative to be under load.</li>
<li><code>events_per_zone</code> and <code>fill_factor</code>: base and multiplier for zone sizing; higher levels multiply by <code>(level+1)</code>.</li>
</ul>
<h2 id="leftover-handling"><a class="header" href="#leftover-handling">Leftover handling</a></h2>
<ul>
<li>Segments that don’t form a complete batch of <code>k</code> are left to accumulate across compaction cycles.</li>
<li>When accumulated leftovers reach a threshold of approximately <code>(k * 2) / 3</code>, they are force-compacted to prevent indefinite accumulation.</li>
<li>This less aggressive approach reduces compaction overhead while still maintaining predictable read performance.</li>
</ul>
<h2 id="invariants-3"><a class="header" href="#invariants-3">Invariants</a></h2>
<ul>
<li>No in-place mutation; only append/replace at the segment set level.</li>
<li>Queries stay available and correct while compaction runs.</li>
<li>Failures are contained to the background task; foreground paths remain healthy.</li>
<li>Multi-UID compaction ensures all UIDs from shared input segments are written to a single output segment, maintaining data locality.</li>
<li>Atomic segment index updates ensure consistency: output segments are verified to exist before the index is updated.</li>
</ul>
<h2 id="what-this-page-is-not"><a class="header" href="#what-this-page-is-not">What this page is not</a></h2>
<ul>
<li>A file-format spec or merge algorithm walkthrough.</li>
<li>A policy recipe for every workload. The defaults aim for good general behavior; heavy write or read-mostly deployments may tune the thresholds differently.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="layering-strategy-in-sneldb"><a class="header" href="#layering-strategy-in-sneldb">Layering Strategy in SnelDB</a></h1>
<p>This page gives a high-level view of how SnelDB is structured. It focuses on what each layer does and how requests flow through the system.</p>
<h2 id="layer-1-frontend--transport-and-connections"><a class="header" href="#layer-1-frontend--transport-and-connections">Layer 1: <code>frontend</code> — Transport and Connections</a></h2>
<ul>
<li>Listens for client connections (e.g., Unix/TCP/HTTP/WebSocket).</li>
<li>Reads requests and writes responses.</li>
<li>Hands off parsing and execution to the <code>command</code> and <code>engine</code> layers.</li>
</ul>
<h2 id="layer-2-command--parse-and-dispatch"><a class="header" href="#layer-2-command--parse-and-dispatch">Layer 2: <code>command</code> — Parse and Dispatch</a></h2>
<ul>
<li>Parses user input (e.g., <code>DEFINE</code>, <code>STORE</code>, <code>QUERY</code>).</li>
<li>Validates and turns text into typed commands.</li>
<li>Dispatches to the appropriate operation in the engine.</li>
</ul>
<h2 id="layer-3-engine--core-logic"><a class="header" href="#layer-3-engine--core-logic">Layer 3: <code>engine</code> — Core Logic</a></h2>
<ul>
<li>Implements the main behaviors: define schemas, store events, run queries, replay, and flush.</li>
<li>Chooses the right shard and updates on-disk data as needed.</li>
<li>Stays independent from how clients connect or send requests.</li>
</ul>
<h2 id="layer-4-shared--common-utilities"><a class="header" href="#layer-4-shared--common-utilities">Layer 4: <code>shared</code> — Common Utilities</a></h2>
<ul>
<li>Configuration and response types used across layers.</li>
<li>Logging setup and other small shared helpers.</li>
</ul>
<h2 id="flow-summary-store-example"><a class="header" href="#flow-summary-store-example">Flow Summary (STORE example)</a></h2>
<ol>
<li>Frontend receives a request.</li>
<li><code>command</code> parses and validates it.</li>
<li>The dispatcher routes to the correct engine operation.</li>
<li><code>engine</code> executes and updates storage.</li>
<li>A response is returned to the client.</li>
</ol>
<h2 id="why-this-layering"><a class="header" href="#why-this-layering">Why this layering?</a></h2>
<ul>
<li>Clean separation: parsing, logic, and transport are independent.</li>
<li>Easy to test: engine logic can be tested without real sockets.</li>
<li>Scales well: clear boundaries support growth and optimization.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="threading-and-async"><a class="header" href="#threading-and-async">Threading and Async</a></h1>
<h2 id="what-it-is-3"><a class="header" href="#what-it-is-3">What it is</a></h2>
<ul>
<li>Networking is handled with async tasks (Tokio) for each client connection.</li>
<li>Work is executed by per-shard worker tasks, communicated via message passing.</li>
<li>This separates I/O from data processing and keeps shard state isolated.</li>
</ul>
<h2 id="core-pieces-1"><a class="header" href="#core-pieces-1">Core pieces</a></h2>
<ul>
<li>Frontends — Unix/TCP/HTTP/WebSocket listeners accept connections and spawn a task per client.</li>
<li>Connection — reads lines, parses commands, and dispatches them for execution.</li>
<li>Shard Manager — owns shards and routes work by hashing <code>context_id</code>.</li>
<li>Shard (worker) — long‑lived task that owns WAL, MemTables, flush queue, and segment list; handles Store, Query, Replay, Flush.</li>
<li>Channels — <code>tokio::sync::mpsc</code> for sending typed messages to shards.</li>
<li>Schema Registry — shared via <code>Arc&lt;tokio::sync::RwLock&lt;SchemaRegistry&gt;&gt;</code>.</li>
</ul>
<h2 id="how-it-works-3"><a class="header" href="#how-it-works-3">How it works</a></h2>
<ul>
<li>
<p>Startup</p>
<ul>
<li>Initialize the schema registry and shard manager.</li>
<li>Bind a Unix listener and start accepting connections.</li>
<li>Spawn background workers (flush, compaction) per shard.</li>
</ul>
</li>
<li>
<p>Connection handling</p>
<ul>
<li>Spawn a task per client.</li>
<li>Read lines, parse into commands, dispatch to the shard manager.</li>
</ul>
</li>
<li>
<p>Store</p>
<ul>
<li>Route to shard by <code>context_id</code>.</li>
<li>Append to WAL, update active MemTable; rotate and enqueue flush when needed.</li>
</ul>
</li>
<li>
<p>Query</p>
<ul>
<li>Broadcast to all shards.</li>
<li>Each shard scans its in‑memory and on‑disk state and returns matches; results are merged.</li>
</ul>
</li>
<li>
<p>Replay</p>
<ul>
<li>Route to the shard for the <code>context_id</code>.</li>
<li>Stream events in original append order for that context.</li>
</ul>
</li>
<li>
<p>Flush</p>
<ul>
<li>Broadcast; shards rotate MemTables and enqueue flush to produce a new segment.</li>
</ul>
</li>
</ul>
<h2 id="why-this-design-1"><a class="header" href="#why-this-design-1">Why this design</a></h2>
<ul>
<li>Async I/O: efficient, scalable handling of many connections.</li>
<li>Shard workers: clear ownership and predictable performance.</li>
<li>Separation of concerns: networking and storage logic don’t intermingle.</li>
</ul>
<h2 id="invariants-4"><a class="header" href="#invariants-4">Invariants</a></h2>
<ul>
<li>Frontends do not perform disk I/O or modify indexes directly.</li>
<li>Shard workers own shard state; cross‑shard mutable sharing is avoided.</li>
<li>Schema access uses async <code>RwLock</code> for safe concurrent reads/writes.</li>
</ul>
<h2 id="operational-notes-3"><a class="header" href="#operational-notes-3">Operational notes</a></h2>
<ul>
<li>Bounded shard mailboxes apply local backpressure; tune channel sizes as needed.</li>
<li>Number of shards controls parallelism; size to match CPU/core availability.</li>
<li>Monitor channel depth and lock contention to spot hotspots.</li>
</ul>
<h2 id="further-reading-3"><a class="header" href="#further-reading-3">Further Reading</a></h2>
<ul>
<li><a href="#sharding">Sharding</a></li>
<li><a href="#storage-engine">Storage Engine</a></li>
<li><a href="#query-and-replay">Query &amp; Replay</a></li>
<li><a href="#infrastructure">Infrastructure</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="logging-1"><a class="header" href="#logging-1">Logging</a></h1>
<h2 id="what-it-is-4"><a class="header" href="#what-it-is-4">What it is</a></h2>
<ul>
<li>SnelDB uses the <code>tracing</code> ecosystem for structured, leveled logs.</li>
<li>Logs are emitted to stdout and to a daily‑rotated file, with independent levels.</li>
<li>Levels and output directory are configured via the config file.</li>
</ul>
<h2 id="core-pieces-2"><a class="header" href="#core-pieces-2">Core pieces</a></h2>
<ul>
<li>Initializer — sets up <code>tracing_subscriber</code> layers for stdout and file.</li>
<li>Config — <code>[logging]</code> section controls <code>log_dir</code>, <code>stdout_level</code>, and <code>file_level</code>.</li>
<li>Levels — <code>error</code>, <code>warn</code>, <code>info</code>, <code>debug</code>, <code>trace</code>.</li>
</ul>
<h2 id="how-it-works-4"><a class="header" href="#how-it-works-4">How it works</a></h2>
<ul>
<li>
<p>Startup</p>
<ul>
<li><code>logging::init()</code> is called from <code>main.rs</code> before starting frontends.</li>
<li>Reads <code>CONFIG.logging</code> to build filters and writers.</li>
<li>Installs two layers: ANSI stdout and file appender (<code>sneldb.log</code>, daily rotation).</li>
</ul>
</li>
<li>
<p>Emitting logs</p>
<ul>
<li>Use <code>tracing::{error!, warn!, info!, debug!, trace!}</code> in code.</li>
<li>Prefer spans (e.g., <code>#[instrument]</code>) to capture context around operations.</li>
</ul>
</li>
</ul>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>Example snippet from <code>config.toml</code>:</p>
<pre><code class="language-toml">[logging]
log_dir = "../data/logs"
stdout_level = "debug"
file_level = "error"
</code></pre>
<ul>
<li><code>stdout_level</code>: global level for console logs.</li>
<li><code>file_level</code>: global level for file logs.</li>
<li><code>log_dir</code>: directory where <code>sneldb.log</code> is created (daily rotation).</li>
</ul>
<h2 id="why-this-design-2"><a class="header" href="#why-this-design-2">Why this design</a></h2>
<ul>
<li>Structured logs with levels and spans ease debugging and operations.</li>
<li>Separate stdout/file control supports local development and production hygiene.</li>
</ul>
<h2 id="operational-notes-4"><a class="header" href="#operational-notes-4">Operational notes</a></h2>
<ul>
<li>Tune levels per environment (e.g., <code>stdout_level=warn</code> in prod).</li>
<li>Ensure <code>log_dir</code> exists and is writable; it is created on first write by the appender.</li>
<li>Use targets when necessary to scope logs for noisy modules.</li>
</ul>
<h2 id="further-reading-4"><a class="header" href="#further-reading-4">Further Reading</a></h2>
<ul>
<li><code>tracing</code> crate docs</li>
<li><code>tracing_subscriber</code> filters and formatters</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="wal-archiving"><a class="header" href="#wal-archiving">WAL Archiving</a></h1>
<h2 id="what-it-is-5"><a class="header" href="#what-it-is-5">What it is</a></h2>
<p>WAL archiving keeps copies of your write-ahead logs before they’re deleted. Instead of throwing away the WAL after a flush, SnelDB can compress and store it as a binary archive—a compact, recoverable snapshot of every event that passed through.</p>
<p>Think of it as insurance: if you ever need to rebuild, audit, or replay events from before your oldest segment, the archives have you covered.</p>
<h2 id="why-it-matters-3"><a class="header" href="#why-it-matters-3">Why it matters</a></h2>
<p>WAL files normally disappear after a successful flush. That’s fine for daily operations—the flushed segments contain everything you need. But sometimes you want more:</p>
<ul>
<li><strong>Disaster recovery</strong>: Rebuild the database from archives if segments are lost.</li>
<li><strong>Audit trails</strong>: Keep a complete, timestamped history of every event for compliance.</li>
<li><strong>Data migration</strong>: Export and replay events into a new system.</li>
<li><strong>Debugging</strong>: Inspect the exact sequence of writes leading up to an issue.</li>
</ul>
<p>WAL archiving turns “ephemeral durability” into “long-term safety” without slowing down the write path.</p>
<h2 id="how-it-works-5"><a class="header" href="#how-it-works-5">How it works</a></h2>
<p>When conservative mode is enabled, SnelDB archives WAL files before deleting them:</p>
<ol>
<li>A flush completes successfully.</li>
<li>The cleaner identifies old WAL files (IDs below the new segment cutoff).</li>
<li>For each file:
<ul>
<li>Read the JSON lines (one event per line).</li>
<li>Serialize to MessagePack (compact binary format).</li>
<li>Compress with Zstandard (fast, high-ratio compression).</li>
<li>Write to <code>archive_dir/shard-N/wal-LOGID-START-END.wal.zst</code>.</li>
</ul>
</li>
<li>Only if archiving succeeds, delete the original WAL file.</li>
</ol>
<p>If archiving fails for any reason, the WAL files are preserved—safety first.</p>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<p>Add to your config file (e.g., <code>config/prod.toml</code>):</p>
<pre><code class="language-toml">[wal]
enabled = true
dir = "../data/wal/"
# ... other WAL settings ...

# Conservative mode
conservative_mode = true
archive_dir = "../data/wal/archived/"
compression_level = 3
compression_algorithm = "zstd"
</code></pre>
<h3 id="settings-explained"><a class="header" href="#settings-explained">Settings explained</a></h3>
<ul>
<li><strong>conservative_mode</strong>: Enables archiving. Set to <code>false</code> to delete WAL files immediately (default behavior).</li>
<li><strong>archive_dir</strong>: Where to store compressed archives. Organized by shard: <code>archive_dir/shard-0/</code>, <code>shard-1/</code>, etc.</li>
<li><strong>compression_level</strong>: Zstandard level (1–22). Level 3 is fast with excellent compression (~90–95% reduction). Level 19+ is slower but maximizes compression.</li>
<li><strong>compression_algorithm</strong>: Currently <code>"zstd"</code> (future: <code>"lz4"</code>, <code>"brotli"</code>).</li>
</ul>
<h2 id="what-gets-archived"><a class="header" href="#what-gets-archived">What gets archived</a></h2>
<p>Each archive is a self-contained snapshot:</p>
<ul>
<li><strong>Header</strong>: Metadata (version, shard ID, log ID, entry count, time range, compression details, creation timestamp).</li>
<li><strong>Body</strong>: All <code>WalEntry</code> records with event types, context IDs, timestamps, and payloads fully preserved.</li>
</ul>
<p>File naming example:</p>
<pre><code>wal-00042-1700000000-1700003600.wal.zst
     ^         ^           ^
     |         |           end timestamp
     |         start timestamp
     log ID (zero-padded to 5 digits)
</code></pre>
<p>This makes it easy to identify and sort archives by time.</p>
<h2 id="compression-performance"><a class="header" href="#compression-performance">Compression performance</a></h2>
<p>Typical results (MessagePack + Zstandard):</p>
<ul>
<li>Original JSON WAL: 1.0 GB</li>
<li>Compressed archive: 50–100 MB (90–95% reduction)</li>
<li>Compression speed: ~500 MB/s (level 3)</li>
<li>Decompression speed: ~1000 MB/s</li>
</ul>
<p>The write path remains fast because archiving happens after the flush completes—it’s background work that doesn’t block ingestion.</p>
<h2 id="cli-tool"><a class="header" href="#cli-tool">CLI tool</a></h2>
<p>SnelDB ships with <code>wal_archive_manager</code> for inspecting and managing archives:</p>
<pre><code class="language-bash"># List all archives for shard 0
./wal_archive_manager list 0

# Show detailed info about an archive
./wal_archive_manager info archive.wal.zst

# Export to JSON for inspection or migration
./wal_archive_manager export archive.wal.zst output.json

# Recover all entries from archives
./wal_archive_manager recover 0

# Manually archive a specific WAL log
./wal_archive_manager archive 0 5
</code></pre>
<p>Example output:</p>
<pre><code>$ ./wal_archive_manager list 0

Found 5 archive(s) for shard 0:

  wal-00001-1700000000-1700003600.wal.zst | Log 00001 | 1000 entries | 45.23 KB
  wal-00002-1700003600-1700007200.wal.zst | Log 00002 | 1500 entries | 67.89 KB
  ...

Total: 5000 entries across 5 archives (234.56 KB)
</code></pre>
<h2 id="recovery"><a class="header" href="#recovery">Recovery</a></h2>
<p>To recover events from archives:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use snel_db::engine::core::{WalArchive, WalArchiveRecovery};

let archive_dir = PathBuf::from("../data/wal/archived/shard-0");
let recovery = WalArchiveRecovery::new(0, archive_dir);

// List all archives
let archives = recovery.list_archives()?;

// Recover all entries in chronological order
let entries = recovery.recover_all()?;

// Or recover from a specific archive
let entries = recovery.recover_from_archive(&amp;archives[0])?;
<span class="boring">}</span></code></pre>
<p>Every <code>WalEntry</code> comes back with full metadata:</p>
<ul>
<li>Event type (e.g., <code>"user_signup"</code>, <code>"purchase"</code>)</li>
<li>Context ID</li>
<li>Timestamp</li>
<li>Complete payload</li>
</ul>
<p>This makes it straightforward to replay, migrate, or audit.</p>
<h2 id="archive-retention"><a class="header" href="#archive-retention">Archive retention</a></h2>
<p>Archives accumulate over time. Plan your retention policy:</p>
<pre><code class="language-bash"># Find archives older than 30 days
find ../data/wal/archived/ -name "*.wal.zst" -mtime +30

# Delete old archives (after backing up to S3, tape, etc.)
find ../data/wal/archived/ -name "*.wal.zst" -mtime +30 -delete
</code></pre>
<p>Or automate with a cron job:</p>
<pre><code class="language-bash"># Backup to S3, then delete locally
0 2 * * * find /data/wal/archived -name "*.wal.zst" -mtime +30 | \
  while read f; do aws s3 cp "$f" s3://backups/wal-archives/ &amp;&amp; rm "$f"; done
</code></pre>
<h2 id="safety-guarantees"><a class="header" href="#safety-guarantees">Safety guarantees</a></h2>
<ul>
<li><strong>No data loss</strong>: If archiving fails, WAL files are not deleted.</li>
<li><strong>Atomic operations</strong>: Archives are written atomically; partial writes are not used.</li>
<li><strong>Read-only recovery</strong>: Recovery never modifies archives.</li>
<li><strong>Format versioning</strong>: Archives include a format version for future compatibility.</li>
</ul>
<h2 id="when-to-enable-it"><a class="header" href="#when-to-enable-it">When to enable it</a></h2>
<p>Enable conservative mode if you need:</p>
<ul>
<li>Long-term event history beyond your oldest segment.</li>
<li>Compliance or audit trails for financial, healthcare, or legal data.</li>
<li>Disaster recovery beyond segment backups.</li>
<li>Data export and migration capabilities.</li>
</ul>
<p>Skip it if:</p>
<ul>
<li>Disk space is tight and segments alone are sufficient.</li>
<li>Events have no long-term value beyond query windows.</li>
<li>You have other backup/archival processes in place.</li>
</ul>
<h2 id="trade-offs"><a class="header" href="#trade-offs">Trade-offs</a></h2>
<p><strong>Pros:</strong></p>
<ul>
<li>Complete event history for recovery or compliance.</li>
<li>High compression (10–20× smaller than JSON).</li>
<li>Background work—no impact on write latency.</li>
<li>Easy to inspect, export, and replay.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Extra disk usage (though compressed archives are small).</li>
<li>One more system to monitor and rotate.</li>
<li>Not a replacement for segment backups—archives complement them.</li>
</ul>
<h2 id="what-this-page-is-not-1"><a class="header" href="#what-this-page-is-not-1">What this page is not</a></h2>
<ul>
<li>A file format specification (see <code>wal_archive.rs</code> for internals).</li>
<li>A backup strategy guide (archives are one tool; combine with segment snapshots and offsite copies).</li>
<li>A performance tuning deep-dive (compression level 3 is a safe default; adjust only under load testing).</li>
</ul>
<p>Conservative mode is about <strong>peace of mind</strong>: you decide how long to keep event history and SnelDB makes sure it’s there when you need it.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="file-formats-and-data-layout"><a class="header" href="#file-formats-and-data-layout">File Formats and Data Layout</a></h1>
<h2 id="what-it-is-6"><a class="header" href="#what-it-is-6">What it is</a></h2>
<ul>
<li>The on-disk layout for shards and segments, and the binary formats used for columns, offsets, zone metadata, indexes, and schemas.</li>
<li>These formats are append-friendly, read-optimized, and simple to parse with memory maps.</li>
</ul>
<h2 id="core-pieces-3"><a class="header" href="#core-pieces-3">Core pieces</a></h2>
<ul>
<li>Segments — <code>xxxxx/</code> (zero-padded numeric) directories under each shard.</li>
<li>Columns — <code>{uid}_{field}.col</code> files storing values with length prefixes (includes the engine-managed <code>event_id</code> column alongside user-defined fields).</li>
<li>Zone Compressed Offsets — <code>{uid}_{field}.zfc</code> files listing per-zone compressed block metadata and in-block offsets.</li>
<li>Zone Metadata — <code>{uid}.zones</code> containing per-zone min/max timestamps and row ranges.</li>
<li>Zone Index — <code>{uid}.idx</code> mapping context_id values to zone ids.</li>
<li>XOR Filters — <code>{uid}_{field}.xf</code> per-field filters for fast membership tests.</li>
<li>Enum Bitmap Indexes — <code>{uid}_{field}.ebm</code> per-enum-field bitmaps for zone pruning.</li>
<li>Zone SuRF Filters — <code>{uid}_{field}.zsrf</code> per-field per-zone succinct range filters for range pruning.</li>
<li>Zone XOR Index — <code>{uid}_{field}.zxf</code> per-field per-zone XOR index for equality pruning.</li>
<li>Temporal Calendar Index — <code>{uid}_{field}.cal</code> per-field day/hour buckets → zone ids for temporal pruning.</li>
<li>Temporal Index (slab) — <code>{uid}_{field}.tfi</code> per-field file containing all per‑zone temporal indexes for that field.</li>
<li>Schemas — <code>schema/schemas.bin</code> append-only records of event type schemas and UIDs.</li>
<li>Index Catalog — <code>{uid}.icx</code> per-segment catalog of available index kinds (per-field and global).</li>
</ul>
<h2 id="binary-headers"><a class="header" href="#binary-headers">Binary headers</a></h2>
<ul>
<li>All binary files now begin with a fixed, 20-byte header to improve safety and detect corruption.</li>
<li>Header layout (little-endian):
<ul>
<li>8 bytes: MAGIC (ASCII tag identifying file kind)</li>
<li>2 bytes: VERSION (u16)</li>
<li>2 bytes: FLAGS (u16)</li>
<li>4 bytes: RESERVED (u32)</li>
<li>4 bytes: HEADER_CRC32 (u32) computed over MAGIC+VERSION+FLAGS+RESERVED</li>
</ul>
</li>
<li>WAL logs remain newline-delimited JSON without a binary header.</li>
</ul>
<p>Magic strings per file kind:</p>
<ul>
<li>Columns (<code>.col</code>): <code>EVDBCOL\0</code></li>
<li>Zone Compressed Offsets (<code>.zfc</code>): <code>EVDBZCF\0</code></li>
<li>Zone Metadata (<code>.zones</code>): <code>EVDBZON\0</code></li>
<li>Zone Index (<code>.idx</code> per-UID/context): <code>EVDBUID\0</code></li>
<li>XOR Filters (<code>.xf</code>): <code>EVDBXRF\0</code></li>
<li>Zone SuRF Filters (<code>.zsrf</code>): <code>EVDBZSF\0</code></li>
<li>Zone XOR Index (<code>.zxf</code>): <code>EVDBZXF\0</code></li>
<li>Shard Segment Index (<code>segments.idx</code>): <code>EVDBSIX\0</code></li>
<li>Schemas (<code>schemas.bin</code>): <code>EVDBSCH\0</code></li>
<li>Index Catalog (<code>.icx</code>): <code>EVDBICX\0</code></li>
<li>Enum Bitmap Index (<code>.ebm</code>): <code>EVDBEBM\0</code></li>
<li>Event Snapshots (<code>.snp</code>): <code>EVDBSNP\0</code></li>
<li>Snapshot Metadata (<code>.smt</code>): <code>EVDBSMT\0</code></li>
</ul>
<p>Compatibility and migration:</p>
<ul>
<li>Readers tolerate legacy files that lack headers and continue to parse them.</li>
<li>New writers always prepend the header.</li>
<li>A future strict mode may enforce headers on read.</li>
</ul>
<h2 id="directory-layout"><a class="header" href="#directory-layout">Directory layout</a></h2>
<pre><code>data/
├── cols/
│   ├── shard-0/
│   │   └── 00000/
│   │       ├── {uid}_{field}.col
│   │       ├── {uid}_{field}.zfc
│   │       ├── {uid}.zones
│   │       ├── {uid}.idx
│   │       ├── {uid}_timestamp.cal
│   │       ├── {uid}_timestamp.tfi
│   │       ├── {uid}_{datetime_field}.cal
│   │       ├── {uid}_{datetime_field}.tfi
│   │       ├── {uid}_{field}.xf
│   │       ├── {uid}_{field}.zsrf
│   │       └── {uid}_{field}.ebm
│   └── shard-1/
│       └── 00000/
├── logs/
│   └── sneldb.log.YYYY-MM-DD
└── schema/
    └── schemas.bin
</code></pre>
<p>Snapshots are ad-hoc utility files and can be written anywhere (not tied to the segment layout). Typical usage writes them to a caller-provided path.</p>
<h2 id="column-files-uid_fieldcol"><a class="header" href="#column-files-uid_fieldcol">Column files: <code>{uid}_{field}.col</code></a></h2>
<ul>
<li>Format per value (binary):
<ul>
<li>File begins with a binary header (MAGIC <code>EVDBCOL\0</code>).</li>
<li><code>[u16]</code> little-endian length</li>
<li><code>[bytes]</code> UTF‑8 string of the value</li>
</ul>
</li>
<li>Access pattern: memory-mapped and sliced using offsets.</li>
</ul>
<h2 id="zone-compressed-offsets-uid_fieldzfc"><a class="header" href="#zone-compressed-offsets-uid_fieldzfc">Zone compressed offsets: <code>{uid}_{field}.zfc</code></a></h2>
<ul>
<li>Binary layout per zone (repeated):
<ul>
<li>File begins with a binary header (MAGIC <code>EVDBZOF\0</code>).</li>
<li><code>[u32] zone_id</code></li>
<li><code>[u32] count</code> number of offsets</li>
<li><code>[u64] * count</code> byte offsets into the corresponding <code>.col</code></li>
</ul>
</li>
<li>Purpose: enables loading only the rows for a given zone by first reading and decompressing the zone block, then slicing values using in-block offsets.</li>
</ul>
<h2 id="zone-metadata-uidzones"><a class="header" href="#zone-metadata-uidzones">Zone metadata: <code>{uid}.zones</code></a></h2>
<ul>
<li>Bincode-encoded <code>Vec&lt;ZoneMeta&gt;</code>.</li>
<li>File begins with a binary header (MAGIC <code>EVDBZON\0</code>).</li>
<li>Fields:
<ul>
<li><code>zone_id: u32</code></li>
<li><code>uid: String</code></li>
<li><code>segment_id: u64</code></li>
<li><code>start_row: u32</code></li>
<li><code>end_row: u32</code></li>
<li><code>timestamp_min: u64</code></li>
<li><code>timestamp_max: u64</code></li>
</ul>
</li>
</ul>
<h2 id="zone-index-uididx"><a class="header" href="#zone-index-uididx">Zone index: <code>{uid}.idx</code></a></h2>
<ul>
<li>Binary map of <code>event_type -&gt; context_id -&gt; [zone_id...]</code>.</li>
<li>Used to quickly locate candidate zones by <code>context_id</code>.</li>
<li>Written via <code>ZoneIndex::write_to_path</code> and read with <code>ZoneIndex::load_from_path</code>.</li>
<li>File begins with a binary header (MAGIC <code>EVDBUID\0</code>).</li>
</ul>
<h2 id="xor-filters-uid_fieldxf"><a class="header" href="#xor-filters-uid_fieldxf">XOR filters: <code>{uid}_{field}.xf</code></a></h2>
<ul>
<li>Bincode-serialized <code>BinaryFuse8</code> filter over unique field values.</li>
<li>Used for fast approximate membership checks during planning.</li>
<li>File begins with a binary header (MAGIC <code>EVDBXRF\0</code>).</li>
</ul>
<h2 id="zone-surf-filters-uid_fieldzsrf"><a class="header" href="#zone-surf-filters-uid_fieldzsrf">Zone SuRF filters: <code>{uid}_{field}.zsrf</code></a></h2>
<ul>
<li>Bincode-serialized <code>ZoneSurfFilter</code> containing <code>Vec&lt;ZoneSurfEntry&gt;</code>.</li>
<li>Purpose: zone-level range pruning for numeric, string, and boolean fields using a succinct trie.</li>
<li>File begins with a binary header (MAGIC <code>EVDBZSF\0</code>).</li>
<li>Contents:
<ul>
<li><code>entries: Vec&lt;ZoneSurfEntry&gt;</code> where each entry is <code>{ zone_id: u32, trie: SurfTrie }</code>.</li>
<li><code>SurfTrie</code> stores compact arrays of degrees, child offsets, labels, and terminal flags.</li>
</ul>
</li>
<li>Built during flush/compaction by <code>ZoneWriter::write_all</code> when enabled by the build plan.</li>
<li>Used by <code>ZoneFinder</code> for <code>Gt/Gte/Lt/Lte</code> operations before falling back to XOR/EBM.</li>
<li>Naming mirrors <code>.xf</code>/<code>.ebm</code>: per <code>uid</code> and <code>field</code>.</li>
</ul>
<h2 id="enum-bitmap-index-uid_fieldebm"><a class="header" href="#enum-bitmap-index-uid_fieldebm">Enum bitmap index: <code>{uid}_{field}.ebm</code></a></h2>
<ul>
<li>Zone-level bitmaps per enum variant for fast Eq/Neq pruning.</li>
<li>File begins with a binary header (MAGIC <code>EVDBEBM\0</code>).</li>
<li>Binary layout:
<ul>
<li><code>[u16] variant_count</code></li>
<li>Repeated <code>variant_count</code> times:
<ul>
<li><code>[u16] name_len</code></li>
<li><code>[bytes] variant_name (UTF‑8)</code></li>
</ul>
</li>
<li><code>[u16] rows_per_zone</code></li>
<li>Repeated per zone present in the file:
<ul>
<li><code>[u32] zone_id</code></li>
<li><code>[u16] variant_count_again</code></li>
<li>Repeated <code>variant_count_again</code> times:
<ul>
<li><code>[u32] bitmap_len_bytes</code></li>
<li><code>[bytes] packed_bitmap</code> (LSB-first within a byte; bit i set ⇒ row i has this variant)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Usage: on a filter <code>plan = "pro"</code>, prune zones where the <code>pro</code> bitmap is all zeros; similarly for <code>!=</code> by checking any non-target variant has a bit set.</li>
<li>Observability: use <code>convertor ebm &lt;segment_dir&gt; &lt;uid&gt; &lt;field&gt;</code> to dump a JSON view of per-zone row positions per variant.</li>
</ul>
<h2 id="zone-xor-index-uid_fieldzxf"><a class="header" href="#zone-xor-index-uid_fieldzxf">Zone XOR index: <code>{uid}_{field}.zxf</code></a></h2>
<ul>
<li>Per-zone <code>BinaryFuse8</code> filters over unique field values; used to quickly prune zones on equality.</li>
<li>File begins with a binary header (MAGIC <code>EVDBZXF\0</code>). Each entry: <code>[u32 zone_id][u32 blob_len][bytes serialized BinaryFuse8]</code>.</li>
<li>Built by <code>ZoneWriter::write_all</code> when <code>ZONE_XOR_INDEX</code> is present in the build plan for the field.</li>
</ul>
<h2 id="index-catalog-uidicx"><a class="header" href="#index-catalog-uidicx">Index Catalog: <code>{uid}.icx</code></a></h2>
<ul>
<li>Binary header (MAGIC <code>EVDBICX\0</code>) followed by a bincode-encoded <code>SegmentIndexCatalog</code>:
<ul>
<li><code>uid: String</code></li>
<li><code>segment_id: String</code></li>
<li><code>field_kinds: HashMap&lt;String, IndexKind&gt;</code></li>
<li><code>global_kinds: IndexKind</code></li>
</ul>
</li>
<li>Writers emit an <code>.icx</code> per segment reflecting exactly the indexes built; readers use it to avoid probing for missing files and to select <code>IndexStrategy</code>.</li>
</ul>
<h2 id="schemas-schemaschemasbin"><a class="header" href="#schemas-schemaschemasbin">Schemas: <code>schema/schemas.bin</code></a></h2>
<ul>
<li>Append-only file of bincode-encoded <code>SchemaRecord</code> entries:
<ul>
<li><code>uid: String</code></li>
<li><code>event_type: String</code></li>
<li><code>schema: MiniSchema</code></li>
</ul>
</li>
<li>Loaded at startup by <code>SchemaRegistry</code>.</li>
<li>File begins with a binary header (MAGIC <code>EVDBSCH\0</code>).</li>
</ul>
<h2 id="shard-segment-index-segmentsidx"><a class="header" href="#shard-segment-index-segmentsidx">Shard segment index: <code>segments.idx</code></a></h2>
<ul>
<li>Bincode-encoded <code>Vec&lt;SegmentEntry&gt;</code>; file begins with a binary header (MAGIC <code>EVDBSIX\0</code>).</li>
</ul>
<h2 id="why-this-design-3"><a class="header" href="#why-this-design-3">Why this design</a></h2>
<ul>
<li>Immutable segments + append-only metadata simplify recovery and concurrency.</li>
<li>Memory-mappable, length-prefixed encodings keep parsing simple and fast.</li>
<li>Separate files per concern (values, offsets, metadata, indexes) enable targeted IO.</li>
</ul>
<h2 id="operational-notes-5"><a class="header" href="#operational-notes-5">Operational notes</a></h2>
<ul>
<li>Segment directories are named <code>00000</code>, <code>00001</code>, … (zero-padded numeric). Levels derive from id ranges of size 10_000.</li>
<li>UIDs are per-event-type identifiers generated at DEFINE; filenames use <code>{uid}</code> not the event type.</li>
<li>New fields simply create new <code>.col/.zfc/.xf</code> files in subsequent segments.</li>
</ul>
<h2 id="further-reading-5"><a class="header" href="#further-reading-5">Further Reading</a></h2>
<ul>
<li><a href="#storage-engine">Storage Engine</a></li>
<li><a href="#sharding">Sharding</a></li>
<li><a href="#query-and-replay">Query &amp; Replay</a></li>
</ul>
<h2 id="event-snapshots-snp"><a class="header" href="#event-snapshots-snp">Event snapshots: <code>*.snp</code></a></h2>
<ul>
<li>Purpose: portable bundles of events (potentially mixed types) for export, testing, or replay.</li>
<li>File begins with a binary header (MAGIC <code>EVDBSNP\0</code>).</li>
<li>Binary layout after header:
<ul>
<li><code>[u32] num_events</code></li>
<li>Repeated <code>num_events</code> times:
<ul>
<li><code>[u32] len_bytes</code></li>
<li><code>[bytes] JSON-serialized Event</code> (same schema as API/Event struct)</li>
</ul>
</li>
</ul>
</li>
<li>Notes:
<ul>
<li>Events are serialized as JSON for compatibility (payloads can contain arbitrary JSON values).</li>
<li>Readers stop gracefully on truncated data (warn and return successfully with the parsed prefix).</li>
</ul>
</li>
</ul>
<h2 id="snapshot-metadata-smt"><a class="header" href="#snapshot-metadata-smt">Snapshot metadata: <code>*.smt</code></a></h2>
<h2 id="temporal-calendar-index-uid_fieldcal"><a class="header" href="#temporal-calendar-index-uid_fieldcal">Temporal calendar index: <code>{uid}_{field}.cal</code></a></h2>
<ul>
<li>Per-field calendar over day/hour buckets mapping to candidate zone ids.</li>
<li>File begins with a binary header (MAGIC <code>EVDBCAL\0</code>, version 2).</li>
<li>Binary layout after header:
<ul>
<li>Hour map, then day map; each map encoded as:
<ul>
<li><code>[u32] entry_count</code></li>
<li>Repeated <code>entry_count</code> times:
<ul>
<li><code>[u32] bucket_id</code></li>
<li><code>[u32] roaring_len_bytes</code></li>
<li><code>[bytes] roaring_bitmap</code> serialized via RoaringBitmap serialization</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Purpose: fast, coarse temporal pruning to pick zones for a specific field (<code>timestamp</code> or a payload <code>datetime</code> field like <code>created_at</code>).</li>
</ul>
<h2 id="temporal-index-slab-uid_fieldtfi"><a class="header" href="#temporal-index-slab-uid_fieldtfi">Temporal index (slab): <code>{uid}_{field}.tfi</code></a></h2>
<ul>
<li>
<p>Per-field slab file containing all per-zone temporal indexes for that field.</p>
</li>
<li>
<p>File begins with a binary header (MAGIC <code>EVDBTFI\0</code>, version 2 for slab format).</p>
</li>
<li>
<p>Directory section:</p>
<ul>
<li><code>[u32] zone_count</code></li>
<li>Repeated <code>zone_count</code> times:
<ul>
<li><code>[u32] zone_id</code></li>
<li><code>[u64] offset_bytes</code> to the zone body</li>
<li><code>[u32] len_bytes</code> length of the zone body</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Zone body (per zone), repeated back-to-back:</p>
<ul>
<li><code>[i64] min_ts</code></li>
<li><code>[i64] max_ts</code></li>
<li><code>[i64] stride</code></li>
<li><code>[u32] key_len</code> then <code>key_len</code> times <code>[u64] key</code></li>
<li><code>[u32] fence_len</code> then <code>fence_len</code> times <code>{ [i64] sample_ts, [u32] approx_row }</code></li>
</ul>
</li>
<li>
<p>Purpose: precise temporal membership checks within a zone for Eq and boundary checks for ranges.</p>
</li>
<li>
<p>Notes:</p>
<ul>
<li>The legacy per-zone <code>{uid}_{zone}.tfi</code> files are replaced by the per-field slab. Writers emit only slab files; readers load specific zones via the slab directory.</li>
<li>The fixed <code>timestamp</code> field also uses <code>{uid}_timestamp.cal</code> and <code>{uid}_timestamp.tfi</code>.</li>
</ul>
</li>
<li>
<p>Purpose: describes snapshot ranges per <code>(uid, context_id)</code> with min/max timestamps.</p>
</li>
<li>
<p>File begins with a binary header (MAGIC <code>EVDBSMT\0</code>).</p>
</li>
<li>
<p>Binary layout after header:</p>
<ul>
<li><code>[u32] num_records</code></li>
<li>Repeated <code>num_records</code> times:
<ul>
<li><code>[u32] len_bytes</code></li>
<li><code>[bytes] JSON-serialized SnapshotMeta { uid, context_id, from_ts, to_ts }</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Notes:</p>
<ul>
<li>JSON is used for the same reasons as snapshots (arbitrary strings/IDs, forward-compat fields).</li>
<li>Readers stop gracefully on truncated data (warn and return successfully with the parsed prefix).</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>


        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-de5ebf4e.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/book-de5ebf4e.js"></script>
        <script src="theme/sneldb-6024da72.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
